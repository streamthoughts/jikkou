[{"body":"Running Server on a Specific Port By default, the server runs on port 28082. However, you can set the server to run on a specific port:\n# ./etc/application.yaml micronaut: server: port: 80 # Port used to access APIs endpoints: all: port: 80 # Port used to access Health endpoints Enabling Specific Extension Providers By default, the server is configured to run only with the core and kafka extension providers. However, you can enable (or disable) additional providers:\njikkou: extensions.provider: # By default, disable all extension providers. default.enabled: false # Explicitly enabled/disable an extension provider #\u003cprovider_name\u003e.enabled: \u003cboolean\u003e core.enabled: true kafka.enabled: true # schemaregistry.enabled: true # aiven.enabled: true # kafkaconnect.enabled: true ","categories":"","description":"Learn how to configure the Jikkou API server.\n","excerpt":"Learn how to configure the Jikkou API server.\n","ref":"/docs/jikkou-api-server/configuration/server_configuration/","tags":"","title":"API Server"},{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\nThe command line interface to Jikkou is the jikkou command, which accepts a variety of subcommands such as jikkou apply or jikkou validate.\nTo view a list of the commands available in your current Jikkou version, run jikkou with no additional arguments:\nUsage: jikkou [-hV] [--logger-level=\u003clevel\u003e] [COMMAND] Jikkou CLI:: A command-line client designed to provide an efficient and easy way to manage, automate, and provision all the assets of your data infrastructure. Find more information at: https://streamthoughts.github.io/jikkou/. OPTIONS: -h, --help Show this help message and exit. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -V, --version Print version information and exit. CORE COMMANDS: apply Update the resources as described by the resource definition files. create Create resources from the resource definition files (only non-existing resources will be created). delete Delete resources that are no longer described by the resource definition files. diff Show changes required by the current resource definitions. get Display one or many specific resources. prepare Prepare the resource definition files for validation. update Create or update resources from the resource definition files validate Check whether the resources definitions meet all validation requirements. SYSTEM MANAGEMENT COMMANDS: action List/execute actions. health Print or describe health indicators. ADDITIONAL COMMANDS: api-extensions Print the supported API extensions api-resources Print the supported API resources config Sets or retrieves the configuration of this client generate-completion Generate bash/zsh completion script for jikkou. help Display help information about the specified command. (The output from your current Jikkou version may be different than the above example.)\nChecking Jikkou Version Run the jikkou --version to display your current installation version:\nJikkou version \"0.32.0\" 2023-11-28 JVM: 21.0.1 (GraalVM Community Substrate VM 21.0.1+12) Shell Tab-completion It is recommended to install the bash/zsh completion script jikkou_completion.\nThe completion script can be downloaded from the project Github repository:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/main/jikkou_completion . jikkou_completion or alternatively, you can run the following command to generate it.\nsource \u003c(jikkou generate-completion) ","categories":"","description":"","excerpt":" Hands-on: Try the Jikkou: Get Started tutorials.\nThe command line ‚Ä¶","ref":"/docs/jikkou-cli/basic-cli-features/","tags":"","title":"Basic CLI Features"},{"body":"You can use a ConfigMap to define reusable data in the form of key/value pairs that can then be referenced and used by other resources.\nSpecification --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: '\u003cCONFIG-MAP-NAME\u003e' # Name of the ConfigMap (required) data: # Map of key-value pairs (required) \u003cKEY_1\u003e: \"\u003cVALUE_1\u003e\" Example For example, the below ConfigMap show how to define default config properties namedcKafkaTopicConfig that can then reference and used to define multiple KafkaTopic. resources.\n--- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'KafkaTopicConfig' data: cleanup.policy: 'delete' min.insync.replicas: 2 retention.ms: 86400000 # (1 day) ","categories":"","description":"Learn how to use ConfigMap objects.\n","excerpt":"Learn how to use ConfigMap objects.\n","ref":"/docs/providers/core/resources/configmap/","tags":"","title":"ConfigMap"},{"body":"This document will guide you through setting up Jikkou in a few minutes and managing your first resources with Jikkou.\nPrerequisites The following prerequisites are required for a successful and properly use of Jikkou.\nMake sure the following is installed:\nAn Apache Kafka cluster. Using Docker, Docker Compose is the easiest way to use it. Java 21 (not required when using the binary version). Start your local Apache Kafka Cluster You must have access to an Apache Kafka cluster for using Jikkou. Most of the time, the latest version of Jikkou is always built for working with the most recent version of Apache Kafka.\nMake sure the Docker is up and running.\nThen, run the following commands:\n$ git clone https://github.com/streamthoughts/jikkou $ cd jikkou $ ./up # use ./down for stopping the docker-compose stack Run Jikkou Download the latest distribution (For Linux) Run the following commands to install the latest version:\nwget https://github.com/streamthoughts/jikkou/releases/download/v0.34.0/jikkou-0.34.0-linux-x86_64.zip \u0026\u0026 \\ unzip jikkou-0.34.0-linux-x86_64.zip \u0026\u0026 \\ cp jikkou-0.34.0-linux-x86_64/bin/jikkou $HOME/.local/bin \u0026\u0026 \\ source \u003c(jikkou generate-completion) \u0026\u0026 \\ jikkou --version For more details, or for other options, see the installation guide.\nConfigure Jikkou for your local Apache Kafka cluster Set configuration context for localhost\njikkou config set-context localhost --config-props=kafka.client.bootstrap.servers=localhost:9092 Show the complete configuration.\njikkou config view --name localhost Finally, let‚Äôs check if your cluster is accessible:\njikkou health get kafka (output)\nIf OK, you should get an output similar to :\n--- name: \"kafka\" status: \"UP\" details: resource: \"urn:kafka:cluster:id:KRzY-7iRTHy4d1UVyNlcuw\" brokers: - id: \"1\" host: \"localhost\" port: 9092 Create your first topics First, create a resource YAML file describing the topics you want to create on your cluster:\nfile: kafka-topics.yaml\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following Jikkou command to trigger the topic creation on the cluster:\njikkou create -f ./kafka-topics.yaml (output)\nTASK [ADD] Add topic 'my-first-topic' (partitions=5, replicas=-1, configs=[cleanup.policy=compact]) - CHANGED { \"changed\": true, \"end\": 1683986528117, \"resource\": { \"name\": \"my-first-topic\", \"partitions\": { \"after\": 5 }, \"replicas\": { \"after\": -1 }, \"configs\": { \"cleanup.policy\": { \"after\": \"compact\", \"operation\": \"ADD\" } }, \"operation\": \"ADD\" }, \"failed\": false, \"status\": \"CHANGED\" } TASK [ADD] Add topic 'my-second-topic' (partitions=4, replicas=-1, configs=[cleanup.policy=delete]) - CHANGED { \"changed\": true, \"end\": 1683986528117, \"resource\": { \"name\": \"my-second-topic\", \"partitions\": { \"after\": 4 }, \"replicas\": { \"after\": -1 }, \"configs\": { \"cleanup.policy\": { \"after\": \"delete\", \"operation\": \"ADD\" } }, \"operation\": \"ADD\" }, \"failed\": false, \"status\": \"CHANGED\" } EXECUTION in 772ms ok: 0, created: 2, altered: 0, deleted: 0 failed: 0 Tips In the above command, we chose to use the create command to create the new topics. But we could just as easily use the update or apply command to get the same result depending on our needs. Finally, you can verify that topics are created on the cluster\njikkou get kafkatopics --default-configs Tips We use the --default-configs to export built-in default configuration for configs that have a default value. Update Kafka Topics Edit your kafka-topics.yaml to add a retention.ms: 86400000 property to the defined topics.\nThen, run the following command.\njikkou update -f ./kafka-topics.yaml Delete Kafka Topics To delete all topics defines in the topics.yaml, add an annotation jikkou.io/delete: true as follows:\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: annotations: # Annotation to specify that all resources must be deleted. jikkou.io/delete: true items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following command:\n$ jikkou apply \\ --files ./kafka-topics.yaml \\ --selector \"metadata.name MATCHES (my-.*-topic)\" \\ --dry-run Using the dry-run option, give you the possibility to check the changes that will be made before applying them.\nNow, rerun the above command without the --dry-run option to definitively delete the topics.\nRecommendation When working in a production environment, we strongly recommend running commands with a --selector option to ensure that changes are only applied to a specific set of resources. Also, always run your command in --dry-run mode to verify the changes that will be executed by Jikkou before continuing. Reading the Help To learn more about the available Jikkou commands, use jikkou help or type a command followed by the -h flag:\n$ jikkou help get Next Steps Now, you‚Äôre ready to use Jikkou!üöÄ\nAs next steps, we suggest reading the following documentation in this order:\nLearn Jikkou concepts Read the Developer Guide to understand how to use the Jikkou API for Java Look at the examples ","categories":"","description":"This guide covers how you can quickly get started using Jikkou.\n","excerpt":"This guide covers how you can quickly get started using Jikkou.\n","ref":"/docs/tutorials/get_started/","tags":"","title":"Jikkou Getting Started"},{"body":"Releases The latest stable release of Jikkou API Server is available:\nAs a Java binary distribution (.zip) from GitHub Releases As a docker image available from Docker Hub. Standalone Installation Follow these few steps to download the latest stable versions and get started.\nPrerequisites To be able to run Jikkou API Server, the only requirement is to have a working Java 21 installation. You can check the correct installation of Java by issuing the following command:\njava -version Step 1: Download Download the latest Java binary distribution from the GitHub Releases (e.g. jikkou-api-server-0.31.0.zip)\nUnpack the download distribution and move the unpacked directory to a desired destination\nunzip jikkou-api-server-$LATEST_VERSION.zip mv jikkou-api-server-$LATEST_VERSION /opt/jikkou Step 2: Start the API Server Launch the application with:\n./bin/jikkou-api-server.sh Step 3: Test the API Server $ curl -sX GET http://localhost:28082 -H \"Accept: application/json\" | jq { \"version\": \"0.31.0\", \"build_time\": \"2023-11-14T18:07:38+0000\", \"commit_id\": \"dae1be11c092256f36c18c8f1d90f16b0c951716\", \"_links\": { \"self\": { \"href\": \"/\", \"templated\": false }, \"get-apis\": { \"href\": \"/apis\", \"templated\": false } } } Step 4: Stop the API Server PID=`ps -ef | grep -v grep | grep JikkouApiServer | awk '{print $2}'` kill $PID Docker # Run Docker docker run -it \\ --net host \\ streamthoughts/jikkou-api-server:latest Development Builds In addition to releases you can download or install development snapshots of Jikkou API Server.\nFrom Docker Hub Docker images are built and push to Docker Hub from the latest main branch.\nThey are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features.\n$ docker run -it streamthoughts/jikkou-api-server:main ","categories":"","description":"This guide shows how to install Jikkou API Server.\n","excerpt":"This guide shows how to install Jikkou API Server.\n","ref":"/docs/jikkou-api-server/install/","tags":"","title":"Install Jikkou API Server"},{"body":" Welcome to the Jikkou documentation! Jikkou, means ‚Äúexecution (e.g. of a plan) or actual state (of things)‚Äù in Japanese.\nWhat Is Jikkou ? Jikkou is a powerful, flexible open-source framework that enables self-serve resource provisioning. It allows developers and DevOps teams to easily manage, automate, and provision all the resources needed for their Apache Kafka¬Æ platform.\nJikkou was born with the aim to streamline day-to-day operations on Apache Kafka¬Æ, ensuring that platform governance is no longer a tedious and boring task for both developers and administrators.\nWhat Are The Use-Cases ? Jikkou is primarily used as a GitOps solution for Kafka configuration management.\nHere are some of the various use cases we‚Äôve observed in different projects:\nTopic as a Service: Build a self-serve platform for managing Kafka topics. ACL Management: Centrally manage all ACLs of an Apache Kafka cluster. Kafka Connectors Management: Deploy and manage Kafka Connect connectors. Ad Hoc Changes: Apply ad hoc changes as needed. Audit: Easily check configurations of topics, brokers, or identify divergences between different environments. Kafka Configuration Backup: Periodically export all critical configurations of your Kafka cluster. Configuration Replication: Replicate the Kafka configuration from one cluster to another. How Does Jikkou Work ? Jikkou offers flexibility in deployment, functioning either as a simple CLI (Command Line Interface) or as a REST server, based on your requirements.\nBy adopting a stateless approach, Jikkou does not store any internal state. Instead, it leverages your platforms or services as the source of truth. This design enables seamless integration with other solutions, such as Ansible and Terraform, or allows for ad hoc use for specific tasks, making Jikkou incredibly flexible and versatile.\nIs Jikkou For Me ? Jikkou can be implemented regardless of the size of your team or data platform.\nSmall Development Team Jikkou is particularly useful for small development teams looking to quickly automate the creation and maintenance of their topics without having to implement a complex solution that requires learning a new technology or language.\nCentralized Infrastructure (DevOps) Team Jikkou can be very effective in larger contexts, where the configuration of your Kafka Topics, ACLs, and Quotas for all your data platform is managed by a single and centralized devops team.\nDecentralized Data Product Teams In an organization adopting Data Mesh principles, Jikkou can be leveraged in a decentralized way by each of your Data Teams to manage all the assets (e.g. Topics, ACLs, Schemas, Connectors, etc.) necessary to expose and manage their Data Products.\nCan I Use Jikkou with my Apache Kafka vendor ? Jikkou can be used any Apache Kafka infrastructures, including:\nApache Kafka Aiven Amazon MSK Confluent Cloud Redpanda ","categories":"","description":"What is Jikkou ?","excerpt":"What is Jikkou ?","ref":"/docs/overview/","tags":"","title":"Overview"},{"body":"Packaging Extensions You can extend Jikkou‚Äôs capabilities by developing custom extensions and resources.\nAn extension must be developed in Java and packaged as a tarball or ZIP archive. The archive must contain a single top-level directory containing the extension JAR files, as well as any resource files or third party libraries required by your extensions. An alternative approach is to create an uber-JAR that contains all the extension‚Äôs JAR files and other resource files needed.\nAn extension package is more commonly described as an Extension Provider.\nDependencies Jikkou‚Äôs sources are available on Maven Central\nTo start developing custom extension for Jikkou, simply add the Core library to your project‚Äôs dependencies.\nFor Maven:\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-core\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e For Gradle:\nimplementation group: 'io.streamthoughts', name: 'jikkou-core', version: ${jikkou.version} Extension Discovery Jikkou uses the standard Java ServiceLoader mechanism to discover and registers custom extensions and resources. For this, you will need to the implement the Service Provider Interface: io.streamthoughts.jikkou.spi.ExtensionProvider\n/** * \u003cpre\u003e * Service interface for registering extensions and resources to Jikkou at runtime. * The implementations are discovered using the standard Java {@link java.util.ServiceLoader} mechanism. * * Hence, the fully qualified name of the extension classes that implement the {@link ExtensionProvider} * interface must be added to a {@code META-INF/services/io.streamthoughts.jikkou.spi.ExtensionProvider} file. * \u003c/pre\u003e */ public interface ExtensionProvider extends HasName, Configurable { /** * Registers the extensions for this provider. * * @param registry The ExtensionRegistry. */ void registerExtensions(@NotNull ExtensionRegistry registry); /** * Registers the resources for this provider. * * @param registry The ResourceRegistry. */ void registerResources(@NotNull ResourceRegistry registry); } Recommendations If you are using Maven as project management tool, we recommended to use the Apache Maven Assembly Plugin to package your extensions as a tarball or ZIP archive.\nSimply create an assembly descriptor in your project as follows:\nsrc/main/assembly/package.xml\n\u003cassembly xmlns=\"http://maven.apache.org/ASSEMBLY/2.2.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/ASSEMBLY/2.2.0 http://maven.apache.org/xsd/assembly-2.2.0.xsd\"\u003e \u003cid\u003epackage\u003c/id\u003e \u003cformats\u003e \u003cformat\u003ezip\u003c/format\u003e \u003c/formats\u003e \u003cincludeBaseDirectory\u003efalse\u003c/includeBaseDirectory\u003e \u003cfileSets\u003e \u003cfileSet\u003e \u003cdirectory\u003e${project.basedir}\u003c/directory\u003e \u003coutputDirectory\u003e${organization.name}-${project.artifactId}/doc\u003c/outputDirectory\u003e \u003cincludes\u003e \u003cinclude\u003eREADME*\u003c/include\u003e \u003cinclude\u003eLICENSE*\u003c/include\u003e \u003cinclude\u003eNOTICE*\u003c/include\u003e \u003c/includes\u003e \u003c/fileSet\u003e \u003c/fileSets\u003e \u003cdependencySets\u003e \u003cdependencySet\u003e \u003coutputDirectory\u003e${organization.name}-${project.artifactId}/lib\u003c/outputDirectory\u003e \u003cuseProjectArtifact\u003etrue\u003c/useProjectArtifact\u003e \u003cuseTransitiveFiltering\u003etrue\u003c/useTransitiveFiltering\u003e \u003cunpack\u003efalse\u003c/unpack\u003e \u003cexcludes\u003e \u003cexclude\u003eio.streamthoughts:jikkou-core\u003c/exclude\u003e \u003c/excludes\u003e \u003c/dependencySet\u003e \u003c/dependencySets\u003e \u003c/assembly\u003e Then, configure the maven-assembly-plugin in the pom.xml file of your project:\n\u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-assembly-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cfinalName\u003e${organization.name}-${project.artifactId}-${project.version}\u003c/finalName\u003e \u003cappendAssemblyId\u003efalse\u003c/appendAssemblyId\u003e \u003cdescriptors\u003e \u003cdescriptor\u003esrc/assembly/package.xml\u003c/descriptor\u003e \u003c/descriptors\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003emake-assembly\u003c/id\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003cexecution\u003e \u003cid\u003etest-make-assembly\u003c/id\u003e \u003cphase\u003epre-integration-test\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e Finally, use the mvn clean package to build your project and create the archive.\nInstalling Extension Providers To install an Extension Provider, all you need to do is to unpacks the archive into a desired location ( e.g., /usr/share/jikkou-extensions). Also, you should ensure that the archive‚Äôs top-level directory name is unique, to prevent overwriting existing files or extensions.\nConfiguring Extension Providers Custom extensions can be supplied to the Jikkou‚Äôs API Server and Jikkou CLI (when running the Java Binary Distribution, i.e., not the native version). For this, you simply need to configure the jikkou.extension.paths property. The property accepts a list of paths from which to load extension providers.\nExample for the Jikkou API Server:\n# application.yaml jikkou: extension.paths: - /usr/share/jikkou-extensions Once your extensions are configured you should be able to list your extensions using either :\nThe Jikkou CLI: jikkou api-extensions list command, or The Jikkou API Server: GET /apis/core.jikkou.io/v1/extensions -H \"Accept: application/json\" ","categories":"","description":"Learn how to package and install custom extensions for Jikkou.\n","excerpt":"Learn how to package and install custom extensions for Jikkou.\n","ref":"/docs/developer-guide/extensions/package/","tags":"","title":"Package Extensions"},{"body":" Jikkou Resources are entities that represent the state of a concrete instance of a concept that are part of the state of your system, like a Topic on an Apache Kafka cluster.\nResource Objects All resources can be distinguished between persistent objects, which are used to describe the desired state of your system, and transient objects, which are only used to enrich or provide additional capabilities for the definition of persistent objects.\nA resource is an object with a type (called a Kind) and a concrete model that describe the associated data. All resource are scoped by an API Group and Version.\nResource Definition Resources are described in YAML format.\nHere is a sample resource that described a Kafka Topic.\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic' labels: environment: test annotations: {} spec: partitions: 1 replicas: 1 configs: min.insync.replicas: 1 cleanup.policy: 'delete' Resource Properties The following are the properties that can be set to describe a resource:\nProperty Description apiVersion The group/version of the resource type. kind The type of the describe resource. metadata.name An optional name to identify the resource. metadata.labels Arbitrary metadata to attach to the resource that can be handy when you have a lot of resources and you only need to identity or filters some objects. metadata.annotations Arbitrary non-identifying metadata to attach to the resource to mark them for a specific operation or to record some metadata. spec The object properties describing a desired state ","categories":"","description":"","excerpt":" Jikkou Resources are entities that represent the state of a concrete ‚Ä¶","ref":"/docs/concepts/resource/","tags":["concept"],"title":"Resource"},{"body":" Here, you will find the list of core resources supported for Jikkou.\nCore Resources More information:\n","categories":"","description":"","excerpt":" Here, you will find the list of core resources supported for Jikkou. ‚Ä¶","ref":"/docs/providers/core/resources/","tags":"","title":"Resources"},{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\nConfiguration To set up the configuration settings used by Jikkou CLI, you will need create a jikkou config file, which is created automatically when you create a configuration context using:\njikkou config set-context \u003ccontext-name\u003e [--config-file=\u003cconfig-gile\u003e] [--config-props=\u003cconfig-value\u003e] By default, the configuration of jikkou is located under the path $HOME/.jikkou/config.\nThis jikkou config file defines all the contexts that can be used by jikkou CLI.\nFor example, below is the config file created during the Getting Started.\n{ \"currentContext\": \"localhost\", \"localhost\": { \"configFile\": null, \"configProps\": { \"kafka.client.bootstrap.servers\": \"localhost:9092\" } } } Most of the time, a context does not directly contain the configuration properties to be used, but rather points to a specific HOCON (Human-Optimized Config Object Notation) through the configFile property.\nThen, the configProps allows you to override some of the property define by this file.\nIn addition, if no configuration file path is specified, Jikkou will lookup for an application.conf to those following locations:\n./application.conf $HOME/.jikkou/application.conf Finally, Jikkou always fallback to a reference.conf file that you can use as a template to define your own configuration.\nreference.conf:\njikkou { extension.providers { # By default, disable all extensions default.enabled: true # Explicitly enabled/disable extensions #\u003cprovider_name\u003e.enabled: \u003cboolean\u003e # schemaregistry.enabled = true # kafka.enabled = true # aiven.enabled = true # kafkaconnect.enabled = true } # Configure Jikkou Proxy Mode # proxy { # url = \"http://localhost:8080\" # } # Kafka Extension kafka { # The default Kafka Client configuration client { bootstrap.servers = \"localhost:9092\" bootstrap.servers = ${?JIKKOU_DEFAULT_KAFKA_BOOTSTRAP_SERVERS} } brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } schemaRegistry { url = \"http://localhost:8081\" url = ${?JIKKOU_DEFAULT_SCHEMA_REGISTRY_URL} } # The default custom transformations to apply on any resources. transformations = [] # The default custom validations to apply on any resources. validations = [ { name = \"topicMustHaveValidName\" type = io.streamthoughts.jikkou.kafka.validation.TopicNameRegexValidation priority = 100 config = { topicNameRegex = \"[a-zA-Z0-9\\\\._\\\\-]+\" topicNameRegex = ${?VALIDATION_DEFAULT_TOPIC_NAME_REGEX} } }, { name = \"topicMustHavePartitionsEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation priority = 100 config = { topicMinNumPartitions = 1 topicMinNumPartitions = ${?VALIDATION_DEFAULT_TOPIC_MIN_NUM_PARTITIONS} } }, { name = \"topicMustHaveReplicasEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactorValidation priority = 100 config = { topicMinReplicationFactor = 1 topicMinReplicationFactor = ${?VALIDATION_DEFAULT_TOPIC_MIN_REPLICATION_FACTOR} } } ] # The default custom reporters to report applied changes. reporters = [ # Uncomment following lines to enable default kafka reporter # { # name = \"default\" # type = io.streamthoughts.jikkou.kafka.reporter.KafkaChangeReporter # config = { # event.source = \"jikkou/cli\" # kafka = { # topic.creation.enabled = true # topic.creation.defaultReplicationFactor = 1 # topic.name = \"jikkou-resource-change-event\" # client = ${jikkou.kafka.client} { # client.id = \"jikkou-reporter-producer\" # } # } # } # } ] } Listing Contexts $ jikkou config get-contexts NAME localhost * development staging production Verify Current Context You can use jikkou config current-context command to show the context currently used by Jikkou CLI.\n$ jikkou config current-context Using context 'localhost' KEY VALUE ConfigFile ConfigProps {\"kafka.client.bootstrap.servers\": \"localhost:9092\"} Verify Current Configuration You can use jikkou config view command to show the configuration currently used by Jikkou CLI.\nTips To debug the configuration use by Jikkou, you can run the following command: jikkou config view --comments or jikkou config view --debug ","categories":"","description":"Learn how to configure Jikkou CLI.\n","excerpt":"Learn how to configure Jikkou CLI.\n","ref":"/docs/jikkou-cli/cli-configuration/","tags":"","title":"CLI Configuration"},{"body":" Here, you will find the list of resources supported by the extension for Aiven.\nConfiguration You can configure the properties to be used to connect the Aiven service through the Jikkou client configuration property jikkou.aiven.\nExample:\njikkou { aiven { # Aiven project name project = \"http://localhost:8081\" # Aiven service name service = generic # URL to the Aiven REST API. apiUrl = \"https://api.aiven.io/v1/\" # Aiven Bearer Token. Tokens can be obtained from your Aiven profile page tokenAuth = null # Enable debug logging debugLoggingEnabled = false } } ","categories":"","description":"Learn how to configure the extensions for Aiven.\n","excerpt":"Learn how to configure the extensions for Aiven.\n","ref":"/docs/providers/aiven/configuration/","tags":"","title":"Configuration"},{"body":" This section describes how to configure the Kafka Connect extension.\nExtension The Kafka Connect extension can be enabled/disabled via the configuration properties:\n# Example jikkou { extensions.provider.kafkaconnect.enabled = true } Configuration You can configure the properties to be used to connect the Kafka Connect cluster through the Jikkou client configuration property: jikkou.kafkaConnect.\nExample:\njikkou { kafkaConnect { # Array of Kafka Connect clusters configurations. clusters = [ { # Name of the cluster (e.g., dev, staging, production, etc.) name = \"locahost\" # URL of the Kafka Connect service url = \"http://localhost:8083\" # Method to use for authenticating on Kafka Connect. Available values are: [none, basicauth, ssl] authMethod = none # Use when 'authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false # Ssl Config: Use when 'authMethod' is 'ssl' # The location of the key store file. sslKeyStoreLocation = \"/certs/registry.keystore.jks\" # The file format of the key store file. sslKeyStoreType = \"JKS\" # The password for the key store file. sslKeyStorePassword = \"password\" # The password of the private key in the key store file. sslKeyPassword = \"password\" # The location of the trust store file. sslTrustStoreLocation = \"/certs/registry.truststore.jks\" # The file format of the trust store file. sslTrustStoreType = \"JKS\" # The password for the trust store file. sslTrustStorePassword = \"password\" # Specifies whether to ignore the hostname verification. sslIgnoreHostnameVerification = true } ] } } ","categories":"","description":"Learn how to configure the extensions for Kafka Connect.\n","excerpt":"Learn how to configure the extensions for Kafka Connect.\n","ref":"/docs/providers/kafka-connect/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nConfiguration The Apache Kafka extension is built on top of the Kafka Admin Client. You can configure the properties to be passed to kafka client through the Jikkou client configuration property jikkou.kafka.client.\nExample:\njikkou { kafka { client { bootstrap.servers = \"localhost:9092\" security.protocol = \"SSL\" ssl.keystore.location = \"/tmp/client.keystore.p12\" ssl.keystore.password = \"password\" ssl.keystore.type = \"PKCS12\" ssl.truststore.location = \"/tmp/client.truststore.jks\" ssl.truststore.password = \"password\" ssl.key.password = \"password\" } } } In addition, the extension support configuration settings to wait for at least a minimal number of brokers before processing.\njikkou { kafka { brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } } ","categories":"","description":"Learn how to configure the extensions for Apache Kafka.\n","excerpt":"Learn how to configure the extensions for Apache Kafka.\n","ref":"/docs/providers/kafka/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for SchemaRegistry.\nConfiguration You can configure the properties to be used to connect the SchemaRegistry service through the Jikkou client configuration property jikkou.schemaRegistry.\nExample:\njikkou { schemaRegistry { # Comma-separated list of URLs for schema registry instances that can be used to register or look up schemas url = \"http://localhost:8081\" # The name of the schema registry implementation vendor - can be any value vendor = generic # Method to use for authenticating on Schema Registry. Available values are: [none, basicauth, ssl] authMethod = none # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false # Ssl Config: Use when 'authMethod' is 'ssl' # The location of the key store file. sslKeyStoreLocation = \"/certs/registry.keystore.jks\" # The file format of the key store file. sslKeyStoreType = \"JKS\" # The password for the key store file. sslKeyStorePassword = \"password\" # The password of the private key in the key store file. sslKeyPassword = \"password\" # The location of the trust store file. sslTrustStoreLocation = \"/certs/registry.truststore.jks\" # The file format of the trust store file. sslTrustStoreType = \"JKS\" # The password for the trust store file. sslTrustStorePassword = \"password\" # Specifies whether to ignore the hostname verification. sslIgnoreHostnameVerification = true } } ","categories":"","description":"Learn how to configure the extensions for SchemaRegistry.\n","excerpt":"Learn how to configure the extensions for SchemaRegistry.\n","ref":"/docs/providers/schema-registry/configuration/","tags":"","title":"Configuration"},{"body":"Jikkou API Server is built with Micronaut Framework.\nThe default configuration file is located in the installation directory of you server under the path /etc/application.yaml.\nYou can either modify this configuration file directly or create a new one. Then, your configuration file path can be targeted through the MICRONAUT_CONFIG_FILES environment variable.\nA YAML Configuration file example can be found here: application.yaml\nNote For more information about how to configure the application, we recommend you to read the official Micronaut documentation (see: Application Configuration). ","categories":"","description":"Learn how to configure Jikkou API Server.\n","excerpt":"Learn how to configure Jikkou API Server.\n","ref":"/docs/jikkou-api-server/configuration/","tags":"","title":"Configurations"},{"body":" This section covers the core classes to develop validation extensions.\nInterface To create a custom validation, you will need to implement the Java interface: io.streamthoughts.jikkou.core.validation.Validation.\nThis interface defines two methods, with a default implementation for each, to give you the option of validating either all resources accepted by validation at once, or each resource one by one.\npublic interface Validation\u003cT extends HasMetadata\u003e extends Interceptor { /** * Validates the specified resource list. * * @param resources The list of resources to be validated. * @return The ValidationResult. */ default ValidationResult validate(@NotNull final List\u003cT\u003e resources) { // code omitted for clarity } /** * Validates the specified resource. * * @param resource The resource to be validated. * @return The ValidationResult. */ default ValidationResult validate(@NotNull final T resource) { // code omitted for clarity } } Examples The validation class below shows how to validate that any resource has a specific non-empty label.\n@Title(\"HasNonEmptyLabelValidation allows validating that resources have a non empty label.\") @Description(\"This validation can be used to ensure that all resources are associated to a specific label. The labe key is passed through the configuration of the extension.\") @Example( title = \"Validate that resources have a non-empty label with key 'owner'.\", full = true, code = {\"\"\" validations: - name: \"resourceMustHaveNonEmptyLabelOwner\" type: \"com.example.jikkou.validation.HasNonEmptyLabelValidation\" priority: 100 config: key: owner \"\"\" } ) @SupportedResources(value = {}) // an empty list implies that the extension supports any resource-type public final class HasNonEmptyLabelValidation implements Validation { // The required config property. static final ConfigProperty\u003cString\u003e LABEL_KEY_CONFIG = ConfigProperty.ofString(\"key\"); private String key; /** * Empty constructor - required. */ public HasNonEmptyLabelValidation() { } /** * {@inheritDoc} */ @Override public void configure(@NotNull final Configuration config) { // Get the key from the configuration. this.key = LABEL_KEY_CONFIG .getOptional(config) .orElseThrow(() -\u003e new ConfigException( String.format(\"The '%s' configuration property is required for %s\", LABEL_KEY_CONFIG.key(), TopicNamePrefixValidation.class.getSimpleName() ) )); } /** * {@inheritDoc} */ @Override public ValidationResult validate(final @NotNull HasMetadata resource) { Optional\u003cString\u003e label = resource.getMetadata() .findLabelByKey(this.key) .map(NamedValue::getValue) .map(Value::asString) .filter(String::isEmpty); // Failure if (label.isEmpty()) { String error = String.format( \"Resource for name '%s' have no defined or empty label for key: '%s'\", resource.getMetadata().getName(), this.key ); return ValidationResult.failure(new ValidationError(getName(), resource, error)); } // Success return ValidationResult.success(); } } ","categories":"","description":"Learn how to develop custom resource validations.\n","excerpt":"Learn how to develop custom resource validations.\n","ref":"/docs/developer-guide/extensions/validation/","tags":"","title":"Develop Custom Validations"},{"body":" This guide describes how developers can write new extensions for Jikkou.\nMore information:\n","categories":"","description":"Learn how to write custom extensions for Jikkou\n","excerpt":"Learn how to write custom extensions for Jikkou\n","ref":"/docs/developer-guide/extensions/","tags":"","title":"Extension Developer Guide"},{"body":" Jikkou can be installed either from source, or from releases.\nFrom SDKMan! (recommended) The latest stable release of jikkou (x86) for Linux, and macOS can be retrieved via SDKMan!:\nsdk install jikkou From The Jikkou Project Releases Every release released versions of Jikkou is available:\nAs a zip/tar.gz package from GitHub Releases (for Linux, MacOS) As a fatJar available from Maven Central As a docker image available from Docker Hub. These are the official ways to get Jikkou releases that you manually downloaded and installed.\nInstall From Release distribution Download your desired version Unpack it (unzip jikkou-0.34.0-linux-x86_64.zip) Move the unpacked directory to the desired destination (mv jikkou-0.34.0-linux-x86_64 /opt/jikkou) Add the executable to your PATH (export PATH=$PATH:/opt/jikkou/bin) From there, you should be able to run the client: jikkou help.\nIt is recommended to install the bash/zsh completion script jikkou_completion:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/master/jikkou_completion . jikkou_completion or alternatively, run the following command for generation the completion script.\n$ source \u003c(jikkou generate-completion) Using Docker Image # Create a Jikkou configfile (i.e., jikkouconfig) cat \u003c\u003c EOF \u003ejikkouconfig { \"currentContext\" : \"localhost\", \"localhost\" : { \"configFile\" : null, \"configProps\" : { \"kafka.client.bootstrap.servers\" : \"localhost:9092\" } } } EOF # Run Docker docker run -it \\ --net host \\ --mount type=bind,source=\"$(pwd)\"/jikkouconfig,target=/etc/jikkou/config \\ streamthoughts/jikkou:latest -V Development Builds In addition to releases you can download or install development snapshots of Jikkou.\nFrom Docker Hub Docker images are built and push to Docker Hub from the latest main branch. They are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features.\n$ docker run -it streamthoughts/jikkou:main From Source (Linux, macOS) Building Jikkou from source is slightly more work, but is the best way to go if you want to test the latest ( pre-release) Jikkou version.\nPrerequisites To build the project you will need:\nJava 21 (i.e. $JAVA_HOME environment variable is configured). GraalVM 22.1.0 or newer to create native executable TestContainer to run integration tests Create Native Executable # Build and run all tests ./mvnw clean verify -Pnative You can then execute the native executable with: ./jikkou-cli/target/jikkou-$PROJECT_VERSION-runner\nBuild Debian Package (.deb) # Build and run all tests ./mvnw clean package -Pnative ./mvnw package -Pdeb You can then install the package with: sudo dpkg -i ./dist/jikkou-$PROJECT_VERSION-linux-x86_64.deb\nNOTE: Jikkou will install itself in the directory : /opt/jikkou\nBuild RPM Package # Build and run all tests ./mvnw clean package -Pnative ./mvnw package -Prpm The RPM package will available in the ./target/rpm/jikkou/RPMS/noarch/ directory.\n","categories":"","description":"This guide shows how to install the Jikkou CLI.\n","excerpt":"This guide shows how to install the Jikkou CLI.\n","ref":"/docs/install/","tags":"","title":"Install Jikkou"},{"body":"Labels You can use labels to attach arbitrary identifying metadata to objects.\nLabels are key/value maps:\nmetadata: labels: \"key1\": \"value-1\" \"key2\": \"value-2\" Note The keys in the map must be string, but values can be any scalar types (string, boolean, or numeric). Labels are not persistent Jikkou is completely stateless. In other words, it will not store any state about the describe resources objects. Thus, when retrieving objects from your system labels may not be reattached to the metadata objects. Example metadata: labels: environment: \"stating\" Annotations You can use annotations to attach arbitrary non-identifying metadata to objects.\nAnnotations are key/value maps:\nmetadata: annotations: key1: \"value-1\" key2: \"value-2\" Note The keys in the map must be string, but the values can be of any scalar types (string, boolean, or numeric). Built-in Annotations jikkou.io/ignore Used on: All Objects.\nThis annotation indicates whether the object should be ignored for reconciliation.\njikkou.io/bypass-validations Used on: All Objects.\nThis annotation indicates whether the object should bypass the validation chain. In other words, no validations will be applied on the object.\njikkou.io/delete Used on: All Objects.\nThis annotation indicates (when set to true) that the object should be deleted from your system.\njikkou.io/resource-location Used by jikkou.\nThis annotation is automatically added by Jikkou to an object when loaded from your local filesystem.\njikkou.io/items-count Used by jikkou.\nThis annotation is automatically added by Jikkou to an object collection grouping several resources of homogeneous type.\n","categories":"","description":"","excerpt":"Labels You can use labels to attach arbitrary identifying metadata to ‚Ä¶","ref":"/docs/concepts/labels-and-annotations/","tags":"","title":"Labels and annotations"},{"body":" Here, you will find the list of resources supported by the extensions for Aiven.\nAiven for Apache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/docs/providers/aiven/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported by the Kafka Connect Extension.\nKafka Connect Resources More information:\n","categories":"","description":"Learn how to use the resources provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the resources provided by the Kafka Connect ‚Ä¶","ref":"/docs/providers/kafka-connect/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nApache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the built-in resources provided by the Extension ‚Ä¶","ref":"/docs/providers/kafka/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Schema Registry.\nSchema Registry Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/docs/providers/schema-registry/resources/","tags":"","title":"Resources"},{"body":" Here, you will find information to use the built-in transformations for Apache Kafka resources.\nMore information:\n","categories":"","description":"Learn how to use the built-in transformation provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the built-in transformation provided by the Extension ‚Ä¶","ref":"/docs/providers/kafka/transformations/","tags":"","title":"Transformations"},{"body":" Try the tutorials for common Jikkou tasks and use cases.\n","categories":"","description":"Learn common Jikkou tasks and use cases.\n","excerpt":"Learn common Jikkou tasks and use cases.\n","ref":"/docs/tutorials/","tags":"","title":"Jikkou Tutorials"},{"body":"Enable Security To enable secure access to the API Server:\nConfiguration File Update the configuration file (i.e., application.yaml) of the server with:\nmicronaut: security: enabled: true Environment Variable As an alternative, you can set the following environment variable MICRONAUT_SECUTIRY_ENABLED=true.\nNote For more information about how Micronaut binds environment variables and configuration property: https://docs.micronaut.io/latest/guide/index.html#_property_value_binding). Unauthorized Access When accessing a secured path, the server will return the following response if access is not authorized:\n{ \"message\": \"Unauthorized\", \"errors\": [ { \"status\": 401, \"error_code\": \"authentication_user_unauthorized\", \"message\": \"Unauthorized\" } ] } ","categories":"","description":"Learn how to secure access to Jikkou API server.\n","excerpt":"Learn how to secure access to Jikkou API server.\n","ref":"/docs/jikkou-api-server/configuration/authentication/","tags":"","title":"Authentication"},{"body":"","categories":"","description":"Learn automating Jikkou\n","excerpt":"Learn automating Jikkou\n","ref":"/docs/jikkou-cli/automating/","tags":"","title":"Automating Jikkou"},{"body":"Jikkou API Server can be secured using a Basic HTTP Authentication Scheme.\nRFC7617 defines the ‚ÄúBasic‚Äù Hypertext Transfer Protocol (HTTP) authentication scheme, which transmits credentials as user-id/password pairs, encoded using Base64.\nBasic Authentication should be used over a secured connection using HTTPS.\nConfigure Basic HTTP Authentication Step1: Enable security Add the following configuration to your server configuration.\n# ./etc/application.yaml micronaut: security: enabled: true Step2: Configure the list of users The list of username/password authorized to connect to the API server can be configured as follows:\n# ./etc/application.yaml jikkou: security: basic-auth: - username: \"admin\" password: \"{noop}password\" For production environment, password must not be configured in plaintext. Password can be passed encoded in bcrypt, scrypt, argon2, and sha256.\nExample echo -n password | sha256sum 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8 # ./etc/application.yaml jikkou: security: basic-auth: - username: \"admin\" password: \"{sha256}5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\" Step3: Validate authentication Encode credentials\necho -n \"admin:password\" | base64 YWRtaW46cGFzc3dvcmQ= Send request\ncurl -IX GET http://localhost:28082/apis/kafka.jikkou.io/v1beta2/kafkabrokers \\ -H \"Accept: application/json\" \\ -H \"Authorization: Basic YWRtaW46cGFzc3dvcmQ\" HTTP/1.1 200 OK Content-Type: application/hal+json content-length: 576 ","categories":"","description":"Learn how to secure Jikkou API Server using Basic HTTP Authentication Scheme.\n","excerpt":"Learn how to secure Jikkou API Server using Basic HTTP Authentication ‚Ä¶","ref":"/docs/jikkou-api-server/configuration/authentication/basic_auth/","tags":"","title":"Basic Auth"},{"body":" This section covers the core classes to develop action extensions.\nInterface To create a custom action, you will need to implement the Java interface: io.streamthoughts.jikkou.core.action.Action.\n/** * Interface for executing a one-shot action on a specific type of resources. * * @param \u003cT\u003e The type of the resource. */ @Category(ExtensionCategory.ACTION) public interface Action\u003cT extends HasMetadata\u003e extends HasMetadataAcceptable, Extension { /** * Executes the action. * * @param configuration The configuration * @return The ExecutionResultSet */ @NotNull ExecutionResultSet\u003cT\u003e execute(@NotNull Configuration configuration); } Examples The Action class below shows how to implement a custom action accepting options`.\n@Named(EchoAction.NAME) @Title(\"Print the input.\") @Description(\"The EchoAction allows printing the text provided in input.\") @ExtensionSpec( options = { @ExtensionOptionSpec( name = INPUT_CONFIG_NAME, description = \"The input text to print.\", type = String.class, required = true ) } ) public final class EchoAction extends ContextualExtension implements Action\u003cHasMetadata\u003e { public static final String NAME = \"EchoAction\"; public static final String INPUT_CONFIG_NAME = \"input\"; @Override public @NotNull ExecutionResultSet\u003cHasMetadata\u003e execute(@NotNull Configuration configuration) { String input = extensionContext().\u003cString\u003econfigProperty(INPUT_CONFIG_NAME).get(configuration); return ExecutionResultSet .newBuilder() .result(ExecutionResult .newBuilder() .status(ExecutionStatus.SUCCEEDED) .data(new EchoOut(input)) .build()) .build(); } @Kind(\"EchoOutput\") @ApiVersion(\"core.jikkou.io/v1\") @Reflectable record EchoOut(@JsonProperty(\"out\") String out) implements HasMetadata { @Override public ObjectMeta getMetadata() { return new ObjectMeta(); } @Override public HasMetadata withMetadata(ObjectMeta objectMeta) { throw new UnsupportedOperationException(); } } } ","categories":"","description":"Learn how to develop custom actions.\n","excerpt":"Learn how to develop custom actions.\n","ref":"/docs/developer-guide/extensions/action/","tags":"","title":"Develop Custom Action"},{"body":" This section covers the core classes to develop transformation extensions.\nInterface To create a custom transformation, you will need to implement the Java interface: io.streamthoughts.jikkou.core.transformation.Transformation.\n/** * This interface is used to transform or filter resources. * * @param \u003cT\u003e The resource type supported by the transformation. */ public interface Transformation\u003cT extends HasMetadata\u003e extends Interceptor { /** * Executes the transformation on the specified {@link HasMetadata} object. * * @param resource The {@link HasMetadata} to be transformed. * @param resources The {@link ResourceListObject} involved in the current operation. * @param context The {@link ReconciliationContext}. * @return The list of resources resulting from the transformation. */ @NotNull Optional\u003cT\u003e transform(@NotNull T resource, @NotNull HasItems resources, @NotNull ReconciliationContext context); } Examples The transformation class below shows how to filter resource having an annotation exclude: true.\nimport java.util.Optional; @Named(\"ExcludeIgnoreResource\") @Title(\"ExcludeIgnoreResource allows filtering resources whose 'metadata.annotations.ignore' property is equal to 'true'\") @Description(\"The ExcludeIgnoreResource transformation is used to exclude from the\" + \" reconciliation process any resource whose 'metadata.annotations.ignore'\" + \" property is equal to 'true'. This transformation is automatically enabled.\" ) @Enabled @Priority(HasPriority.HIGHEST_PRECEDENCE) public final class ExcludeIgnoreResourceTransformation implements Transformation\u003cHasMetadata\u003e { /** {@inheritDoc}**/ @Override public @NotNull Optional\u003cHasMetadata\u003e transform(@NotNull HasMetadata resource, @NotNull HasItems resources, @NotNull ReconciliationContext context) { return Optional.of(resource) .filter(r -\u003e HasMetadata.getMetadataAnnotation(resource, \"ignore\") .map(NamedValue::getValue) .map(Value::asBoolean) .orElse(false) ); } } ","categories":"","description":"Learn how to develop custom resource transformations.\n","excerpt":"Learn how to develop custom resource transformations.\n","ref":"/docs/developer-guide/extensions/transformation/","tags":"","title":"Develop Custom Transformations"},{"body":"Setup Jikkou The streamthoughts/setup-jikkou action is a JavaScript action that sets up Jikkou in your GitHub Actions workflow by:\nDownloading a specific version of Jikkou CLI and adding it to the PATH. Configuring JIKKOU CLI with a custom configuration file. After you‚Äôve used the action, subsequent steps in the same job can run arbitrary Jikkou commands using the GitHub Actions run syntax. This allows most Jikkou commands to work exactly like they do on your local command line.\nUsage steps: - uses: streamthoughts/setup-jikkou@v1 A specific version of Jikkou CLI can be installed:\nsteps: - uses: streamthoughts/setup-jikkou@v0.1.0 with: jikkou_version: 0.29.0 A custom configuration file can be specified:\nsteps: - uses: streamthoughts/setup-jikkou@v0.1.0 with: jikkou_config: ./config/jikkouconfig.json Inputs This Action additionally supports the following inputs :\nProperty Default Description jikkou_version latest The version of Jikkou CLI to install. A value of latest will install the latest version of Jikkou CLI. jikkou_config The path to the Jikkou CLI config file. If set, Jikkou CLI will be configured through the JIKKOUCONFIG environment variable. ","categories":"","description":"Learn Jikkou Setup Github Action in your CI/CD Workflows\n","excerpt":"Learn Jikkou Setup Github Action in your CI/CD Workflows\n","ref":"/docs/jikkou-cli/automating/githubactions/","tags":"","title":"Automate Jikkou with GitHub Actions"},{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\n","categories":"","description":"Learn Jikkou's CLI-based workflows.\n","excerpt":"Learn Jikkou's CLI-based workflows.\n","ref":"/docs/jikkou-cli/","tags":"","title":"Jikkou CLI Documentation"},{"body":"Jikkou API Server can be secured using JWT (JSON Web Token) Authentication.\nConfigure JWT Step1: Set JWT signature secret Add the following configuration to your server configuration.\n# ./etc/application.yaml micronaut: security: enabled: true authentication: bearer \u003c1\u003e token: enabled: true jwt: signatures: secret: generator: secret: ${JWT_GENERATOR_SIGNATURE_SECRET:pleaseChangeThisSecretForANewOne} \u003c2\u003e \u003c1\u003e Set authentication to bearer to receive a JSON response from the login endpoint. \u003c2\u003e Change this to your own secret and keep it safe (do not store this in your VCS). Step2: Generate a Token Generate a valid JSON Web Token on https://jwt.io/ using your secret.\nExample with pleaseChangeThisSecretForANewOne as signature secret.\nTOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.6cD3MnZmX2xyEAWyh-GgGD11TX8SmvmHVLknuAIJ8yE Step3: Validate authentication $ curl -I -X GET http://localhost:28082/apis/kafka.jikkou.io/v1beta2/kafkabrokers \\ -H \"Accept: application/json\" \\ -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.6cD3MnZmX2xyEAWyh-GgGD11TX8SmvmHVLknuAIJ8yE\" HTTP/1.1 200 OK Content-Type: application/hal+json content-length: 576 ","categories":"","description":"Learn how to secure Jikkou API Server using JWT (JSON Web Token) Authentication.\n","excerpt":"Learn how to secure Jikkou API Server using JWT (JSON Web Token) ‚Ä¶","ref":"/docs/jikkou-api-server/configuration/authentication/jwt/","tags":"","title":"JWT"},{"body":" In the context of Jikkou, reconciliation refers to the process of comparing the desired state of an object with the actual state of the system and making any necessary corrections or adjustments to align them.\nChanges A Change represents a difference, detected during reconciliation, between two objects that can reconciled or corrected by adding, updating, or deleting an object or property attached to the actual state of the system.\nA Change represents a detected difference between two objects during the reconciliation process. These differences can be reconciled or corrected by adding, updating, or deleting an object or property associated with the actual state of the system\nJikkou identifies four types of changes:\nADD: Indicates the addition of a new object or property to an existing object.\nUPDATE: Indicates modifications made to an existing object or property of an existing object.\nDELETE: Indicates the removal of an existing object or property of an existing object.\nNONE: Indicates that no changes were made to an existing object or property.\nReconciliation Modes Depending on the chosen reconciliation mode, only specific types of changes will be applied.\nJikkou provides four distinct reconciliation modes that determine the types of changes to be applied:\nCREATE: This mode only applies changes that create new resource objects in your system. DELETE: This mode only applies changes that delete existing resource objects in your system. UPDATE: This mode only applies changes that create or update existing resource objects in your system. APPLY_ALL: This mode applies all changes to ensure that the actual state of a resource in the cluster matches the desired state defined in your resource definition file, regardless of the specific type of change. Each mode corresponds to a command offered by the Jikkou CLI (i.e., create, update, delete, and apply). Choose the appropriate mode based on your requirements.\nUsing JIKKOU CLI Some reconciliation modes might not be supported for all resources. Use jikkou extensions list --type Controller to check which actions could be perfomed for each resources. Reconciliation Options Depending on the type of resources being reconciled, the controller that will be involved in the reconciliation process might accept some options (i.e., using --options argument).\nMark Resource for Deletion To delete all the states associated with resource‚Äôs entities, you must add the following annotation to the resource definition:\nmetadata: annotations: jikkou.io/delete: true ","categories":"","description":"","excerpt":" In the context of Jikkou, reconciliation refers to the process of ‚Ä¶","ref":"/docs/concepts/reconciliation/","tags":["concept"],"title":"Reconciliation"},{"body":"Configuration Step 1: Enable Proxy Mode To enable proxy mode so that the CLI communicates directly with your API Server, add the following parameters to your configuration:\njikkou { # Proxy Configuration proxy { # Specify whether proxy mode is enabled (default: false). enabled = true # URL of the API Server url = \"http://localhost:28082\" # Specifcy whether HTTP request debugging should be enabled (default: false) debugging = false # The connect timeout in millisecond (if not configured used ` default-timeout` ). connect-timeout = 10000 # The read timeout in millisecond (if not configured used ` default-timeout` ). read-timeout = 10000 # The write timeout in millisecond (if not configured used ` default-timeout` ). write-timeout = 10000 # The default timeout (i.e., for read/connect) in millisecond (default: 10000) default-timeout = 10000 # Security settings to authenticate to the API Server. security = { # For Token based Authentication. # access-token = \"\" # For Username/Password Basic-Authentication. # basic-auth = { # username = \"\" # password = \"\" # } } } } Step 2: Check connection When enabling Proxy Mode, Jikkou CLI provides the additional command server-info. You can use it to verify the connectivity with teh server.\n$ jikkou server-info -o JSON | jq { \"version\": \"0.31.0\", \"build_time\": \"2023-11-15T10:35:22+0100\", \"commit_id\": \"f3384d38e606fb32599c175895d0cbef28258540\" } ","categories":"","description":"Learn how to configure Jikkou CLI in proxy mode.\n","excerpt":"Learn how to configure Jikkou CLI in proxy mode.\n","ref":"/docs/jikkou-api-server/configuration/cli_proxy_mode/","tags":"","title":"CLI Proxy Mode"},{"body":"Jikkou API Server provides a REST interface to any platform supported by Jikkou, making it even easier to manage, automate and visualise all your data platform assets.\nJikkou CLI can be used in combination with Jikkou API Server by configuring it in proxy mode. In this mode, the CLI no longer connects directly to your various platforms, but forwards all operations to the API server. This deployment method allows you to enhance the overall security of the platforms managed through Jikkou.\n","categories":"","description":"Learn Jikkou's API Server usages.\n","excerpt":"Learn Jikkou's API Server usages.\n","ref":"/docs/jikkou-api-server/","tags":"","title":"Jikkou API Server Documentation"},{"body":" Selectors allows you to include or exclude some resource objects from being returned or reconciled by Jikkou.\nSelector Expressions Selectors are passed as arguments to Jikkou as expression strings in the following form:\n\u003cSELECTOR\u003e: \u003cKEY\u003e \u003cOPERATOR\u003e VALUE \u003cSELECTOR\u003e: \u003cKEY\u003e \u003cOPERATOR\u003e (VALUE[, VALUES]) or (using default field selector):\n\u003cKEY\u003e \u003cOPERATOR\u003e VALUE \u003cKEY\u003e \u003cOPERATOR\u003e (VALUE[, VALUES]) Selectors Field (default) Jikkou packs with a built-in FieldSelector allowing to filter resource objects based on a field key.\nFor example, the expression below shows you how to select only resource having a label environement equals to either staging or production.\nmetadata.labels.environement IN (staging, production) Note: In the above example, we have omitted the selector because field is the default selector.\nExpression Operators Five kinds of operators are supported:\nIN NOTIN EXISTS MATCHES DOESNOTMATCH Using JIKKOU CLI Selectors can be specified via the Jikkou CLI option: --selector. Matching Strategies Jikkou allows you to use multiple selector expressions. To indicate how these expressions are to be combined, you can pass one of the following matching strategies:\nALL: A resource is selected if it matches all selectors. ANY: A resource is selected if it matches one of the selectors. NONE: A resource is selected if it matches none of the selectors. Example:\njikkou get kafkatopics \\ --selector 'metadata.name MATCHES (^__.*)' \\ --selector 'metadata.name IN (_schemas)' \\ --selector-match ANY ","categories":"","description":"","excerpt":" Selectors allows you to include or exclude some resource objects from ‚Ä¶","ref":"/docs/concepts/selectors/","tags":["concept","feature"],"title":"Selectors"},{"body":" This section explains key concepts used within Jikkou:\n","categories":"","description":"Learn the differents concepts used within Jikkou\n","excerpt":"Learn the differents concepts used within Jikkou\n","ref":"/docs/concepts/","tags":"","title":"Concepts"},{"body":" The section helps you learn more about the built-in Extension Providers for Jikkou.\n","categories":"","description":"Learn how to use Jikkou Extension Provider to provision and manage configuration assets on your data infrastructure.\n","excerpt":"Learn how to use Jikkou Extension Provider to provision and manage ‚Ä¶","ref":"/docs/providers/","tags":["how-to","docs"],"title":"Extension Providers"},{"body":" Transformations are applied to inbound resources. Transformations are used to transform, enrich, or filter resource entities before they are validated and thus before the reconciliation process is executed on them.\nAvailable Transformations You can list all the available transformations using the Jikkou CLI command:\njikkou extensions list --type=Transformation [-kinds \u003ca resource kind to filter returned results\u003e] Transformation chain When using Jikkou CLI, you can configure a transformation chain that will be applied to every resource. This chain consists of multiple transformations, each designed to handle different types of resources. Jikkou ensures that a transformation is executed only for the resource types it supports. In cases where a resource is not accepted by a transformation, it is passed to the next transformation in the chain. This process continues until a suitable transformation is found or until all transformations have been attempted.\nConfiguration jikkou { # The list of transformations to execute transformations: [ { # Simple or fully qualified class name of the transformation extension. type = \"\" # Priority to be used for executing this transformation extension. # The lowest value has the highest priority, so it's run first. Minimum value is -2^31 (highest) and a maximum value is 2^31-1 (lowest). # Usually, values under 0 should be reserved for internal transformation extensions. priority = 0 config = { # Configuration properties for this transformation } } ] } Tips The config object of a Transformation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute transformations: [ { # Enforce a minimum number of replicas for a kafka topic type = KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 4 } }, { # Enforce a {@code min.insync.replicas} for a kafka topic. type = KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" Transformations are applied to inbound resources. Transformations are ‚Ä¶","ref":"/docs/concepts/transformations/","tags":["concept","feature","extension"],"title":"Transformations"},{"body":" Here, you will find the necessary information to develop with the Jikkou API.\nMore information:\n","categories":"","description":"Learn how to use the Jikkou Core API\n","excerpt":"Learn how to use the Jikkou Core API\n","ref":"/docs/developer-guide/","tags":"","title":"Developer Guide"},{"body":" Validations are applied to inbound resources to ensure that the resource entities adhere to specific rules or constraints. These validations are carried out after the execution of the transformation chain and before the reconciliation process takes place.\nAvailable Validations You can list all the available validations using the Jikkou CLI command:\njikkou api-extensions list --category=validation [--kinds \u003ca resource kind to filter returned results\u003e] Validation chain When using Jikkou CLI, you can configure a validation chain that will be applied to every resource. This chain consists of multiple validations, each designed to handle different types of resources. Jikkou ensures that a validation is executed only for the resource types it supports. In cases where a resource is not accepted by a validation, it is passed to the next validation in the chain. This process continues until a suitable validation is found or until all validations have been attempted.\nConfiguration jikkou { # The list of validations to execute validations: [ { # Custom name for the validation rule name = \"\" # Simple or fully qualified class name of the validation extension. type = \"\" config = { # Configuration properties for this validation } } ] } Tips The config object of a Validation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute validations: [ { # Custom name for the validation rule name = topicMustBePrefixedWithRegion # Simple or fully qualified class name of the validation extension. type = TopicNameRegexValidation # The config values that will be passed to the validation. config = { topicNameRegex = \"(europe|northamerica|asiapacific)-.+\" } } ] } ","categories":"","description":"","excerpt":" Validations are applied to inbound resources to ensure that the ‚Ä¶","ref":"/docs/concepts/validations/","tags":["concept","feature","extension"],"title":"Validations"},{"body":" Template helps you to dynamically define resource definition files from external data.\nTemplate Engine Jikkou provides a simple templating mechanism based-on Jinjava, a Jinja template engine for Java.\nRead the official documentation of Jinja to learn more about the syntax and semantics of the template engine.\nHow Does It Work ? Jikkou performs the rendering of your template in two phases:\nFirst, an initial rendering is performed using only the values and labels passed through the command-lines arguments. Thus, it is perfectly OK if your resource file is not initially a valid YAML file. Then, a second and final rendering is performed after parsing the YAML resource file using the additional values and labels as defined into the YAML resource file. Therefore, it‚Äôs important that your resource file is converted into a valid YAML file after the first rendering. Important You should use {% raw %}...{% endraw %} tags to ensure the variables defined into the template are not be interpreted during the first rendering. Variables Jikkou defines a number of top-level variables that are passed to the template engine.\nvalues:\nThe values passed into the template through the command-line --values-files and/or --set-value arguments In addition, values can be defined into the application.conf file and directly into the template file using the property template.values. By default, values is empty. labels:\nThe labels passed into the template through the command-line argument: --set-label. In addition, labels can be defined into the template file using the property metadata.labels. By default, labels is empty. system.env:\nThis provides access to all environment variables. system.props:\nThis provides access to all system properties. Template Values When using templating, a resource definition file may contain the additional property template. fields:\napiVersion: The api version (required) kind: The resource kind (required) metadata: labels: The set of key/value pairs that you can use to describe your resource file (optional) annotations: The set of key/value pairs automatically generated by the tool (optional) template: values: The set of key/value pairs to be passed to the template engine (optional) spec: Specification of the resource Values Data File Values Data File are used to define all the necessary values (i.e., the variables) to be used for generating a template.\nExample # file: ./values.yaml topicConfigs: partitions: 4 replicas: 3 topicPrefix: \"{{ system.env.TOPIC_PREFIX | default('test', true) }}\" countryCodes: - fr - be - de - es - uk - us Template Resource File Example # file: ./kafka-topics.tpl apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaTopicList' items: { % for country in values.countryCodes % } - metadata: name: \"{{ values.topicPrefix}}-iot-events-{{ country }}\" spec: partitions: { { values.topicConfigs.partitions } } replicas: { { values.topicConfigs.replicas } } configMapRefs: - TopicConfig { % endfor % } --- apiVersion: \"core.jikkou.io/v1beta2\" kind: \"ConfigMap\" metadata: name: TopicConfig template: values: default_min_insync_replicas: \"{{ values.topicConfigs.replicas | default(3, true) | int | add(-1) }}\" data: retention.ms: 3600000 max.message.bytes: 20971520 min.insync.replicas: '{% raw %}{{ values.default_min_insync_replicas }}{% endraw %}' Command\n$ TOPIC_PREFIX=local jikkou validate --files topics.tpl --values-files values.yaml (Output)\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: labels: { } annotations: jikkou.io/resource-location: \"file:///tmp/jikkou/topics.tpl\" spec: topics: - metadata: name: \"local-iot-events-fr\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-be\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-de\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-es\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-uk\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-us\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 Configuration jinja { # Enable/Disable recursive macro calls for rendering enableRecursiveMacroCalls = false } ","categories":"","description":"","excerpt":" Template helps you to dynamically define resource definition files ‚Ä¶","ref":"/docs/concepts/template/","tags":["concept","feature"],"title":"Template"},{"body":" Collectors are used to collect and describe all entities that exist into your system for a specific resource type.\nAvailable Collectors You can list all the available collectors using the Jikkou CLI command:\njikkou extensions list --type=Collector [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Collectors are used to collect and describe all entities that exist ‚Ä¶","ref":"/docs/concepts/collector/","tags":["concept","feature","extension"],"title":"Collectors"},{"body":" Controllers are used to compute and apply changes required to reconcile resources into a managed system.\nAvailable Controllers You can list all the available controllers using the Jikkou CLI command:\njikkou extensions list --type=Controller [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Controllers are used to compute and apply changes required to ‚Ä¶","ref":"/docs/concepts/controller/","tags":["concept","feature","extension"],"title":"Controllers"},{"body":" The KafkaTopicAclEntry resources are used to manage the Access Control Lists in Aiven for Apache Kafka¬Æ. A KafkaTopicAclEntry resource defines the permission to be granted to a user for one or more kafka topics.\nKafkaTopicAclEntry Specification Here is the resource definition file for defining a KafkaTopicAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE, READWRITE, ADMIN username: \u003c\u003e # The username topic: \u003c\u003e # Topic name or glob pattern Example Here is a simple example that shows how to define a single ACL entry using the KafkaTopicAclEntry resource type.\nfile: kafka-topic-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntry\" metadata: labels: { } annotations: { } spec: permission: \"READWRITE\" username: \"alice\" topic: \"public-*\" KafkaTopicAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a KafkaTopicAclEntryList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntryList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaTopicAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaTopicAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntryList\" items: - spec: permission: \"READWRITE\" username: \"alice\" topic: \"public-*\" - spec: permission: \"READ\" username: \"bob\" topic: \"public-*\" ","categories":"","description":"Learn how to manage Access Control Lists (ACLs) in Aiven for Apache Kafka¬Æ\n","excerpt":"Learn how to manage Access Control Lists (ACLs) in Aiven for Apache ‚Ä¶","ref":"/docs/providers/aiven/resources/kafka-topic-acl/","tags":["feature","resources"],"title":"ACL for Aiven Apache Kafka¬Æ"},{"body":" This section describes the resource definition format for kafkabrokers entities, which can be used to define the brokers you plan to manage on a specific Kafka cluster.\nListing KafkaBroker You can retrieve the state of Kafka Consumer Groups using the jikkou get kafkabrokers (or jikkou get kb) command.\nUsage Usage: Get all 'KafkaBroker' resources. jikkou get kafkabrokers [-hV] [--default-configs] [--dynamic-broker-configs] [--list] [--static-broker-configs] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkabrokers when you want to describe the state of all resources of type 'KafkaBroker'. OPTIONS: --default-configs Describe built-in default configuration for configs that have a default value. --dynamic-broker-configs Describe dynamic configs that are configured as default for all brokers or for specific broker in the cluster. -h, --help Show this help message and exit. --list Get resources as ResourceListObject. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. --static-broker-configs Describe static configs provided as broker properties at start up (e.g. server.properties file). -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kafkabrokers --static-broker-configs (output)\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaBroker\" metadata: name: \"101\" labels: {} annotations: kafka.jikkou.io/cluster-id: \"xtzWWN4bTjitpL3kfd9s5g\" spec: id: \"101\" host: \"localhost\" port: 9092 configs: advertised.listeners: \"PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\" authorizer.class.name: \"org.apache.kafka.metadata.authorizer.StandardAuthorizer\" broker.id: \"101\" controller.listener.names: \"CONTROLLER\" controller.quorum.voters: \"101@kafka:29093\" inter.broker.listener.name: \"PLAINTEXT\" listener.security.protocol.map: \"CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\" listeners: \"PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092,CONTROLLER://kafka:29093\" log.dirs: \"/var/lib/kafka/data\" node.id: \"101\" offsets.topic.replication.factor: \"1\" process.roles: \"broker,controller\" transaction.state.log.replication.factor: \"1\" zookeeper.connect: \"\" ","categories":"","description":"Learn how to manage Kafka Brokers.\n","excerpt":"Learn how to manage Kafka Brokers.\n","ref":"/docs/providers/kafka/resources/brokers/","tags":["feature","resources"],"title":"Kafka Brokers"},{"body":" This section describes the resource definition format for KafkaConsumerGroup entities, which can be used to define the consumer groups you plan to manage on a specific Kafka cluster.\nListing KafkaConsumerGroup You can retrieve the state of Kafka Consumer Groups using the jikkou get kafkaconsumergroups (or jikkou get kcg) command.\nUsage $ jikkou get kafkaconsumergroups --help Usage: Get all 'KafkaConsumerGroup' resources. jikkou get kafkaconsumergroups [-hV] [--list] [--offsets] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--in-states=PARAM]... [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkaconsumergroups when you want to describe the state of all resources of type 'KafkaConsumerGroup'. OPTIONS: -h, --help Show this help message and exit. --in-states=PARAM If states is set, only groups in these states will be returned. Otherwise, all groups are returned. This operation is supported by brokers with version 2.6.0 or later --list Get resources as ResourceListObject. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). --offsets Specify whether consumer group offsets should be described. -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. -V, --version Print version information and exit (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kafkaconsumergroups --in-states STABLE --offsets (output)\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false status: state: \"STABLE\" members: - memberId: \"console-consumer-b103994e-bcd5-4236-9d03-97065057e594\" clientId: \"console-consumer\" host: \"/127.0.0.1\" assignments: - \"my-topic-0\" offsets: - topic: \"my-topic\" partition: 0 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 ","categories":"","description":"Learn how to manage Kafka Consumer Groups.\n","excerpt":"Learn how to manage Kafka Consumer Groups.\n","ref":"/docs/providers/kafka/resources/consumer_groups/","tags":["feature","resources"],"title":"Kafka Consumer Groups"},{"body":" Here, you will find information to use the Core extensions.\nMore information:\n","categories":"","description":"The core Extensions for Jikkou\n","excerpt":"The core Extensions for Jikkou\n","ref":"/docs/providers/core/","tags":"","title":"Core"},{"body":"Prerequisites Jdk 17 (see https://sdkman.io/ for installing java locally) Git Docker and Docker-Compose Your favorite IDE Building Jikkou We use Maven Wrapper to build our project. The simplest way to get started is:\nFor building distribution files.\n$ ./mvnw clean package -Pdist -DskipTests Alternatively, we also use Make to package and build the Docker image for Jikkou:\n$ make Running tests For running all tests and checks:\n$ ./mvnw clean verify Code Format This project uses the Maven plugin Spotless to format all Java classes and to apply some code quality checks.\nBugs \u0026 Security This project uses the Maven plugin SpotBugs and FindSecBugs to run some static analysis to look for bugs in Java code.\nReported bugs can be analysed using SpotBugs GUI:\n$ ./mvnw spotbugs:gui ","categories":"","description":"How to set up your environment for developing on Jikkou.\n","excerpt":"How to set up your environment for developing on Jikkou.\n","ref":"/docs/community/developer-guide/","tags":"","title":"Developer Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Documentation"},{"body":"Extension Providers Most of the Jikkou‚Äôs features are provided by Jikkou Extension Providers. A provider is a module providing a set of extensions used to manage one or more resources.\nBuilt-in Extension Providers Jikkou ships with a number of extension providers:\nAiven Apache Kafka Core Kafka Connect Schema Registry ","categories":"","description":"","excerpt":"Extension Providers Most of the Jikkou‚Äôs features are provided by ‚Ä¶","ref":"/docs/concepts/extensions/","tags":["feature","extensions"],"title":"Extensions"},{"body":" This section regroups all frequently asked questions about Jikkou.\nIs Jikkou Free to Use? Yes, Jikkou is developed and distributed under the Apache License 2.0.\nCan I Use Jikkou with Any Kafka Implementation? Yes, Jikkou can be used with a wide range of Apache Kafka infrastructures, including:\nApache Kafka Aiven Amazon MSK Confluent Cloud Redpanda Why would I use Jikkou over Terraform? What is Terraform and how is it typically used? Terraform (OpenToFu) is widely recognized as the leading solution for infrastructure provisioning and management. It is commonly used by operations teams for managing cloud infrastructure through its HCL (HashiCorp Configuration Language) syntax.\nWhat are the limitations of Terraform for Kafka Users ? Many development teams find Terraform challenging to use because:\nThey need to learn HCL syntax, which is not commonly known among developers. They often lack the necessary permissions to apply configuration files directly. They often struggle with Terraform states. How does Jikkou address these limitations? Jikkou is designed to be a straightforward CLI tool for both developers and operations teams. It simplifies the process of managing infrastructure, especially for development teams who may not have expertise in HCL or the permissions required for Terraform.\nWhat are the benefits of using Jikkou for Kafka management? On-Premises and Multi-Cloud Support: Unlike many Terraform providers which focus on cloud-based Kafka services ( e.g., Confluent Cloud), Jikkou supports on-premises, multi-cloud, and hybrid infrastructures.\nVersatility: Jikkou can manage Kafka topics across various environments, including local Kafka clusters in Docker, ephemeral clusters in Kubernetes for CI/CD, and production clusters in Aiven Cloud.\nAuditing and Backup: Beyond provisioning, Jikkou can audit Kafka platforms for configuration issues and create backups of Kafka configurations (Topics, ACLs, Quotas, etc.).\nThere are, of course, many reasons to use Terraform rather than Jikkou and vice versa. As usual, the choice of tool really depends on your needs, the organization you‚Äôre in, the skills of the people involved and so on.\n","categories":"","description":"","excerpt":" This section regroups all frequently asked questions about Jikkou.\nIs ‚Ä¶","ref":"/docs/frequently-asked-questions/","tags":"","title":"Frequently Asked Questions"},{"body":" This section describes the resource definition format for KafkaConnector entities, which can be used to define the configuration and status of connectors you plan to create and manage on specific Kafka Connect clusters.\nDefinition Format of KafkaConnector Below is the overall structure of the KafkaConnector resource.\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" # The api version (required) kind: \"KafkaConnector\" # The resource kind (required) metadata: name: \u003cstring\u003e # The name of the connector (required) labels: # Name of the Kafka Connect cluster to create the connector instance in (required). kafka.jikkou.io/connect-cluster: \u003cstring\u003e annotations: # Override client properties to connect to Kafka Connect cluster (optional). jikkou.io/config-override: | \u003cjson\u003e spec: connectorClass: \u003cstring\u003e # Name or alias of the class for this connector. tasksMax: \u003cinteger\u003e # The maximum number of tasks for the Kafka Connector. config: # Configuration properties of the connector. \u003ckey\u003e: \u003cvalue\u003e state: \u003cstring\u003e # The state the connector should be in. Defaults to running. See below for details about all these fields.\nMetadata metadata.name [required] The name of the connector.\nlabels.kafka.jikkou.io/connect-cluster [required] The name of the Kafka Connect cluster to create the connector instance in. The cluster name must be configured through the kafkaConnect.clusters[] Jikkou‚Äôs configuration setting (see: Configuration).\njikkou.io/config-override: [optional] The JSON client configurations to override for connecting to the Kafka Connect cluster. The configuration properties passed through this annotation override any cluster properties defined in the Jikkou‚Äôs configuration setting (see: Configuration).\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"my-connector\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: jikkou.io/config-override: | { \"url\": \"http://localhost:8083\" } Specification spec.connectorClass [required] The name or alias of the class for this connector.\nspec.tasksMax [optional] The maximum number of tasks for the Kafka Connector. Default is 1.\nspec.config [required] The connector‚Äôs configuration properties.\nspec.state [optional] The state the connector should be in. Defaults to running.\nBelow are the valid values:\nrunning: Transition the connector and its tasks to RUNNING state. paused: Pause the connector and its tasks, which stops message processing until the connector is resumed. stopped: Completely shut down the connector and its tasks. The connector config remains present in the config topic of the cluster (if running in distributed mode), unmodified. Examples The following is an example of a resource describing a Kafka connector:\n--- # Example: file: kafka-connector-filestream-sink.yaml apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" Listing KafkaConnector You can retrieve the state of Kafka Connector instances running on your Kafka Connect clusters using the jikkou get kafkaconnectors (or jikkou get kc) command.\nUsage $jikkou get kc --help Usage: Get all 'KafkaConnector' resources. jikkou get kafkaconnectors [-hV] [--expand-status] [-o=\u003cformat\u003e] [-s=\u003cexpressions\u003e]... Description: Use jikkou get kafkaconnectors when you want to describe the state of all resources of type 'KafkaConnector'. Options: --expand-status Retrieves additional information about the status of the connector and its tasks. -h, --help Show this help message and exit. -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). -s, --selector=\u003cexpressions\u003e The selector expression use for including or excluding resources. -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kc --expand-status (output)\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"localhost\" spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" worker_id: \"localhost:8083\" tasks: id: 1 state: \"RUNNING\" worker_id: \"localhost:8083\" The status.connectorStatus provides the connector status, as reported by the Kafka Connect REST API.\n","categories":"","description":"Learn how to manage Kafka Connectors.\n","excerpt":"Learn how to manage Kafka Connectors.\n","ref":"/docs/providers/kafka-connect/resources/connector/","tags":["feature","resources"],"title":"KafkaConnectors"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your SchemaRegistry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nDefinition Format of SchemaRegistrySubject Below is the overall structure of the SchemaRegistrySubject resource.\napiVersion: \"schemaregistry.jikkou.io/v1beta2\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: \u003cvendor_name\u003e # (optional) The vendor of the SchemaRegistry, e.g., Confluent, Karapace, etc compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003cstring\u003e # The name for the reference. subject: \u003cstring\u003e # The subject under which the referenced schema is registered. version: \u003cstring\u003e # The exact version of the schema under the registered subject. ] Metadata The metadata.name property is mandatory and specifies the name of the Subject.\nSpecification To use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } SchemaRegistrySubjectList If you need to manage multiple Schemas at once (e.g. using a template), it may be more suitable to use the resource collection SchemaRegistrySubjectList.\nSpecification Here the resource definition file for defining a SchemaRegistrySubjectList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"SchemaRegistrySubjectList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # The array of SchemaRegistrySubject ","categories":"","description":"Learn how to manage SchemaRegistry Subjects.\n","excerpt":"Learn how to manage SchemaRegistry Subjects.\n","ref":"/docs/providers/schema-registry/resources/subject/","tags":["feature","resources"],"title":"Schema Registry Subjects"},{"body":" KafkaTopic resources are used to define the topics you want to manage on your Kafka Cluster(s). A KafkaTopic resource defines the number of partitions, the replication factor, and the configuration properties to be associated to a topics.\nKafkaTopic Specification Here is the resource definition file for defining a KafkaTopic.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopic\" # The resource kind (required) metadata: name: \u003cThe name of the topic\u003e # (required) labels: { } annotations: { } spec: partitions: \u003cNumber of partitions\u003e # (optional) replicas: \u003cNumber of replicas\u003e # (optional) configs: \u003cconfig_key\u003e: \u003cConfig Value\u003e # The topic config properties keyed by name to override (optional) configMapRefs: [ ] # The list of ConfigMap to be applied to this topic (optional) The metadata.name property is mandatory and specifies the name of the kafka topic.\nTo use the cluster default values for the number of partitions and replicas you can set the property spec.partitions and spec.replicas to -1.\nExample Here is a simple example that shows how to define a single YAML file containing two topic definition using the KafkaTopic resource type.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p1-r1' # Name of the topic labels: environment: example spec: partitions: 1 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p2-r1' # Name of the topic labels: environment: example spec: partitions: 2 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' See official Apache Kafka documentation for details about the topic-level configs.\nTips: Multiple topics can be included in the same YAML file by using --- lines. KafkaTopicList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaTopicList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopicList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaTopic Example Here is a simple example that shows how to define a single YAML file containing two topic definitions using the KafkaTopicList resource type. In addition, the example uses a ConfigMap object to define the topic configuration only once.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopicList metadata: labels: environment: example items: - metadata: name: 'my-topic-p1-r1' spec: partitions: 1 replicas: 1 configMapRefs: [ \"TopicConfig\" ] - metadata: name: 'my-topic-p2-r1' spec: partitions: 2 replicas: 1 configMapRefs: [ \"TopicConfig\" ] --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'TopicConfig' data: min.insync.replicas: 1 cleanup.policy: 'delete' ","categories":"","description":"Learn how to manage Kafka Topics.\n","excerpt":"Learn how to manage Kafka Topics.\n","ref":"/docs/providers/kafka/resources/topics/","tags":["feature","resources"],"title":"Kafka Topics"},{"body":" This section describes the resource definition format for KafkaUser entities, which can be used to manage SCRAM Users for Apache Kafka.\nDefinition Format of KafkaUser Below is the overall structure of the KafkaUser resource.\n--- apiVersion: kafka.jikkou.io/v1 # The api version (required) kind: KafkaUser # The resource kind (required) metadata: name: \u003cstring\u003e annotations: # force update kafka.jikkou.io/force-password-renewal: \u003cboolean\u003e spec: authentications: - type: \u003cenum\u003e # or password: \u003cstring\u003e # leave empty to generate secure password See below for details about all these fields.\nMetadata metadata.name [required] The name of the User.\nkafka.jikkou.io/force-password-renewal [optional] Specification spec.authentications [required] The list of authentications to manage for the user.\nspec.authentications[].type [required] The authentication type:\nscram-sha-256 scram-sha-512 spec.authentications[].password [required] The password of the user.\nExamples The following is an example of a resource describing a User:\n--- # Example: file: kafka-scram-users.yaml apiVersion: \"kafka.jikkou.io/v1\" kind: \"User\" metadata: name: \"Bob\" spec: authentications: - type: scram-sha-256 password: null - type: scram-sha-512 password: null Listing Kafka Users You can retrieve the SCRAM users of a Kafka cluster using the jikkou get kafkausers (or jikkou get ku) command.\nUsage $ jikkou get kc --help Usage: Get all 'KafkaUser' resources. jikkou get kafkausers [-hV] [--list] [--logger-level=\u003clevel\u003e] [--name=\u003cname\u003e] [-o=\u003cformat\u003e] [--selector-match=\u003cselectorMatchingStrategy\u003e] [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkausers when you want to describe the state of all resources of type 'KafkaUser'. OPTIONS: -h, --help Show this help message and exit. --list Get resources as ResourceListObject (default: false). --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` --name=\u003cname\u003e The name of the resource. -o, --output=\u003cformat\u003e Prints the output in the specified format. Valid values: JSON, YAML (default: YAML). -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. --selector-match=\u003cselectorMatchingStrategy\u003e The selector matching strategy. Valid values: NONE, ALL, ANY (default: ALL) -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get ku (output)\napiVersion: \"kafka.jikkou.io/v1\" kind: \"KafkaUser\" metadata: name: \"Bob\" labels: {} annotations: kafka.jikkou.io/cluster-id: \"xtzWWN4bTjitpL3kfd9s5g\" spec: authentications: - type: \"scram-sha-256\" iterations: 8192 - type: \"scram-sha-512\" iterations: 8192 ","categories":"","description":"Learn how to manage Kafka Users.\n","excerpt":"Learn how to manage Kafka Users.\n","ref":"/docs/providers/kafka/resources/users/","tags":["feature","resources"],"title":"Kafka Users"},{"body":" Reporters can be used to report changes applied by Jikkou to a third-party system.\nConfiguration Jikkou allows you to configure multiple reporters as follows:\njikkou { # The list of reporters to execute reporters: [ { # Custom name for the reporter name = \"\" # Simple or fully qualified class name of the transformation extension. type = \"\" config = { # Configuration properties for this reporter } } ] } Tips The config object passed to a reporter will fallback on the top-level jikkou config. This allows you to globally declare some configuration settings. Built-in implementations Jikkou packs with some built-in ChangeReporter implementations:\nKafkaChangeReporter The KafkaChangeReporter can be used to send change results into a given kafka topic. Changes will be published as Cloud Events.\nConfiguration The below example shows how to configure the KafkaChangeReporter.\njikkou { # The default custom reporters to report applied changes. reporters = [ { name = \"kafka-reporter\" type = io.streamthoughts.jikkou.kafka.reporter.KafkaChangeReporter config = { # The 'source' of the event that will be generated. event.source = \"jikkou/cli\" kafka = { # If 'true', topic will be automatically created if it does not already exist. topic.creation.enabled = true # The default replication factor used for creating topic. topic.creation.defaultReplicationFactor = 1 # The name of the topic the events will be sent. topic.name = \"jikkou-resource-change-event\" # The configuration settings for Kafka Producer and AdminClient client = ${jikkou.kafka.client} { client.id = \"jikkou-reporter-producer\" } } } } ] } ","categories":"","description":"","excerpt":" Reporters can be used to report changes applied by Jikkou to a ‚Ä¶","ref":"/docs/concepts/reporters/","tags":["feature","extensions"],"title":"Reporters"},{"body":" Actions allow a user to execute a specific and one-shot operation on resources.\nAvailable Actions (CLI) You can list all the available actions using the Jikkou CLI command:\njikkou api-extensions list --category=action [-kinds \u003ca resource kind to filter returned results\u003e] Execution Actions (CLI) You can execute a specific extension using the Jikkou CLI command:\njikkou action \u003cACTION_NAME\u003e execute [\u003coptions\u003e] ","categories":"","description":"","excerpt":" Actions allow a user to execute a specific and one-shot operation on ‚Ä¶","ref":"/docs/concepts/actions/","tags":["feature","extensions"],"title":"Actions"},{"body":" Here, you will find information to use the Apache Kafka extensions.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extension Provider for Apache Kafka.\n","excerpt":"Lean how to use the Jikkou Extension Provider for Apache Kafka.\n","ref":"/docs/providers/kafka/","tags":"","title":"Apache Kafka"},{"body":" KafkaPrincipalAuthorization resources are used to define Access Control Lists (ACLs) for principals authenticated to your Kafka Cluster.\nJikkou can be used to describe all ACL policies that need to be created on Kafka Cluster\nKafkaPrincipalAuthorization Specification --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: [ ] # List of roles to be added to the principal (optional) acls: # List of KafkaPrincipalACL (required) - resource: type: \u003cThe type of the resource\u003e # (required) pattern: \u003cThe pattern to be used for matching resources\u003e # (required) patternType: \u003cThe pattern type\u003e # (required) type: \u003cThe type of this ACL\u003e # ALLOW or DENY (required) operations: [ ] # Operation that will be allowed or denied (required) host: \u003cHOST\u003e # IP address from which principal will have access or will be denied (optional) For more information on how to define authorization and ACLs, see the official Apache Kafka documentation: Security\nOperations The list below describes the valid values for the spec.acls.[].operations property :\nREAD WRITE CERATE DELETE ALTER DESCRIBE CLUSTER_ACTION DESCRIBE_CONFIGS ALTER_CONFIGS IDEMPOTENT_WRITE CREATE_TOKEN DESCRIBE_TOKENS ALL For more information see official Apache Kafka documentation: Operations in Kafka\nResource Types The list below describes the valid values for the spec.acls.[].resource.type property :\nTOPIC GROUP CLUSTER USER TRANSACTIONAL_ID For more information see official Apache Kafka documentation: Resources in Kafka\nPattern Types The list below describes the valid values for the spec.acls.[].resource.patternType property :\nLITERAL: Use to allow or denied a principal to have access to a specific resource name. MATCH: Use to allow or denied a principal to have access to all resources matching the given regex. PREFIXED: Use to allow or denied a principal to have access to all resources having the given prefix. Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalAuthorization\" # The resource kind (required) metadata: name: \"User:Alice\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: \"ALLOW\" operations: [ 'READ', 'WRITE' ] host: \"*\" - resource: type: 'topic' pattern: 'my-other-topic-.*' patternType: 'MATCH' type: 'ALLOW' operations: [ 'READ' ] host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: 'ALLOW' operations: [ 'READ', 'WRITE' ] host: \"*\" KafkaPrincipalRole Specification apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalRole\" # The resource kind (required) metadata: name: \u003cName of role\u003e # The name of the role (required) spec: acls: [ ] # A list of KafkaPrincipalACL (required) Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicRead\" spec: acls: - type: \"ALLOW\" operations: [ 'READ' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicWrite\" spec: acls: - type: \"ALLOW\" operations: [ 'WRITE' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: - \"KafkaTopicPublicRead\" - \"KafkaTopicPublicWrite\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: roles: - \"KafkaTopicPublicRead\" ","categories":"","description":"Learn how to manage Kafka Authorizations and ACLs. \n","excerpt":"Learn how to manage Kafka Authorizations and ACLs. \n","ref":"/docs/providers/kafka/resources/acls/","tags":["feature","resources"],"title":"Kafka Authorizations"},{"body":" ","categories":"","description":"What does your user need to know to try your project?\n","excerpt":"What does your user need to know to try your project?\n","ref":"/docs/community/","tags":"","title":"Community"},{"body":"Jikkou is an open source project, and we love getting patches and contributions to make Jikkou and its docs even better.\nContributing to Jikkou The Jikkou project itself lives in https://github.com/streamthoughts/jikkou\nCode reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nCreating issues Alternatively, if there‚Äôs something you‚Äôd like to see in Jikkou (or if you‚Äôve found something that isn‚Äôt working the way you‚Äôd expect), but you‚Äôre not sure how to fix it yourself, please create an issue.\n","categories":"","description":"How to contribute to Jikkou\n","excerpt":"How to contribute to Jikkou\n","ref":"/docs/community/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":" The KafkaConnectRestartConnectors action allows a user to restart all or just the failed Connector and Task instances for one or multiple named connectors.\nUsage (CLI) Usage: Execute the action. jikkou action KafkaConnectRestartConnectors execute [-hV] [--include-tasks] [--only-failed] [--connect-cluster=PARAM] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--connector-name=PARAM]... DESCRIPTION: The KafkaConnectRestartConnectors action a user to restart all or just the failed Connector and Task instances for one or multiple named connectors. OPTIONS: --connect-cluster=PARAM The name of the connect cluster. --connector-name=PARAM The connector's name. -h, --help Show this help message and exit. --include-tasks Specifies whether to restart the connector instance and task instances (includeTasks=true) or just the connector instance (includeTasks=false) --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: JSON, YAML (default YAML). --only-failed Specifies whether to restart just the instances with a FAILED status (onlyFailed=true) or all instances (onlyFailed=false) -V, --version Print version information and exit. Examples Restart all connectors for all Kafka Connect clusters. jikkou action kafkaconnectrestartconnectors execute (output)\n--- kind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: {} annotations: {} results: - status: \"SUCCEEDED\" data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: {} spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" workerId: \"connect:8083\" tasks: - id: 0 state: \"RUNNING\" workerId: \"connect:8083\" Restart all connectors with a FAILED status on all Kafka Connect clusters. jikkou action kafkaconnectrestartconnectors execute \\ --only-failed Restart specific connector and tasks for on Kafka Connect cluster jikkou action kafkaconnectrestartconnectors execute \\ --cluster-name my-connect-cluster --connector-name local-file-sink \\ --include-tasks ","categories":"","description":"Learn how to use the KafkaConnectRestartConnector action. \n","excerpt":"Learn how to use the KafkaConnectRestartConnector action. \n","ref":"/docs/providers/kafka-connect/actions/kafkaconnectrestartconnectors/","tags":["action","apache kafka","kafka connect"],"title":"KafkaConnectRestartConnectors"},{"body":" The KafkaConsumerGroupsResetOffsets action allows resetting offsets of consumer group. It supports one consumer group at the time, and group should be in EMPTY state.\nUsage (CLI) Usage: Execute the action. jikkou action KafkaConsumerGroupsResetOffsets execute [-hV] [--all] [--dry-run] [--to-earliest] [--to-latest] [--group=PARAM] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--to-datetime=PARAM] [--to-offset=PARAM] [--excludes=PARAM]... [--groups=PARAM]... [--includes=PARAM]... --topic=PARAM [--topic=PARAM]... DESCRIPTION: Reset offsets of consumer group. Supports multiple consumer groups, and groups should be in EMPTY state. You must choose one of the following reset specifications: to-datetime, by-duration, to-earliest, to-latest, to-offset. OPTIONS: --all Specifies to act on all consumer groups. --dry-run Only show results without executing changes on Consumer Groups. --excludes=PARAM List of patterns to match the consumer groups that must be excluded from the reset-offset action. --group=PARAM The consumer group to act on. --groups=PARAM The consumer groups to act on. -h, --help Show this help message and exit. --includes=PARAM List of patterns to match the consumer groups that must be included in the reset-offset action. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: JSON, YAML (default YAML). --to-datetime=PARAM Reset offsets to offset from datetime. Format: 'YYYY-MM-DDTHH:mm:SS.sss' --to-earliest Reset offsets to earliest offset. --to-latest Reset offsets to latest offset. --to-offset=PARAM Reset offsets to a specific offset. --topic=PARAM The topic whose partitions must be included in the reset-offset action. -V, --version Print version information and exit. Examples Reset Single Consumer Group to the earliest offsets jikkou action kafkaconsumergroupresetoffsets execute \\ --group my-group \\ --topic test \\ --to-earliest (output)\n--- kind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: {} annotations: configs.jikkou.io/to-earliest: \"true\" configs.jikkou.io/group: \"my-group\" configs.jikkou.io/dry-run: \"false\" configs.jikkou.io/topic: - \"test\" results: - status: \"SUCCEEDED\" errors: [] data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false annotations: {} status: state: \"EMPTY\" members: [] offsets: - topic: \"test\" partition: 1 offset: 0 - topic: \"test\" partition: 0 offset: 0 - topic: \"test\" partition: 2 offset: 0 - topic: \"--test\" partition: 0 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 Reset All Consumer Groups to the earliest offsets jikkou action kafkaconsumergroupresetoffsets execute \\ --all \\ --topic test \\ --to-earliest ","categories":"","description":"Learn how to use the KafkaConsumerGroupsResetOffsets action. \n","excerpt":"Learn how to use the KafkaConsumerGroupsResetOffsets action. \n","ref":"/docs/providers/kafka/actions/kafkaconsumergroupsresetoffsets/","tags":["action","apache kafka","kafka"],"title":"KafkaConsumerGroupsResetOffsets"},{"body":" The KafkaQuota resources are used to manage the Quotas in Aiven for Apache Kafka¬Æ service. For more details, see https://docs.aiven.io/docs/products/kafka/concepts/kafka-quotas\nKafkaQuota Specification Here is the resource definition file for defining a KafkaQuota.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaQuota\" # The resource kind (required) metadata: labels: { } annotations: { } spec: user: \u003cstring\u003e # The username: (Optional: 'default' if null) clientId: \u003cstring\u003e # The client-id consumerByteRate: \u003cnumber\u003e # The quota in bytes for restricting data consumption producerByteRate: \u003cnumber\u003e # The quota in bytes for restricting data production requestPercentage: \u003cnumber\u003e Example Here is a simple example that shows how to define a single ACL entry using the KafkaQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaQuota\" spec: user: \"default\" clientId: \"default\" consumerByteRate: 1048576 producerByteRate: 1048576 requestPercentage: 25 KafkaQuotaList If you need to define multiple Kafka quotas (e.g. using a template), it may be easier to use a KafkaQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaQuotaList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaQuotaList Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaQuotaList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaQuotaList\" items: - spec: user: \"default\" clientId: \"default\" consumerByteRate: 1048576 producerByteRate: 1048576 requestPercentage: 5 - spec: user: \"avnadmin\" consumerByteRate: 5242880 producerByteRate: 5242880 requestPercentage: 25 ","categories":"","description":"Learn how to manage Quotas in Aiven for Apache Kafka¬Æ\n","excerpt":"Learn how to manage Quotas in Aiven for Apache Kafka¬Æ\n","ref":"/docs/providers/aiven/resources/kafka-quota/","tags":["feature","resources"],"title":"Quotas for Aiven Apache Kafka¬Æ"},{"body":" The SchemaRegistryAclEntry resources are used to manage the Access Control Lists in Aiven for Schema Registry. A SchemaRegistryAclEntry resource defines the permission to be granted to a user for one or more Schema Registry Subjects.\nSchemaRegistryAclEntry Specification Here is the resource definition file for defining a SchemaRegistryAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE username: \u003c\u003e # The username resource: \u003c\u003e # The Schema Registry ACL entry resource name pattern NOTE: The resource name pattern should be Config: or Subject:\u003csubject_name\u003e where subject_name must consist of alpha-numeric characters, underscores, dashes, dots and glob characters * and ?.\nExample Here is an example that shows how to define a simple ACL entry using the SchemaRegistryAclEntry resource type.\nfile: schema-registry-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntry\" spec: permission: \"READ\" username: \"Alice\" resource: \"Subject:*\" SchemaRegistryAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a SchemaRegistryAclEntryList resource.\nSpecification Here the resource definition file for defining a SchemaRegistryAclEntryList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntryList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of SchemaRegistryAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the SchemaRegistryAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntryList\" items: - spec: permission: \"READ\" username: \"alice\" resource: \"Config:\" - spec: permission: \"WRITE\" username: \"alice\" resource: \"Subject:*\" ","categories":"","description":"Learn how to manage Access Control Lists (ACLs) in Aiven for Schema Registry\n","excerpt":"Learn how to manage Access Control Lists (ACLs) in Aiven for Schema ‚Ä¶","ref":"/docs/providers/aiven/resources/schema-registry-acl/","tags":["feature","resources"],"title":"ACL for Aiven Schema Registry"},{"body":" Here, you will find information to use the Kafka Connect extension for Jikkou.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extension Provider for Apache Kafka Connect.\n","excerpt":"Lean how to use the Jikkou Extension Provider for Apache Kafka ‚Ä¶","ref":"/docs/providers/kafka-connect/","tags":"","title":"Apache Kafka Connect"},{"body":" KafkaClientQuota resources are used to define the quota limits to be applied on Kafka consumers and producers. A KafkaClientQuota resource can be used to apply limit to consumers and/or producers identified by a client-id or a user principal.\nKafkaClientQuota Specification Here is the resource definition file for defining a KafkaClientQuota.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuota\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } spec: type: \u003cThe quota type\u003e # (required) entity: clientId: \u003cThe id of the client\u003e # (required depending on the quota type). user: \u003cThe principal of the user\u003e # (required depending on the quota type). configs: requestPercentage: \u003cThe quota in percentage (%) of total requests\u003e # (optional) producerByteRate: \u003cThe quota in bytes for restricting data production\u003e # (optional) consumerByteRate: \u003cThe quota in bytes for restricting data consumption\u003e # (optional) Quota Types The list below describes the supported quota types:\nUSERS_DEFAULT: Set default quotas for all users. USER: Set quotas for a specific user principal. USER_CLIENT: Set quotas for a specific user principal and a specific client-id. USER_ALL_CLIENTS: Set default quotas for a specific user and all clients. CLIENT: Set default quotas for a specific client. CLIENTS_DEFAULT: Set default quotas for all clients. Example Here is a simple example that shows how to define a single YAML file containing two quota definitions using the KafkaClientQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 --- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 KafkaClientQuotaList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaClientQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuotaList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaClientQuota Example Here is a simple example that shows how to define a single YAML file containing two KafkaClientQuota definition using the KafkaClientQuotaList resource type.\napiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuotaList' metadata: labels: { } annotations: { } items: - spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 - spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 ","categories":"","description":"Learn how to manage Kafka Client Quotas\n","excerpt":"Learn how to manage Kafka Client Quotas\n","ref":"/docs/providers/kafka/resources/quotas/","tags":["feature","resources"],"title":"Kafka Quotas"},{"body":" A KafkaTableRecord resource can be used to produce a key/value record into a given compacted topic, i.e., a topic with cleanup.policy=compact (a.k.a. KTable).\nKafkaTableRecord Specification Here is the resource definition file for defining a KafkaTableRecord.\napiVersion: \"kafka.jikkou.io/v1beta1\" # The api version (required) kind: \"KafkaTableRecord\" # The resource kind (required) metadata: labels: { } annotations: { } spec: type: \u003cstring\u003e # The topic name (required) headers: # The list of headers - name: \u003cstring\u003e value: \u003cstring\u003e key: # The record-key (required) type: \u003cstring\u003e # The record-key type. Must be one of: BINARY, STRING, JSON (required) data: # The record-key in JSON serialized form. $ref: \u003curl or path\u003e # Or an url to a local file containing the JSON string value. value: # The record-value (required) type: \u003cstring\u003e # The record-value type. Must be one of: BINARY, STRING, JSON (required) data: # The record-value in JSON serialized form. $ref: \u003curl or path\u003e # Or an url to a local file containing the JSON string value. Usage The KafkaTableRecord resource has been designed primarily to manage reference data published and shared via Kafka. Therefore, it is highly recommended to use this resource only with compacted Kafka topics containing a small amount of data.\nExamples Here are some examples that show how to a KafkaTableRecord using the different supported data type.\nSTRING:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" spec: topic: \"my-topic\" headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: STRING data: | \"foo\" JSON:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" spec: topic: \"my-topic\" headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: JSON data: | { \"foo\": \"bar\" } BINARY:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" spec: topic: \"my-topic\" headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: BINARY data: | \"eyJmb28iOiAiYmFyIn0K\" ","categories":"","description":"Learn how to manage a KTable Topic Records\n","excerpt":"Learn how to manage a KTable Topic Records\n","ref":"/docs/providers/kafka/resources/ktable-records/","tags":["feature","resources"],"title":"Kafka Table Records"},{"body":"Jikkou 0.32.0: Moving Beyond Apache Kafka. Introducing new features: Extension Providers, Actions, etc.! I‚Äôm thrilled to announce the release of Jikkou 0.32.0 which packs two major features: External Extension Providers and Actions. üôÇ\nHighlights: What‚Äôs new in Jikkou 0.32.0? New External Extension Provider mechanism to extend Jikkou features.\nNew extension type Action to execute specific operations against resources.\nNew action for resetting consumer group offsets.\nNew action for restarting connector and tasks for Kafka Connect.\nNew option selector-match to exclude/include resources from being returned or reconciled by Jikkou.\nNew API to get resources by their name.\nExtension Providers Jikkou is a project that continues to reinvent and redefine itself with each new version. Initially developed exclusively to manage the configuration of Kafka topics, it can now be used to manage Schema Registries, Kafka Connect connectors, and more. But, the funny thing is that Jikkou isn‚Äôt coupled with Kafka. It was designed around a concept of pluggable extensions that enable new capabilities and kind of resources to be seamlessly added to the project. For this, Jikkou uses the Java Service Loader mechanism to automatically discover new extensions at runtime.\nUnfortunately, until now there has been no official way of using this mechanism with Jikkou CLI or Jikkou API Server. For this reason, Jikkou 0.32.0 brings the capability to easily configuration external extensions.\nSo how does it work? Well, let‚Äôs imagine you want to be able to load Text Files from the local filesystem using Jikkou.\nFirst, we need to create a new Java project and add the Jikkou Core library to your project‚Äôs dependencies ( io.streamthoughts:jikkou-core:0.32.0 dependency).\nThen, you will need to create some POJO classes to represent your resource (e.g., V1File.class) and to implement the Collector interface :\n@SupportedResource(type = V1File.class) @ExtensionSpec( options = { @ExtensionOptionSpec( name = \"directory\", description = \"The absolute path of the directory from which to collect files\", type = String.class, required = true ) } ) @Description(\"FileCollector allows listing all files in a given directory.\") public final class FileCollector extends ContextualExtension implements Collector\u003cV1File\u003e { private static final Logger LOG = LoggerFactory.getLogger(FileCollector.class); @Override public ResourceListObject\u003cV1File\u003e listAll(@NotNull Configuration configuration, @NotNull Selector selector) { // Get the 'directory' option. String directory = extensionContext().\u003cString\u003econfigProperty(\"directory\").get(configuration); // Collect all files. List\u003cV1File\u003e objects = Stream.of(new File(directory).listFiles()) .filter(file -\u003e !file.isDirectory()) .map(file -\u003e { try { Path path = file.toPath(); String content = Files.readString(path); V1File object = new V1File(ObjectMeta .builder() .withName(file.getName()) .withAnnotation(\"system.jikkou.io/fileSize\", Files.size(path)) .withAnnotation(\"system.jikkou.io/fileLastModifier\", Files.getLastModifiedTime(path)) .build(), new V1FileSpec(content) ); return Optional.of(object); } catch (IOException e) { LOG.error(\"Cannot read content from file: {}\", file.getName(), e); return Optional.\u003cV1File\u003eempty(); } }) .flatMap(Optional::stream) .toList(); ObjectMeta objectMeta = ObjectMeta .builder() .withAnnotation(\"system.jikkou.io/directory\", directory) .build(); return new DefaultResourceListObject\u003c\u003e(\"FileList\", \"system.jikkou.io/v1\", objectMeta, objects); } } Next, you will need to implement the ExtensionProvider interface to register both your extension and your resource kind.\npublic final class FileExtensionProvider implements ExtensionProvider { /** * Registers the extensions for this provider. * * @param registry The ExtensionRegistry. */ public void registerExtensions(@NotNull ExtensionRegistry registry) { registry.register(FileCollector.class, FileCollector::new); } /** * Registers the resources for this provider. * * @param registry The ResourceRegistry. */ public void registerResources(@NotNull ResourceRegistry registry) { registry.register(V1File.class); } } Then, the fully qualified name of the class must be added to the resource file META-INF/service/io.streamthoughts.jikkou.spi.ExtensionProvider.\nFinally, all you need to do is to package your project as a tarball or ZIP archive. The archive must contain a single top-level directory containing the extension JAR files, as well as any resource files or third-party libraries required by your extensions.\nTo install your Extension Provider, all you need to do is to unpacks the archive into a desired location (e.g., /usr/share/jikkou-extensions) and to configure the Jikkou‚Äôs API Server or Jikkou CLI (when running the Java Binary Distribution, i.e., not the native version) with the jikkou.extension.paths property (e.g., jikkou.extension.paths=/usr/share/jikkou-extensions). For people who are familiar with how Kafka Connect works, it‚Äôs more or less the same mechanism.\n(The full code source of this example is available on GitHub).\nAnd that‚Äôs it! üôÇ\nExtension Providers open up the possibility of infinitely expanding Jikkou to manage your own resources, and use it with systems other than Kafka.\nActions Jikkou uses a declarative approach to manage the asset state of your data infrastructure using resource descriptors written in YAML. But sometimes, ops and development teams may need to perform specific operations on their resources that cannot be included in their descriptor files. For example, you may need to reset offsets for one or multiple Kafka Consumer Groups, restart failed connectors and tasks for Kafka Connect, etc. So instead of having to switch from one tool to another, why not just use Jikkou for this?\nWell, to solve that need, Jikkou 0.32.0 introduces a new type of extension called ‚ÄúActions‚Äù that allows users to perform specific operations on resources.\nCombined with the External Extension Provider mechanism, you can now implement the Action interface to add custom operations to Jikkou.\n@Category(ExtensionCategory.ACTION) public interface Action\u003cT extends HasMetadata\u003e extends HasMetadataAcceptable, Extension { /** * Executes the action. * * @param configuration The configuration * @return The ExecutionResultSet */ @NotNull ExecutionResultSet\u003cT\u003e execute(@NotNull Configuration configuration); } Actions are fully integrated with Jikkou API Server through the new Endpoint: /api/v1/actions/{name}/execute{?[options]\nKafka Consumer Groups Altering Consumer Group Offsets Jikkou 0.32.0 introduces the new KafkaConsumerGroupsResetOffsets action allows resetting offsets of consumer groups.\nHere is an example showing how to reset the group my-group to the earliest offsets for topic test.\n$ jikkou action kafkaconsumergroupresetoffsets execute \\ --group my-group \\ --topic test \\ --to-earliest (output)\nkind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: { } annotations: configs.jikkou.io/to-earliest: \"true\" configs.jikkou.io/group: \"my-group\" configs.jikkou.io/dry-run: \"false\" configs.jikkou.io/topic: - \"test\" results: - status: \"SUCCEEDED\" errors: [ ] data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false annotations: { } status: state: \"EMPTY\" members: [ ] offsets: - topic: \"test\" partition: 1 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 This action is pretty similar to the kafka-consumer-group script that ships with Apache Kafka. You can use it to reset a consumer group to the earliest or latest offsets, to a specific datetime or specific offset.\nIn addition, it can be executed in a dry-run.\nDeleting Consumer Groups You can now add the core annotation jikkou.io/delete to a KafkaConsumerGroup resource to mark it for deletion:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false annotations: jikkou.io/delete: true $ jikkou delete --files my-consumer-group.yaml -o wide TASK [DELETE] Delete consumer group 'my-group' - CHANGED ************************************************ { \"status\" : \"CHANGED\", \"changed\" : true, \"failed\" : false, \"end\" : 1701162781494, \"data\" : { \"apiVersion\" : \"core.jikkou.io/v1beta2\", \"kind\" : \"GenericResourceChange\", \"metadata\" : { \"name\" : \"my-group\", \"labels\" : { \"kafka.jikkou.io/is-simple-consumer\" : false }, \"annotations\" : { \"jikkou.io/delete\" : true, \"jikkou.io/managed-by-location\" : \"./my-consumer-group.yaml\" } }, \"change\" : { \"before\" : { \"apiVersion\" : \"kafka.jikkou.io/v1beta1\", \"kind\" : \"KafkaConsumerGroup\", \"metadata\" : { \"name\" : \"my-group\", \"labels\" : { \"kafka.jikkou.io/is-simple-consumer\" : false }, \"annotations\" : { } } }, \"operation\" : \"DELETE\" } }, \"description\" : \"Delete consumer group 'my-group'\" } EXECUTION in 64ms ok : 0, created : 0, altered : 0, deleted : 1 failed : 0 Kafka Connect Restarting Connector and Tasks This new release also packs with the new action KafkaConnectRestartConnectors action allows a user to restart all or just the failed Connector and Task instances for one or multiple named connectors.\nHere are a few examples from the documentation:\nRestarting all connectors for all Kafka Connect clusters. $ jikkou action kafkaconnectrestartconnectors execute --- kind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: {} annotations: {} results: - status: \"SUCCEEDED\" data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: {} spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" workerId: \"connect:8083\" tasks: - id: 0 state: \"RUNNING\" workerId: \"connect:8083\" Restarting a specific connector and tasks for on Kafka Connect cluster $ jikkou action kafkaconnectrestartconnectors execute \\ --cluster-name my-connect-cluster --connector-name local-file-sink \\ --include-tasks New Selector Matching Strategy Jikkou CLI allows you to provide one or multiple *selector expressions *in order to include or exclude resources from being returned or reconciled by Jikkou. In previous versions, selectors were cumulative, so resources had to match all selectors to be returned. Now, you can specify a selector matching strategy to determine how expressions must be combined using the option ‚Äìselector-match=[ANY|ALL|NONE].\nALL: A resource is selected if it matches all selectors.\nANY: A resource is selected if it matches one of the selectors.\nNONE: A resource is selected if it matches none of the selectors.\nFor example, the below command will only return topics matching the regex ^__.* or having a name equals to _schemas.\n$ jikkou get kafkatopics \\ --selector 'metadata.name MATCHES (^__connect-*)' --selector 'metadata.name IN (_schemas)' --selector-match ANY New Get Resource by Name In some cases, it may be necessary to retrieve only a specific resource for a specific name. In previous versions, the solution was to use selectors. Unfortunately, this approach isn‚Äôt very efficient, as it involves retrieving all the resources and then filtering them. To start solving that issue, Jikkou v0.32.0 adds a new API to retrieve a resource by its name.\nExample (using Jikkou CLI):\n$ jikkou get kafkatopics --name _schemas Example (using Jikkou API Server):\n$ curl -sX GET \\ http://localhost:28082/apis/kafka.jikkou.io/v1/kafkatopics/_schemas \\ -H 'Accept:application/json' Note : Currently not all resources have been updated to use that new API, so it‚Äôs possible that selectors are used as a default implementation by internal Jikkou API.\n","categories":"","description":"","excerpt":"Jikkou 0.32.0: Moving Beyond Apache Kafka. Introducing new features: ‚Ä¶","ref":"/docs/releases/release-v0.32.0/","tags":"","title":"Release v0.32.0"},{"body":"Introducing Jikkou 0.33.0 We‚Äôre excited to unveil the latest release of Jikkou 0.33.0. üéâ\nTo install the new version, please visit the installation guide. For detailed release notes, check out the GitHub page.\nWhat‚Äôs New in Jikkou 0.33.0? Enhanced resource change format. Added support for the patch command. Introduced the new --status option for KafkaTopic resources. Exported offset-lag to the status of KafkaConsumerGroup resources. Below is a summary of these new features with examples.\nDiff/Patch Commands In previous versions, Jikkou provided the diff command to display changes required to reconcile input resources. However, this command lacked certain capabilities to be truly useful. This new version introduces a standardized change format for all resource types, along with two new options for filtering changes:\n--filter-resource-op=: Filters out all state changes except those corresponding to the given operations. --filter-change-op=: Filters out all resources except those corresponding to the given operations. The new output format you can expect from the diff command is as follows:\n--- apiVersion: [ group/version of the change ] kind: [ kind of the change ] metadata: [ resource metadata ] spec: # Array of state changes changes: - name: [ name of the changed state ] op: [ change operation ] before: [ value of the state before the operation ] after: [ value of the state after the operation ] data: [ static data attached to the resource ] op: [ resource change operation ] The primary motivation behind this new format is the introduction of a new patch command. Prior to Jikkou 0.33.0, when using the apply command after a dry-run or a diff command, Jikkou couldn‚Äôt guarantee that the applied changes matched those returned from the previous command. With Jikkou 0.33.0, you can now directly pass the result of the diff command to the new patch command to efficiently apply the desired state changes.\nHere‚Äôs a workflow to create your resources:\nStep 1) Create a resource descriptor file\ncat \u003c\u003c EOF \u003e my-topic.yaml --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic' labels: environment: example spec: partitions: 3 replicas: 1 configs: min.insync.replicas: 1 cleanup.policy: 'delete' EOF Step 2) Run diff\njikkou diff -f ./my-topic.yaml \u003e my-topic-diff.yaml (output) --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicChange\" metadata: name: \"my-topic\" labels: environment: \"example\" annotations: jikkou.io/managed-by-location: \"my-topic.yaml\" spec: changes: - name: \"partitions\" op: \"CREATE\" after: 3 - name: \"replicas\" op: \"CREATE\" after: 1 - name: \"config.cleanup.policy\" op: \"CREATE\" after: \"delete\" - name: \"config.min.insync.replicas\" op: \"CREATE\" after: 1 op: \"CREATE\" Step 3) Run patch\njikkou patch -f ./my-topic-diff.yaml --mode FULL --output compact (output)\nTASK [ CREATE ] Create topic 'my-topic' (partitions=3, replicas=1, configs=[cleanup.policy=delete, min.insync.replicas=1]) - CHANGED EXECUTION in 3s 797ms ok: 0, created: 1, altered: 0, deleted: 0 failed: 0 Attempting to apply the changes a second time may result in an error from the remote system:\n{ \"status\": \"FAILED\", \"description\": \"Create topic 'my-topic' (partitions=3, replicas=1,configs=[cleanup.policy=delete,min.insync.replicas=1])\", \"errors\": [ { \"message\": \"TopicExistsException: Topic 'my-topic' already exists.\" } ], ... } Resource Provider for Apache Kafka Jikkou 0.33.0 also packs with some minor improvements for the Apache Kafka provider.\nKafkaTopic Status You can now describe the status of a topic-partitions by using the new --status option\nwhen getting a KafkaTopic resource.\njikkou get kt --status --selector \"metadata.name IN (my-topic)\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopic\" metadata: name: \"my-topic\" labels: kafka.jikkou.io/topic-id: \"UbZI2N2YQTqfNcbKKHps5A\" annotations: kafka.jikkou.io/cluster-id: \"xtzWWN4bTjitpL3kfd9s5g\" spec: partitions: 1 replicas: 1 configs: cleanup.policy: \"delete\" configMapRefs: [ ] status: partitions: - id: 0 leader: 101 isr: - 101 replicas: - 101 KafkaConsumerGroup OffsetLags With Jikkou 0.33.0, you can export the offset-lag of a KafkaConsumerGroup resource using the --offsets option.\njikkou get kafkaconsumergroups --offsets --- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false status: state: \"EMPTY\" members: [ ] offsets: - topic: \"my-topic\" partition: 0 offset: 16 offset-lag: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 Finally, all those new features are also completely available through the Jikkou REST Server.\nWrapping Up We hope you enjoy these new features. If you encounter any issues with Jikkou v0.33.0, please feel free to open a GitHub issue on our project page. Don‚Äôt forget to give us a ‚≠êÔ∏è on Github to support the team, and join us on Slack.\n","categories":"","description":"","excerpt":"Introducing Jikkou 0.33.0 We‚Äôre excited to unveil the latest release ‚Ä¶","ref":"/docs/releases/release-v0.33.0/","tags":"","title":"Release v0.33.0"},{"body":"Introducing Jikkou 0.34.0 We‚Äôre excited to unveil the latest release of Jikkou 0.34.0. üéâ\nTo install the new version, please visit the installation guide. For detailed release notes, check out the GitHub page.\nWhat‚Äôs New in Jikkou 0.34.0? Enhanced Aiven provider with support for Kafka topics. Added SSL support for Kafka Connect and Schema Registry Introduced dynamic connection for Kafka Connect clusters Below is a summary of these new features with examples.\nTopic Aiven for Apache Kafka Jikkou 0.34.0 adds a new KafkaTopic kind that can be used to manage kafka Topics directly though the Aiven API.\nYou can list kafka topics using the new command below:\njikkou get avn-kafkatopics In addition, topics can be described, created and updated using the same resource model as the Apache Kafka provider.\n# file:./aiven-kafkat-topics.yaml --- apiVersion: \"kafka.aiven.io/v1beta2\" kind: \"KafkaTopic\" metadata: name: \"test\" labels: tag.aiven.io/my-tag: \"test-tag\" spec: partitions: 1 replicas: 3 configs: cleanup.policy: \"delete\" compression.type: \"producer\" The main advantages of using this new resource kind are the use of the Aiven Token API to authenticate to the Aiven API and the ability to manage tags for kafka topics.\nSSL support for Kafka Connect and Schema Registry Jikkou 0.34.0 also brings SSL support for the Kafka Connect and Schema Registry providers. Therefore, it‚Äôs now possible to configure the providers to authenticate using SSL certificate.\nExample for the Schema Registry:\njikkou { schemaRegistry { url = \"https://localhost:8081\" authMethod = \"SSL\" sslKeyStoreLocation = \"/certs/registry.keystore.jks\" sslKeyStoreType = \"JKS\" sslKeyStorePassword = \"password\" sslKeyPassword = \"password\" sslTrustStoreLocation = \"/certs/registry.truststore.jks\" sslTrustStoreType = \"JKS\" sslTrustStorePassword = \"password\" sslIgnoreHostnameVerification = true } } Dynamically connection for Kafka Connect clusters Before Jikkou 0.34.0, to deploy a Kafka Connect connector, it was mandatory to configure a connection to a target cluster:\njikkou { extensions.provider.kafkaconnect.enabled = true kafkaConnect { clusters = [ { name = \"my-connect-cluster\" url = \"http://localhost:8083\" } ] } } This connection could then be referenced in a connector resource definition via the annotation kafka.jikkou.io/connect-cluster.\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"mu-connector\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" This approach is suitable for most use cases, but can be challenging if you need to manage a large and dynamic number of Kafka Connect clusters.\nTo meet this need, it is now possible to provide connection information for the cluster to connect to directly, through the new metadata annotation new metadata annotation: jikkou.io/config-override.\nHere is a simple example showing the use of the new annotation:\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"mu-connector\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: jikkou.io/config-override: | { \"url\": \"http://localhost:8083\" } This new annotation can be used with the Jikkou‚Äôs Jinja template creation mechanism to define dynamic configuration.\nWrapping Up We hope you enjoy these new features. If you encounter any issues with Jikkou v0.33.0, please feel free to open a GitHub issue on our project page. Don‚Äôt forget to give us a ‚≠êÔ∏è on Github to support the team, and join us on Slack.\n","categories":"","description":"","excerpt":"Introducing Jikkou 0.34.0 We‚Äôre excited to unveil the latest release ‚Ä¶","ref":"/docs/releases/release-v0.34.0/","tags":"","title":"Release v0.34.0"},{"body":" Here, you will find information to use the Schema Registry extensions.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extension Provider for Schema Registry.\n","excerpt":"Lean how to use the Jikkou Extension Provider for Schema Registry.\n","ref":"/docs/providers/schema-registry/","tags":"","title":"Schema Registry"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your Schema Registry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nSchemaRegistrySubject Specification Here is the resource definition file for defining a SchemaRegistrySubject.\napiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: 'Karapace' # (optional) The vendor of the Schema Registry compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003c\u003e # The name for the reference. subject: \u003c\u003e # The subject under which the referenced schema is registered. version: \u003c\u003e # The exact version of the schema under the registered subject. ] The metadata.name property is mandatory and specifies the name of the Subject.\nTo use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } ","categories":"","description":"Learn how to manage Schema Registry Subject Schema in Aiven.\n","excerpt":"Learn how to manage Schema Registry Subject Schema in Aiven.\n","ref":"/docs/providers/aiven/resources/schema-registry-subject/","tags":["feature","resources"],"title":"Subject for Aiven Schema Registry"},{"body":" Here, you will find information to use the Aiven for Kafka extensions.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extensions Providers for Aiven.\n","excerpt":"Lean how to use the Jikkou Extensions Providers for Aiven.\n","ref":"/docs/providers/aiven/","tags":"","title":"Aiven"},{"body":"Jikkou ships with the following built-in validations:\nNo validation\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/docs/providers/aiven/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\n","categories":"","description":"Learn how to use the validations provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the validations provided by the Kafka Connect ‚Ä¶","ref":"/docs/providers/kafka-connect/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nTopics NoDuplicateTopicsAllowedValidation (auto registered)\nTopicConfigKeysValidation (auto registered)\ntype = io.streamthoughts.jikkou.kafka.validation.TopicConfigKeysValidation The TopicConfigKeysValidation allows checking if the specified topic configs are all valid.\nTopicMinNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation The TopicMinNumPartitions allows checking if the specified number of partitions for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinNumPartitions Int Minimum number of partitions allowed TopicMaxNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMaxNumPartitions The TopicMaxNumPartitions allows checking if the number of partitions for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxNumPartitions Int Maximum number of partitions allowed TopicMinReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactor The TopicMinReplicationFactor allows checking if the specified replication factor for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinReplicationFactor Int Minimum replication factor allowed TopicMaxReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMaxReplicationFactor The TopicMaxReplicationFactor allows checking if the specified replication factor for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxReplicationFactor Int Maximum replication factor allowed TopicNamePrefix type = io.streamthoughts.jikkou.kafka.validation.TopicNamePrefix The TopicNamePrefix allows checking if the specified name for a topic starts with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNamePrefixes List List of topic name prefixes allows TopicNameSuffix type = io.streamthoughts.jikkou.kafka.validation.TopicNameSuffix The TopicNameSuffix allows checking if the specified name for a topic ends with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNameSuffixes List List of topic name suffixes allows ACLs NoDuplicateUsersAllowedValidation (auto registered)\nNoDuplicateRolesAllowedValidation (auto registered)\nQuotas QuotasEntityValidation (auto registered)\n","categories":"","description":"Learn how to use the built-in validations provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the built-in validations provided by the Extension ‚Ä¶","ref":"/docs/providers/kafka/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nSubject SchemaCompatibilityValidation type = io.streamthoughts.jikkou.schema.registry.validation.SchemaCompatibilityValidation The SchemaCompatibilityValidation allows testing the compatibility of the schema with the latest version already registered in the Schema Registry using the provided compatibility-level.\nAvroSchemaValidation The AvroSchemaValidation allows checking if the specified Avro schema matches some specific avro schema definition rules;\ntype = io.streamthoughts.jikkou.schema.registry.validation.AvroSchemaValidation Configuration\nName Type Description Default fieldsMustHaveDoc Boolean Verify that all record fields have a doc property false fieldsMustBeNullable Boolean Verify that all record fields are nullable false fieldsMustBeOptional Boolean Verify that all record fields are optional false ","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/docs/providers/schema-registry/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":" Here, you will find information about the annotations provided by the Aiven extension for Jikkou.\nList of built-in annotations kafka.aiven.io/acl-entry-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the ID of an ACL entry.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/aiven/annotations/","tags":"","title":"Annotations"},{"body":" This section lists a number of well known annotations, that have defined semantics. They can be attached to KafkaConnect resources through the metadata.annotations field and consumed as needed by extensions (i.e., validations, transformations, controller, collector, etc.).\nList of built-in annotations ","categories":"","description":"Learn how to use the metadata annotations provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the metadata annotations provided by the Kafka ‚Ä¶","ref":"/docs/providers/kafka-connect/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided the Apache Kafka extension for Jikkou.\nList of built-in annotations kafka.jikkou.io/cluster-id Used by jikkou.\nThe annotation is automatically added by Jikkou to a describe object part of an Apache Kafka cluster.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/kafka/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided by the Schema Registry extension for Jikkou.\nList of built-in annotations schemaregistry.jikkou.io/url Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the SchemaRegistry URL from which a subject schema is retrieved.\nschemaregistry.jikkou.io/schema-version Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject schema.\nschemaregistry.jikkou.io/schema-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject id.\nschemaregistry.jikkou.io/normalize-schema Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThis annotation can be used to normalize the schema on SchemaRegistry server side. Note, that Jikkou will attempt to normalize AVRO and JSON schema.\nSee: Confluent SchemaRegistry API Reference\nschemaregistry.jikkou.io/permanante-delete Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThe annotation can be used to specify a hard delete of the subject, which removes all associated metadata including the schema ID. The default is false. If the flag is not included, a soft delete is performed. You must perform a soft delete first, then the hard delete.\nSee: Confluent SchemaRegistry API Reference\nschemaregistry.jikkou.io/use-canonical-fingerprint This annotation can be used to use a canonical fingerprint to compare schemas (only supported for Avro schema).\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/schema-registry/annotations/","tags":"","title":"Annotations"},{"body":" This section lists a number of well known labels, that have defined semantics. They can be attached to KafkaConnect resources through the metadata.labels field and consumed as needed by extensions (i.e., validations, transformations, controller, collector, etc.).\nLabels kafka.jikkou.io/connect-cluster # Example --- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: labels: kafka.jikkou.io/connect-cluster: 'my-connect-cluster' The value of this label defined the name of the Kafka Connect cluster to create the connector instance in. The cluster name must be configured through the kafkaConnect.clusters[] Jikkou‚Äôs configuration setting (see: Configuration).\n","categories":"","description":"Learn how to use the metadata labels provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the metadata labels provided by the Kafka Connect ‚Ä¶","ref":"/docs/providers/kafka-connect/labels/","tags":"","title":"Labels"},{"body":" Here, you will find the list of actions provided by the Extension Provider for Kafka Connect.\nKafka Connect Action More information:\n","categories":"","description":"Learn how to use the actions provided by the Extension Provider for Kafka Connect.\n","excerpt":"Learn how to use the actions provided by the Extension Provider for ‚Ä¶","ref":"/docs/providers/kafka-connect/actions/","tags":"","title":"Actions"},{"body":" Here, you will find the list of actions provided by the Extension Provider for Apache Kafka.\nApache Kafka Action More information:\n","categories":"","description":"Learn how to use the actions provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the actions provided by the Extension Provider for ‚Ä¶","ref":"/docs/providers/kafka/actions/","tags":"","title":"Actions"},{"body":"The Apache Kafka extension for Jikkou utilizes the Kafka Admin Client which is compatible with any Kafka infrastructure, such as :\nAiven Apache Kafka Confluent Cloud MSK Redpanda etc. In addition, Kafka Protocol has a ‚Äúbidirectional‚Äù client compatibility policy. In other words, new clients can talk to old servers, and old clients can talk to new servers.\n","categories":"","description":"Compatibility for Apache Kafka.\n","excerpt":"Compatibility for Apache Kafka.\n","ref":"/docs/providers/kafka/compatibility/","tags":"","title":"Compatibility"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/action/","tags":"","title":"action"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/apache-kafka/","tags":"","title":"apache kafka"},{"body":"","categories":"","description":"Jikkou - API References","excerpt":"Jikkou - API References","ref":"/docs/jikkou-api-server/api-references/","tags":"","title":"Jikkou - API References"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/concept/","tags":"","title":"concept"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/extension/","tags":"","title":"extension"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/extensions/","tags":"","title":"extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/feature/","tags":"","title":"feature"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/how-to/","tags":"","title":"how-to"},{"body":" Open source\nResource as Code\nFramework\nfor Apache Kafka¬Æ Install Jikkou What is Jikkou ? Jikkou is a powerful, flexible open-source framework that enables self-serve resource provisioning. It allows developers and DevOps teams to easily manage, automate, and provision all the resources needed for their Apache Kafka platform. Get started with Jikkou Declarative \u0026 Automated Describe the entire desired state of any resource you need to manage using YAML descriptor files.\nDesigned for Apache Kafka¬Æ Jikkou was initially developed to manage Apache Kafka resources. You can use it with most of Apache Kafka vendors: Apache Kafka, Aiven, Amazon MSK, Confluent Cloud, Redpanda.\nExtensible and Customizable Jikkou can be extended to manage almost anything. It provides a simple and powerful Core API (in Java) allowing you to write custom extensions for managing your own system and resources.\nOpen source Jikkou is released under the Apache License 2.0. Anyone can contribute to Jikkou by opening an issue, a pull request (PR) or just by discussing with other users on the Slack Channel.\nJoin us on Slack Join the Jikkou community on Slack\nJoins Us Contributions welcome Want to join the fun on Github? New users are always welcome!\nContribute Support Jikkou Team Add a star to the GitHub project, it only takes 5 seconds!\nStar ","categories":"","description":"","excerpt":" Open source\nResource as Code\nFramework\nfor Apache Kafka¬Æ Install ‚Ä¶","ref":"/","tags":"","title":"Jikkou"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kafka/","tags":"","title":"kafka"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kafka-connect/","tags":"","title":"kafka connect"},{"body":" This transformation can be used to enforce a maximum value for the number of partitions of kafka topics.\nConfiguration Name Type Description Default maxNumPartitions Int maximum value for the number of partitions to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMaxNumPartitions priority = 100 config = { maxNumPartitions = 50 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicmaxnumpartitions/","tags":"","title":"KafkaTopicMaxNumPartitions"},{"body":" This transformation can be used to enforce a maximum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default maxRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { maxRetentionMs = 2592000000 # 30 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicmaxretentionms/","tags":"","title":"KafkaTopicMaxRetentionMs"},{"body":" This transformation can be used to enforce a minimum value for the min.insync.replicas property of kafka topics.\nConfiguration Name Type Description Default minInSyncReplicas Int Minimum value of min.insync.replicas to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicmininsyncreplicas/","tags":"","title":"KafkaTopicMinInSyncReplicas"},{"body":" This transformation can be used to enforce a minimum value for the replication factor of kafka topics.\nConfiguration Name Type Description Default minReplicationFactor Int Minimum value of replication factor to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 3 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicminreplicas/","tags":"","title":"KafkaTopicMinReplicas"},{"body":" This transformation can be used to enforce a minimum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default minRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { minRetentionMs = 604800000 # 7 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicminretentionms/","tags":"","title":"KafkaTopicMinRetentionMs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/releases/","tags":"","title":"Releases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/resources/","tags":"","title":"resources"},{"body":"Blog GitOps \u0026 Kafka: Enabling smooth and seamless Data Schema management with Jikkou and GitHub Actions (by Florian Hussonnois) Jikkou: Declarative ACLs configuration for Apache Kafka¬Æ and Schema Registry on Aiven (by Florian Hussonnois) Kafka Connect + Jikkou- Easily manage Kafka connectors (by Florian Hussonnois) Why is Managing Kafka Topics Still Such a Pain? Introducing Jikkou! (Florian Hussonnois) ","categories":"","description":"","excerpt":"Blog GitOps \u0026 Kafka: Enabling smooth and seamless Data Schema ‚Ä¶","ref":"/docs/resources/","tags":"","title":"Resources"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]