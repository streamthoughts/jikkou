[{"body":"üöÄ Introducing Jikkou 0.37.0 We‚Äôre pleased to announce Jikkou 0.37.0.\nThis release tackles real pain points reported by the community ‚Äî from managing multiple Kafka environments in a single config, to fixing broken Schema Registry workflows. Here‚Äôs the overview:\nüÜï Multiple provider instances ‚Äî manage prod, staging, and dev from one configuration üîÑ New replace command for full resource recreation üõ°Ô∏è Schema Registry overhaul: subject modes, failover, regex validation, and more ‚öôÔ∏è KIP-980: create Kafka connectors in STOPPED or PAUSED state üì¶ Directories as input for --values-files üìë Jinja template file locations for reusable templates üìê All resource schemas promoted to v1 API version üîß Java 25 migration and REST client modernization To install, check out the installation guide. For the full changelog, see the GitHub release page.\nüìê All Resource Schemas Promoted to v1 All resource apiVersion values have been promoted from v1beta1/v1beta2 to v1. This applies across every provider ‚Äî Kafka, Schema Registry, Kafka Connect, Aiven, AWS, and core resources.\n# Before (still works, but deprecated) apiVersion: \"kafka.jikkou.io/v1beta2\" # After apiVersion: \"kafka.jikkou.io/v1\" Existing YAML files using v1beta1 or v1beta2 will continue to work ‚Äî Jikkou automatically resolves old versions to the latest registered resource class and normalises the apiVersion during deserialisation. But we recommend updating your files to v1 going forward.\nüÜï Multiple Provider Instances Most teams don‚Äôt run a single Kafka cluster. You have production, staging, maybe a dev cluster ‚Äî and until now, you needed separate Jikkou configurations or manual overrides to target each one. There was no way to register multiple instances of the same provider type.\nJikkou 0.37.0 introduces named provider instances. Register as many Kafka (or Schema Registry, or Kafka Connect) providers as you need, each with its own name and configuration. Then target the right one with --provider on the CLI or \"provider\" in API requests.\njikkou { provider.kafka-prod { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider default = true config = { client { bootstrap.servers = \"prod-kafka:9092\" } } } provider.kafka-staging { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { client { bootstrap.servers = \"staging-kafka:9092\" } } } } # Preview changes against production jikkou diff --values-files topics.yaml --provider kafka-prod # Apply to staging jikkou create --values-files topics.yaml --provider kafka-staging How Provider Selection Works The --provider flag is always optional. Jikkou resolves the target provider with a simple fallback chain:\nSingle provider of a given type: It‚Äôs used automatically ‚Äî no --provider flag needed, no default flag needed. If you only have one Kafka provider configured, everything works exactly as before. Multiple providers, one marked default = true: The default is used when --provider is omitted. You only need the flag when targeting a non-default provider. Multiple providers, no default: You must specify --provider on every command. Omitting it will fail with an explicit error: ‚ÄúNo default configuration defined, and multiple configurations found for provider type‚Äù. This means existing single-provider configurations continue to work without any changes. The default and --provider flags only matter once you add a second provider.\nProvider selection works across all commands ‚Äî create, update, delete, diff, validate, replace, and patch ‚Äî and extends to the REST API server with a provider field in reconciliation request bodies.\nüîÑ New replace Command If you‚Äôve used Terraform‚Äôs taint or --replace flag, you know the pattern: sometimes you don‚Äôt want to update a resource in place ‚Äî you want to tear it down and recreate it from scratch.\nJikkou now has its own version of this. The new replace command forces a full delete-then-create cycle on all targeted resources, regardless of whether an in-place update would be possible.\njikkou replace --values-files resources.yaml # Preview first jikkou replace --values-files resources.yaml --dry-run A typical use case: development environments where you want to drop and recreate all your Kafka topics daily to start from a clean state. Rather than chaining jikkou delete and jikkou create in a script, replace handles it in one pass.\nüõ°Ô∏è Schema Registry ‚Äî Major Improvements This release brings five targeted fixes and features for the Schema Registry provider, most addressing specific community-reported issues.\nSubject Modes Schema Registry subjects now support mode management directly from your resource definitions. This is essential for schema migration workflows ‚Äî set a subject to IMPORT mode, register schemas with specific IDs, then switch back to READWRITE.\napiVersion: \"schemaregistry.jikkou.io/v1\" kind: \"SchemaRegistrySubject\" metadata: name: \"user-events\" spec: mode: \"IMPORT\" compatibilityLevel: \"BACKWARD\" schemaType: \"AVRO\" schema: | { \"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"id\", \"type\": \"int\"}] } Supported modes: IMPORT, READONLY, READWRITE, FORWARD.\nSchema ID and Version on Create When migrating schemas between registries, you often need to preserve the original schema IDs and versions. Jikkou now lets you specify both via annotations:\nmetadata: name: \"events\" annotations: schemaregistry.jikkou.io/schema-id: \"100\" schemaregistry.jikkou.io/schema-version: \"1\" Combined with IMPORT mode, this gives you full control over registry migrations.\nMultiple URLs with Failover The documentation promised comma-separated Schema Registry URLs for failover, but the raw string was passed directly to the underlying HTTP client. Failover simply didn‚Äôt work.\nJikkou now properly parses comma-separated URLs. On connection failure, it automatically tries the next URL in the list.\nprovider.schemaregistry { config = { url = \"http://sr-primary:8081,http://sr-backup:8081,http://sr-dr:8081\" } } Subject Name Regex Validation Enforce naming conventions on your Schema Registry subjects with a regex pattern. Jikkou will reject any subject that doesn‚Äôt match ‚Äî before it ever reaches the registry.\nvalidations: - name: \"subjectMustHaveValidName\" type: \"io.streamthoughts.jikkou.schema.registry.validation.SubjectNameRegexValidation\" config: subjectNameRegex: \"[a-zA-Z0-9\\\\._\\\\-]+\" Permanent Schema Deletion in One Step Permanently deleting a schema used to require two separate Jikkou runs ‚Äî first a soft delete, then a hard delete. In a CI/CD pipeline, that‚Äôs impractical. You declare ‚Äúdelete this permanently‚Äù and expect it to happen in one operation.\nWhen you set jikkou.io/permanent-delete: true, Jikkou now automatically performs the soft delete followed by the hard delete in a single reconciliation pass.\n‚öôÔ∏è Kafka Connect: KIP-980 Support When creating Kafka connectors, the state field was silently ignored ‚Äî every connector started in RUNNING state, even if you specified STOPPED or PAUSED. For production deployments, you often want to create a connector, verify its configuration, and only then start it manually.\nJikkou now uses the KIP-980 API to pass initial_state when creating connectors:\napiVersion: \"kafkaconnect.jikkou.io/v1\" kind: \"KafkaConnector\" metadata: name: \"jdbc-source\" annotations: kafkaconnect.jikkou.io/initial_state: \"STOPPED\" spec: connectorClass: \"io.confluent.connect.jdbc.JdbcSourceConnector\" config: connection.url: \"jdbc:mysql://db:3306/mydb\" Options: RUNNING, STOPPED, PAUSED.\nüì¶ Directory Support for --values-files Teams organizing Kafka configurations by team or environment ‚Äî e.g., configurations/cluster1/resources/teamA/values.yml ‚Äî couldn‚Äôt pass a directory to --values-files. You‚Äôd get java.io.IOException: Is a directory, and wildcard expansion didn‚Äôt work either.\n--values-files now accepts directories. Jikkou recursively loads all matching files and deep-merges their values:\njikkou create --values-files config/environments/prod/ config/environments/prod/ ‚îú‚îÄ‚îÄ teamA/ ‚îÇ ‚îú‚îÄ‚îÄ topics.yml ‚îÇ ‚îî‚îÄ‚îÄ acls.yml ‚îú‚îÄ‚îÄ teamB/ ‚îÇ ‚îî‚îÄ‚îÄ topics.yml ‚îî‚îÄ‚îÄ shared.yaml All files are loaded and their values are merged recursively. No more listing every file individually.\nüìë Jinja Template File Locations Jinja‚Äôs {% include %} directive only resolved templates from the Java classpath. If you wanted to split a large Jikkou YAML into reusable fragments, you had to mess with CLASSPATH_PREFIX or embed files into container images. For a tool that champions declarative configuration, this was a rough edge.\nYou can now configure filesystem directories where Jinja resolves template includes:\njikkou { jinja { resourceLocations = [ \"/etc/jikkou/templates\", \"/opt/shared-templates\" ] } } Your templates can reference local files naturally:\n{%- include \"kafka/topic-defaults.jinja\" -%} {%- import \"macros/common.jinja\" as m -%} {{ m.topic_config(name, partitions) }} No classpath gymnastics required.\nüîß Under the Hood Java 25: The project now targets Java 25 with GraalVM 25.0.2. Native image metadata has been migrated to the newer reachability-metadata format for better compilation support. REST client modernization: Migrated from Jersey to RESTEasy proxy client, removing dependency on Jersey internals. OkHttp upgraded from 4.12.0 to 5.3.2. Security fixes: Addressed CVE-2024-47561, CVE-2025-12183, CVE-2025-55163, and CVE-2026-25526 (Jinjava). Bug fixes: Fixed repository double-loading on reconcile, GitHub repository file-pattern config, invalid supplier for Schema Registry basic auth, and comma-separated key-value pair parsing. Server: API resources now use a blocking executor for improved stability. ‚úÖ Wrapping Up A lot of what‚Äôs in this release came directly from issues and feedback you opened on GitHub ‚Äî so thank you. Keep it coming.\nIf you run into problems, open a GitHub issue. Give us a ‚≠êÔ∏è on GitHub and join the conversation on Slack.\n","categories":"","description":"","excerpt":"üöÄ Introducing Jikkou 0.37.0 We‚Äôre pleased to announce Jikkou 0.37.0. ‚Ä¶","ref":"/docs/releases/release-v0.37.0/","tags":"","title":"Release v0.37.0"},{"body":"üöÄ Introducing Jikkou 0.36.0 We‚Äôre excited to announce the release of Jikkou 0.36.0! üéâ\nThis release brings major new features to make Jikkou more powerful, flexible, and GitOps-friendly than ever before:\nüÜï New resource for managing AWS Glue Schemas üõ°Ô∏è New resource for defining ValidatingResourcePolicy üîé New selector based on Google Common Expression Language üì¶ New concept of Resource Repositories ‚öôÔ∏è Enhanced Kafka actions üîÑ Evolved provider configuration system To install the new version, check out the installation guide.\nFor detailed release notes, see the GitHub page.\nüÜï AWS Glue Schema Provider We know that many developers and DevOps teams rely on Jikkou to manage their AWS MSK clusters.\nWith this release, we‚Äôre going one step further: Jikkou 0.36.0 adds a new provider for AWS Glue Schemas.\nYou can now fully manage schemas registered in AWS Glue Registries ‚Äî just like you already do with Confluent Schema Registry.\nExample:\n# file: ./aws-glue-schema-user.yaml --- apiVersion: \"aws.jikkou.io/v1\" kind: \"AwsGlueSchema\" metadata: name: \"Person\" labels: glue.aws.amazon.com/registry-name: Test annotations: glue.aws.amazon.com/normalize-schema: true spec: compatibility: \"BACKWARD\" dataFormat: \"AVRO\" schemaDefinition: | { \"namespace\": \"example\", \"type\": \"record\", \"name\": \"Person\", \"fields\": [ { \"name\": \"id\", \"type\": \"int\", \"doc\": \"The person's unique ID (required)\" }, { \"name\": \"firstname\", \"type\": \"string\", \"doc\": \"The person's legal firstname (required)\" }, { \"name\": \"lastname\", \"type\": \"string\", \"doc\": \"The person's legal lastname (required)\" } } üëâ With the Jikkou CLI, you can now run commands like:\njikkou get aws-glueschemas üëâ Learn more: AWS Provider Documentation\nüõ°Ô∏è ValidatingResourcePolicy for Smarter Governance Validating resources has always been a challenge in Jikkou.\nEarlier releases provided a validation chain with built-in checks (e.g., TopicMinReplicationFactor, TopicMaxNumPartitions, TopicNamePrefix).\nHowever, these were limited, provider-specific, and mostly resource-scoped.\nWith Jikkou 0.36, we‚Äôre introducing ValidatingResourcePolicy ‚Äî a declarative, reusable way to enforce governance and compliance across any resource.\n‚ö° How It Works A ValidatingResourcePolicy defines:\nSelectors:\nMatch by resource kinds or operations Match by labels Use Google CEL expressions for advanced logic Rules:\nWrite expressions using Google Common Expression Language to enforce validation logic.\nFailure Policies:\nDecide what happens when validation fails:\nFAIL ‚Üí abort the operation CONTINUE ‚Üí log but proceed FILTER ‚Üí automatically remove invalid resources üìë Examples Enforcing min/max partitions for KafkaTopic:\n--- apiVersion: core.jikkou.io/v1 kind: ValidatingResourcePolicy metadata: name: KafkaTopicPolicy spec: failurePolicy: FAIL selector: matchResources: - kind: KafkaTopic rules: - name: MaxTopicPartitions expression: \"resource.spec.partitions \u003c= 50\" messageExpression: \"'Topic partitions MUST be \u003c= 50, but was: ' + string(resource.spec.partitions)\" - name: MinTopicPartitions expression: \"resource.spec.partitions \u003e= 3\" message: \"Topic must have at least 3 partitions\" Filtering out DELETE operations on Kafka Topics:\n--- apiVersion: core.jikkou.io/v1 kind: ValidatingResourcePolicy metadata: name: KafkaTopicPolicy spec: failurePolicy: FILTER selector: matchResources: - kinds: KafkaTopicChange rules: - name: FilterDeleteOperation expression: \"resource.spec.op == 'DELETE'\" messageExpression: \"'Operation ' + resource.spec.op + ' on topics is not authorized'\" üëâ Learn more: ValidatingResourcePolicy Documentation\nüì¶ Resource Repositories ‚Äî GitOps-Friendly Resource Management Managing and sharing resource definitions just got easier.\nJikkou 0.36 introduces Resource Repositories, allowing you to load resources directly from GitHub repositories or local directories.\nRepositories are perfect for:\nReusable resources across multiple environments Shared definitions across teams Keeping transient or computed resources (e.g., ConfigMap, ValidatingResourcePolicy) separate from persistent ones Injecting dynamic configuration without polluting your main repo üí° Use Case Spotlight:\nInstead of keeping temporary validation policies or config maps in your CLI input, you can store them in a repository and inject them dynamically. This makes transient resources clean, shareable, and environment-specific.\nExample Configuration jikkou { repositories = [ { name = \"github-repository\" type = io.streamthoughts.jikkou.core.repository.GitHubResourceRepository config { repository = \"streamthoughts/jikkou\" branch = \"main\" paths = [ \"examples/\", ] # Optionally set an access token for private repositories # token = ${?GITHUB_TOKEN} } } ] } üëâ Learn more: Repositories Documentation\nüîé Expression-based CLI Selectors Selectors just became more powerful with Google CEL.\nYou can now filter resources dynamically based on any attribute.\nExample: List all topics with more than 12 partitions:\njikkou get kafkatopics --selector \"expr: resource.spec.partitions \u003e= 12\" üëâ Learn more: Expression Documentation\n‚öôÔ∏è Actions Improvements Added TruncateKafkaTopicRecords action ‚Üí truncate topic-partitions to a specific datetime. Extended KafkaConsumerGroupsResetOffsets: Reset offsets for multiple consumer groups. New options: --all: apply to all consumer groups --groups: specify consumer groups --includes: regex patterns for inclusion --excludes: regex patterns for exclusion üîÑ Migration: Provider Configurations Starting in 0.36.0, provider configuration has evolved to support future extensibility.\nBefore 0.36.0:\njikkou { extension.providers { kafka.enabled = true } kafka { client { bootstrap.servers = \"localhost:9092\" } } } After 0.36.0:\nprovider.kafka { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { client { bootstrap.servers = \"localhost:9092\" } } } ‚ö†Ô∏è Old configs still work for now, but we recommend migrating.\n‚úÖ Wrapping Up We can‚Äôt wait to see what you build with this new release.\nIf you encounter any issues, please open a GitHub issue on our project page.\nDon‚Äôt forget to give us a ‚≠êÔ∏è on GitHub and join the community on Slack.\n","categories":"","description":"","excerpt":"üöÄ Introducing Jikkou 0.36.0 We‚Äôre excited to announce the release of ‚Ä¶","ref":"/docs/releases/release-v0.36.0/","tags":"","title":"Release v0.36.0"},{"body":"Introducing Jikkou 0.34.0 We‚Äôre excited to unveil the latest release of Jikkou 0.34.0. üéâ\nTo install the new version, please visit the installation guide. For detailed release notes, check out the GitHub page.\nWhat‚Äôs New in Jikkou 0.34.0? Enhanced Aiven provider with support for Kafka topics. Added SSL support for Kafka Connect and Schema Registry Introduced dynamic connection for Kafka Connect clusters Below is a summary of these new features with examples.\nTopic Aiven for Apache Kafka Jikkou 0.34.0 adds a new KafkaTopic kind that can be used to manage kafka Topics directly though the Aiven API.\nYou can list kafka topics using the new command below:\njikkou get avn-kafkatopics In addition, topics can be described, created and updated using the same resource model as the Apache Kafka provider.\n# file:./aiven-kafkat-topics.yaml --- apiVersion: \"kafka.aiven.io/v1beta2\" kind: \"KafkaTopic\" metadata: name: \"test\" labels: tag.aiven.io/my-tag: \"test-tag\" spec: partitions: 1 replicas: 3 configs: cleanup.policy: \"delete\" compression.type: \"producer\" The main advantages of using this new resource kind are the use of the Aiven Token API to authenticate to the Aiven API and the ability to manage tags for kafka topics.\nSSL support for Kafka Connect and Schema Registry Jikkou 0.34.0 also brings SSL support for the Kafka Connect and Schema Registry providers. Therefore, it‚Äôs now possible to configure the providers to authenticate using SSL certificate.\nExample for the Schema Registry:\njikkou { schemaRegistry { url = \"https://localhost:8081\" authMethod = \"SSL\" sslKeyStoreLocation = \"/certs/registry.keystore.jks\" sslKeyStoreType = \"JKS\" sslKeyStorePassword = \"password\" sslKeyPassword = \"password\" sslTrustStoreLocation = \"/certs/registry.truststore.jks\" sslTrustStoreType = \"JKS\" sslTrustStorePassword = \"password\" sslIgnoreHostnameVerification = true } } Dynamically connection for Kafka Connect clusters Before Jikkou 0.34.0, to deploy a Kafka Connect connector, it was mandatory to configure a connection to a target cluster:\njikkou { extensions.provider.kafkaconnect.enabled = true kafkaConnect { clusters = [ { name = \"my-connect-cluster\" url = \"http://localhost:8083\" } ] } } This connection could then be referenced in a connector resource definition via the annotation kafka.jikkou.io/connect-cluster.\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"mu-connector\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" This approach is suitable for most use cases, but can be challenging if you need to manage a large and dynamic number of Kafka Connect clusters.\nTo meet this need, it is now possible to provide connection information for the cluster to connect to directly, through the new metadata annotation new metadata annotation: jikkou.io/config-override.\nHere is a simple example showing the use of the new annotation:\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"mu-connector\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: jikkou.io/config-override: | { \"url\": \"http://localhost:8083\" } This new annotation can be used with the Jikkou‚Äôs Jinja template creation mechanism to define dynamic configuration.\nWrapping Up We hope you enjoy these new features. If you encounter any issues with Jikkou v0.34.0, please feel free to open a GitHub issue on our project page. Don‚Äôt forget to give us a ‚≠êÔ∏è on Github to support the team, and join us on Slack.\n","categories":"","description":"","excerpt":"Introducing Jikkou 0.34.0 We‚Äôre excited to unveil the latest release ‚Ä¶","ref":"/docs/releases/release-v0.34.0/","tags":"","title":"Release v0.34.0"},{"body":"Introducing Jikkou 0.33.0 We‚Äôre excited to unveil the latest release of Jikkou 0.33.0. üéâ\nTo install the new version, please visit the installation guide. For detailed release notes, check out the GitHub page.\nWhat‚Äôs New in Jikkou 0.33.0? Enhanced resource change format. Added support for the patch command. Introduced the new --status option for KafkaTopic resources. Exported offset-lag to the status of KafkaConsumerGroup resources. Below is a summary of these new features with examples.\nDiff/Patch Commands In previous versions, Jikkou provided the diff command to display changes required to reconcile input resources. However, this command lacked certain capabilities to be truly useful. This new version introduces a standardized change format for all resource types, along with two new options for filtering changes:\n--filter-resource-op=: Filters out all state changes except those corresponding to the given operations. --filter-change-op=: Filters out all resources except those corresponding to the given operations. The new output format you can expect from the diff command is as follows:\n--- apiVersion: [ group/version of the change ] kind: [ kind of the change ] metadata: [ resource metadata ] spec: # Array of state changes changes: - name: [ name of the changed state ] op: [ change operation ] before: [ value of the state before the operation ] after: [ value of the state after the operation ] data: [ static data attached to the resource ] op: [ resource change operation ] The primary motivation behind this new format is the introduction of a new patch command. Prior to Jikkou 0.33.0, when using the apply command after a dry-run or a diff command, Jikkou couldn‚Äôt guarantee that the applied changes matched those returned from the previous command. With Jikkou 0.33.0, you can now directly pass the result of the diff command to the new patch command to efficiently apply the desired state changes.\nHere‚Äôs a workflow to create your resources:\nStep 1) Create a resource descriptor file\ncat \u003c\u003c EOF \u003e my-topic.yaml --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic' labels: environment: example spec: partitions: 3 replicas: 1 configs: min.insync.replicas: 1 cleanup.policy: 'delete' EOF Step 2) Run diff\njikkou diff -f ./my-topic.yaml \u003e my-topic-diff.yaml (output) --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicChange\" metadata: name: \"my-topic\" labels: environment: \"example\" annotations: jikkou.io/managed-by-location: \"my-topic.yaml\" spec: changes: - name: \"partitions\" op: \"CREATE\" after: 3 - name: \"replicas\" op: \"CREATE\" after: 1 - name: \"config.cleanup.policy\" op: \"CREATE\" after: \"delete\" - name: \"config.min.insync.replicas\" op: \"CREATE\" after: 1 op: \"CREATE\" Step 3) Run patch\njikkou patch -f ./my-topic-diff.yaml --mode FULL --output compact (output)\nTASK [ CREATE ] Create topic 'my-topic' (partitions=3, replicas=1, configs=[cleanup.policy=delete, min.insync.replicas=1]) - CHANGED EXECUTION in 3s 797ms ok: 0, created: 1, altered: 0, deleted: 0 failed: 0 Attempting to apply the changes a second time may result in an error from the remote system:\n{ \"status\": \"FAILED\", \"description\": \"Create topic 'my-topic' (partitions=3, replicas=1,configs=[cleanup.policy=delete,min.insync.replicas=1])\", \"errors\": [ { \"message\": \"TopicExistsException: Topic 'my-topic' already exists.\" } ], ... } Resource Provider for Apache Kafka Jikkou 0.33.0 also packs with some minor improvements for the Apache Kafka provider.\nKafkaTopic Status You can now describe the status of a topic-partitions by using the new --status option\nwhen getting a KafkaTopic resource.\njikkou get kt --status --selector \"metadata.name IN (my-topic)\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopic\" metadata: name: \"my-topic\" labels: kafka.jikkou.io/topic-id: \"UbZI2N2YQTqfNcbKKHps5A\" annotations: kafka.jikkou.io/cluster-id: \"xtzWWN4bTjitpL3kfd9s5g\" spec: partitions: 1 replicas: 1 configs: cleanup.policy: \"delete\" configMapRefs: [ ] status: partitions: - id: 0 leader: 101 isr: - 101 replicas: - 101 KafkaConsumerGroup OffsetLags With Jikkou 0.33.0, you can export the offset-lag of a KafkaConsumerGroup resource using the --offsets option.\njikkou get kafkaconsumergroups --offsets --- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false status: state: \"EMPTY\" members: [ ] offsets: - topic: \"my-topic\" partition: 0 offset: 16 offset-lag: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 Finally, all those new features are also completely available through the Jikkou REST Server.\nWrapping Up We hope you enjoy these new features. If you encounter any issues with Jikkou v0.33.0, please feel free to open a GitHub issue on our project page. Don‚Äôt forget to give us a ‚≠êÔ∏è on Github to support the team, and join us on Slack.\n","categories":"","description":"","excerpt":"Introducing Jikkou 0.33.0 We‚Äôre excited to unveil the latest release ‚Ä¶","ref":"/docs/releases/release-v0.33.0/","tags":"","title":"Release v0.33.0"},{"body":"Jikkou 0.32.0: Moving Beyond Apache Kafka. Introducing new features: Extension Providers, Actions, etc.! I‚Äôm thrilled to announce the release of Jikkou 0.32.0 which packs two major features: External Extension Providers and Actions. üôÇ\nHighlights: What‚Äôs new in Jikkou 0.32.0? New External Extension Provider mechanism to extend Jikkou features.\nNew extension type Action to execute specific operations against resources.\nNew action for resetting consumer group offsets.\nNew action for restarting connector and tasks for Kafka Connect.\nNew option selector-match to exclude/include resources from being returned or reconciled by Jikkou.\nNew API to get resources by their name.\nExtension Providers Jikkou is a project that continues to reinvent and redefine itself with each new version. Initially developed exclusively to manage the configuration of Kafka topics, it can now be used to manage Schema Registries, Kafka Connect connectors, and more. But, the funny thing is that Jikkou isn‚Äôt coupled with Kafka. It was designed around a concept of pluggable extensions that enable new capabilities and kind of resources to be seamlessly added to the project. For this, Jikkou uses the Java Service Loader mechanism to automatically discover new extensions at runtime.\nUnfortunately, until now there has been no official way of using this mechanism with Jikkou CLI or Jikkou API Server. For this reason, Jikkou 0.32.0 brings the capability to easily configuration external extensions.\nSo how does it work? Well, let‚Äôs imagine you want to be able to load Text Files from the local filesystem using Jikkou.\nFirst, we need to create a new Java project and add the Jikkou Core library to your project‚Äôs dependencies ( io.streamthoughts:jikkou-core:0.32.0 dependency).\nThen, you will need to create some POJO classes to represent your resource (e.g., V1File.class) and to implement the Collector interface :\n@SupportedResource(type = V1File.class) @ExtensionSpec( options = { @ExtensionOptionSpec( name = \"directory\", description = \"The absolute path of the directory from which to collect files\", type = String.class, required = true ) } ) @Description(\"FileCollector allows listing all files in a given directory.\") public final class FileCollector extends ContextualExtension implements Collector\u003cV1File\u003e { private static final Logger LOG = LoggerFactory.getLogger(FileCollector.class); @Override public ResourceListObject\u003cV1File\u003e listAll(@NotNull Configuration configuration, @NotNull Selector selector) { // Get the 'directory' option. String directory = extensionContext().\u003cString\u003econfigProperty(\"directory\").get(configuration); // Collect all files. List\u003cV1File\u003e objects = Stream.of(new File(directory).listFiles()) .filter(file -\u003e !file.isDirectory()) .map(file -\u003e { try { Path path = file.toPath(); String content = Files.readString(path); V1File object = new V1File(ObjectMeta .builder() .withName(file.getName()) .withAnnotation(\"system.jikkou.io/fileSize\", Files.size(path)) .withAnnotation(\"system.jikkou.io/fileLastModifier\", Files.getLastModifiedTime(path)) .build(), new V1FileSpec(content) ); return Optional.of(object); } catch (IOException e) { LOG.error(\"Cannot read content from file: {}\", file.getName(), e); return Optional.\u003cV1File\u003eempty(); } }) .flatMap(Optional::stream) .toList(); ObjectMeta objectMeta = ObjectMeta .builder() .withAnnotation(\"system.jikkou.io/directory\", directory) .build(); return new DefaultResourceListObject\u003c\u003e(\"FileList\", \"system.jikkou.io/v1\", objectMeta, objects); } } Next, you will need to implement the ExtensionProvider interface to register both your extension and your resource kind.\npublic final class FileExtensionProvider implements ExtensionProvider { /** * Registers the extensions for this provider. * * @param registry The ExtensionRegistry. */ public void registerExtensions(@NotNull ExtensionRegistry registry) { registry.register(FileCollector.class, FileCollector::new); } /** * Registers the resources for this provider. * * @param registry The ResourceRegistry. */ public void registerResources(@NotNull ResourceRegistry registry) { registry.register(V1File.class); } } Then, the fully qualified name of the class must be added to the resource file META-INF/service/io.streamthoughts.jikkou.spi.ExtensionProvider.\nFinally, all you need to do is to package your project as a tarball or ZIP archive. The archive must contain a single top-level directory containing the extension JAR files, as well as any resource files or third-party libraries required by your extensions.\nTo install your Extension Provider, all you need to do is to unpacks the archive into a desired location (e.g., /usr/share/jikkou-extensions) and to configure the Jikkou‚Äôs API Server or Jikkou CLI (when running the Java Binary Distribution, i.e., not the native version) with the jikkou.extension.paths property (e.g., jikkou.extension.paths=/usr/share/jikkou-extensions). For people who are familiar with how Kafka Connect works, it‚Äôs more or less the same mechanism.\n(The full code source of this example is available on GitHub).\nAnd that‚Äôs it! üôÇ\nExtension Providers open up the possibility of infinitely expanding Jikkou to manage your own resources, and use it with systems other than Kafka.\nActions Jikkou uses a declarative approach to manage the asset state of your data infrastructure using resource descriptors written in YAML. But sometimes, ops and development teams may need to perform specific operations on their resources that cannot be included in their descriptor files. For example, you may need to reset offsets for one or multiple Kafka Consumer Groups, restart failed connectors and tasks for Kafka Connect, etc. So instead of having to switch from one tool to another, why not just use Jikkou for this?\nWell, to solve that need, Jikkou 0.32.0 introduces a new type of extension called ‚ÄúActions‚Äù that allows users to perform specific operations on resources.\nCombined with the External Extension Provider mechanism, you can now implement the Action interface to add custom operations to Jikkou.\n@Category(ExtensionCategory.ACTION) public interface Action\u003cT extends HasMetadata\u003e extends HasMetadataAcceptable, Extension { /** * Executes the action. * * @param configuration The configuration * @return The ExecutionResultSet */ @NotNull ExecutionResultSet\u003cT\u003e execute(@NotNull Configuration configuration); } Actions are fully integrated with Jikkou API Server through the new Endpoint: /api/v1/actions/{name}/execute{?[options]\nKafka Consumer Groups Altering Consumer Group Offsets Jikkou 0.32.0 introduces the new KafkaConsumerGroupsResetOffsets action allows resetting offsets of consumer groups.\nHere is an example showing how to reset the group my-group to the earliest offsets for topic test.\n$ jikkou action kafkaconsumergroupresetoffsets execute \\ --group my-group \\ --topic test \\ --to-earliest (output)\nkind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: { } annotations: configs.jikkou.io/to-earliest: \"true\" configs.jikkou.io/group: \"my-group\" configs.jikkou.io/dry-run: \"false\" configs.jikkou.io/topic: - \"test\" results: - status: \"SUCCEEDED\" errors: [ ] data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false annotations: { } status: state: \"EMPTY\" members: [ ] offsets: - topic: \"test\" partition: 1 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 This action is pretty similar to the kafka-consumer-group script that ships with Apache Kafka. You can use it to reset a consumer group to the earliest or latest offsets, to a specific datetime or specific offset.\nIn addition, it can be executed in a dry-run.\nDeleting Consumer Groups You can now add the core annotation jikkou.io/delete to a KafkaConsumerGroup resource to mark it for deletion:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false annotations: jikkou.io/delete: true $ jikkou delete --files my-consumer-group.yaml -o wide TASK [DELETE] Delete consumer group 'my-group' - CHANGED ************************************************ { \"status\" : \"CHANGED\", \"changed\" : true, \"failed\" : false, \"end\" : 1701162781494, \"data\" : { \"apiVersion\" : \"core.jikkou.io/v1beta2\", \"kind\" : \"GenericResourceChange\", \"metadata\" : { \"name\" : \"my-group\", \"labels\" : { \"kafka.jikkou.io/is-simple-consumer\" : false }, \"annotations\" : { \"jikkou.io/delete\" : true, \"jikkou.io/managed-by-location\" : \"./my-consumer-group.yaml\" } }, \"change\" : { \"before\" : { \"apiVersion\" : \"kafka.jikkou.io/v1beta1\", \"kind\" : \"KafkaConsumerGroup\", \"metadata\" : { \"name\" : \"my-group\", \"labels\" : { \"kafka.jikkou.io/is-simple-consumer\" : false }, \"annotations\" : { } } }, \"operation\" : \"DELETE\" } }, \"description\" : \"Delete consumer group 'my-group'\" } EXECUTION in 64ms ok : 0, created : 0, altered : 0, deleted : 1 failed : 0 Kafka Connect Restarting Connector and Tasks This new release also packs with the new action KafkaConnectRestartConnectors action allows a user to restart all or just the failed Connector and Task instances for one or multiple named connectors.\nHere are a few examples from the documentation:\nRestarting all connectors for all Kafka Connect clusters. $ jikkou action kafkaconnectrestartconnectors execute --- kind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: {} annotations: {} results: - status: \"SUCCEEDED\" data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: {} spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" workerId: \"connect:8083\" tasks: - id: 0 state: \"RUNNING\" workerId: \"connect:8083\" Restarting a specific connector and tasks for on Kafka Connect cluster $ jikkou action kafkaconnectrestartconnectors execute \\ --cluster-name my-connect-cluster --connector-name local-file-sink \\ --include-tasks New Selector Matching Strategy Jikkou CLI allows you to provide one or multiple *selector expressions *in order to include or exclude resources from being returned or reconciled by Jikkou. In previous versions, selectors were cumulative, so resources had to match all selectors to be returned. Now, you can specify a selector matching strategy to determine how expressions must be combined using the option ‚Äìselector-match=[ANY|ALL|NONE].\nALL: A resource is selected if it matches all selectors.\nANY: A resource is selected if it matches one of the selectors.\nNONE: A resource is selected if it matches none of the selectors.\nFor example, the below command will only return topics matching the regex ^__.* or having a name equals to _schemas.\n$ jikkou get kafkatopics \\ --selector 'metadata.name MATCHES (^__connect-*)' --selector 'metadata.name IN (_schemas)' --selector-match ANY New Get Resource by Name In some cases, it may be necessary to retrieve only a specific resource for a specific name. In previous versions, the solution was to use selectors. Unfortunately, this approach isn‚Äôt very efficient, as it involves retrieving all the resources and then filtering them. To start solving that issue, Jikkou v0.32.0 adds a new API to retrieve a resource by its name.\nExample (using Jikkou CLI):\n$ jikkou get kafkatopics --name _schemas Example (using Jikkou API Server):\n$ curl -sX GET \\ http://localhost:28082/apis/kafka.jikkou.io/v1/kafkatopics/_schemas \\ -H 'Accept:application/json' Note : Currently not all resources have been updated to use that new API, so it‚Äôs possible that selectors are used as a default implementation by internal Jikkou API.\n","categories":"","description":"","excerpt":"Jikkou 0.32.0: Moving Beyond Apache Kafka. Introducing new features: ‚Ä¶","ref":"/docs/releases/release-v0.32.0/","tags":"","title":"Release v0.32.0"},{"body":"Running Server on a Specific Port By default, the server runs on port 28082. However, you can set the server to run on a specific port:\n# ./etc/application.yaml micronaut: server: port: 80 # Port used to access APIs endpoints: all: port: 80 # Port used to access Health endpoints Enabling Specific Extension Providers By default, the server is configured to run only with the core and kafka extension providers. However, you can enable (or disable) additional providers:\njikkou: # The providers provider: # Core core: enabled: true type: io.streamthoughts.jikkou.core.CoreExtensionProvider # Default configuration for Apache Kafka kafka: enabled: true type: io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config: client: bootstrap.servers: localhost:9092 # Default configuration for Schema Registry schemaregistry: enabled: true type: io.streamthoughts.jikkou.schema.registry.SchemaRegistryExtensionProvider config: url: http://localhost:8081 # Default configuration for Kafka Connect kafkaconnect: enabled: true type: io.streamthoughts.jikkou.kafka.connect.KafkaConnectExtensionProvider config: clusters: - name: localhost url: http://localhost:8083 ","categories":"","description":"Learn how to configure the Jikkou API server.\n","excerpt":"Learn how to configure the Jikkou API server.\n","ref":"/docs/jikkou-api-server/configuration/server_configuration/","tags":"","title":"API Server"},{"body":"You can use a ConfigMap to define reusable data in the form of key/value pairs that can then be referenced and used by other resources.\nSpecification --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: '\u003cCONFIG-MAP-NAME\u003e' # Name of the ConfigMap (required) data: # Map of key-value pairs (required) \u003cKEY_1\u003e: \"\u003cVALUE_1\u003e\" Example For example, the below ConfigMap show how to define default config properties namedcKafkaTopicConfig that can then reference and used to define multiple KafkaTopic. resources.\n--- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'KafkaTopicConfig' data: cleanup.policy: 'delete' min.insync.replicas: 2 retention.ms: 86400000 # (1 day) ","categories":"","description":"Learn how to use ConfigMap objects.\n","excerpt":"Learn how to use ConfigMap objects.\n","ref":"/docs/providers/core/resources/configmap/","tags":"","title":"ConfigMap"},{"body":"This document will guide you through setting up Jikkou in a few minutes and managing your first resources with Jikkou.\nPrerequisites The following prerequisites are required for a successful and properly use of Jikkou.\nMake sure the following is installed:\nAn Apache Kafka cluster. Using Docker, Docker Compose is the easiest way to use it. Java 25 (not required when using the binary version). Start your local Apache Kafka Cluster You must have access to an Apache Kafka cluster for using Jikkou. Most of the time, the latest version of Jikkou is always built for working with the most recent version of Apache Kafka.\nMake sure the Docker is up and running.\nThen, run the following commands:\n$ git clone https://github.com/streamthoughts/jikkou $ cd jikkou $ ./up # use ./down for stopping the docker-compose stack Run Jikkou Download the latest distribution (For Linux) Run the following commands to install the latest version:\nwget https://github.com/streamthoughts/jikkou/releases/download/v0.37.0/jikkou-0.37.0-linux-x86_64.zip \u0026\u0026 \\ unzip jikkou-0.37.0-linux-x86_64.zip \u0026\u0026 \\ cp jikkou-0.37.0-linux-x86_64/bin/jikkou $HOME/.local/bin \u0026\u0026 \\ source \u003c(jikkou generate-completion) \u0026\u0026 \\ jikkou --version For more details, or for other options, see the installation guide.\nConfigure Jikkou for your local Apache Kafka cluster Set configuration context for localhost\njikkou config set-context localhost \\ --provider kafka \\ --config-props client.bootstrap.servers=localhost:9092 Show the complete configuration.\njikkou config view --name localhost Finally, let‚Äôs check if your cluster is accessible:\njikkou health get kafka (output)\nIf OK, you should get an output similar to :\n--- apiVersion: \"core.jikkou.io/v1\" kind: \"ApiHealthResult\" name: \"kafka\" status: name: \"UP\" details: resource: \"urn:kafka:cluster:id:xtzWWN4bTjitpL3kfd9s5g\" brokers: - id: \"101\" host: \"localhost\" port: 9092 Create your first topics First, create a resource YAML file describing the topics you want to create on your cluster:\nfile: kafka-topics.yaml\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following Jikkou command to trigger the topic creation on the cluster:\njikkou create -f ./kafka-topics.yaml (output)\nTASK [CREATE] Create topic 'my-second-topic' (partitions=4, replicas=-1, configs=[cleanup.policy=delete]) - CHANGED { \"end\" : \"2025-08-26T00:00:00.000000Z\", \"status\" : \"CHANGED\", \"description\" : \"Create topic 'my-second-topic' (partitions=4, replicas=-1, configs=[cleanup.policy=delete])\", \"change\" : { \"apiVersion\" : \"kafka.jikkou.io/v1beta2\", \"kind\" : \"KafkaTopicChange\", \"metadata\" : { \"name\" : \"my-second-topic\", \"labels\" : { }, \"annotations\" : { \"jikkou.io/items-count\" : 2 } }, \"spec\" : { \"changes\" : [ { \"name\" : \"partitions\", \"op\" : \"CREATE\", \"after\" : 4 }, { \"name\" : \"replicas\", \"op\" : \"CREATE\", \"after\" : -1 }, { \"name\" : \"config.cleanup.policy\", \"op\" : \"CREATE\", \"after\" : \"delete\" } ], \"op\" : \"CREATE\", \"data\" : { } } }, \"changed\" : true, \"failed\" : false } TASK [CREATE] Create topic 'my-first-topic' (partitions=5, replicas=-1, configs=[cleanup.policy=compact]) - CHANGED { \"end\" : \"2025-08-26T00:00:00.000000Z\", \"status\" : \"CHANGED\", \"description\" : \"Create topic 'my-first-topic' (partitions=5, replicas=-1, configs=[cleanup.policy=compact])\", \"change\" : { \"apiVersion\" : \"kafka.jikkou.io/v1beta2\", \"kind\" : \"KafkaTopicChange\", \"metadata\" : { \"name\" : \"my-first-topic\", \"labels\" : { }, \"annotations\" : { \"jikkou.io/items-count\" : 2 } }, \"spec\" : { \"changes\" : [ { \"name\" : \"partitions\", \"op\" : \"CREATE\", \"after\" : 5 }, { \"name\" : \"replicas\", \"op\" : \"CREATE\", \"after\" : -1 }, { \"name\" : \"config.cleanup.policy\", \"op\" : \"CREATE\", \"after\" : \"compact\" } ], \"op\" : \"CREATE\", \"data\" : { } } }, \"changed\" : true, \"failed\" : false } EXECUTION in 114ms ok : 0, created : 2, altered : 0, deleted : 0 failed : 0 Tips In the above command, we chose to use the create command to create the new topics. But we could just as easily use the update or apply command to get the same result depending on our needs. Finally, you can verify that topics are created on the cluster\njikkou get kafkatopics --default-configs Tips We use the --default-configs to export built-in default configuration for configs that have a default value. Update Kafka Topics Edit your kafka-topics.yaml to add a retention.ms: 86400000 property to the defined topics.\nThen, run the following command.\njikkou update -f ./kafka-topics.yaml Delete Kafka Topics To delete all topics defines in the topics.yaml, add an annotation jikkou.io/delete: true as follows:\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: annotations: # Annotation to specify that all resources must be deleted. jikkou.io/delete: true items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following command:\n$ jikkou apply \\ --files ./kafka-topics.yaml \\ --selector \"metadata.name MATCHES (my-.*-topic)\" \\ --dry-run Using the dry-run option, give you the possibility to check the changes that will be made before applying them.\nNow, rerun the above command without the --dry-run option to definitively delete the topics.\nRecommendation When working in a production environment, we strongly recommend running commands with a --selector option to ensure that changes are only applied to a specific set of resources. Also, always run your command in --dry-run mode to verify the changes that will be executed by Jikkou before continuing. Reading the Help To learn more about the available Jikkou commands, use jikkou help or type a command followed by the -h flag:\n$ jikkou help get Next Steps Now, you‚Äôre ready to use Jikkou!üöÄ\nAs next steps, we suggest reading the following documentation in this order:\nLearn Jikkou concepts Read the Developer Guide to understand how to use the Jikkou API for Java Look at the examples ","categories":"","description":"This guide covers how you can quickly get started using Jikkou.\n","excerpt":"This guide covers how you can quickly get started using Jikkou.\n","ref":"/docs/tutorials/get_started/","tags":"","title":"Jikkou Getting Started"},{"body":"Overview The GithubResourceRepository can be used to load resources from a public or private GitHub repository.\nConfiguration jikkou { repositories = [ { # Name of your local repositories name = \"\u003cstring\u003e\" # The fully qualified class name (FQCN) of the repository type = io.streamthoughts.jikkou.core.repository.GithubResourceRepository config { # Specify the GitHub repository in the format 'owner/repo' repository = \"\u003cstring\u003e\" # Specify the branch or ref to load resources from branch = main # Specify the paths/directories in the repository containing the resource definitions paths = [] # Default: **/*.{yaml,yml} # Specify the pattern used to match YAML file paths. # Pattern should be passed in the form of 'syntax:pattern'. The \"glob\" and \"regex\" syntaxes are supported (e.g.: **/*.{yaml,yml}). # If no syntax is specified the 'glob' syntax is used. file-pattern = \"**/*.{yaml,yml}\" # Specify the locations of the values-files containing the variables to pass into the template engine built-in object 'Values'. values-files = [] # Specify the pattern used to match YAML file paths when one or multiple directories are given through the `values-files` property. # Pattern should be passed in the form of 'syntax:pattern'. The \"glob\" and \"regex\" syntaxes are supported (e.g.: **/*.{yaml,yml}). # If no syntax is specified the 'glob' syntax is used. values-file-name = \"\u003cstring\u003e\" # Default: **/*.{yaml,yml} # The labels to be added to all resources loaded from the repository labels { \u003clabel_key\u003e = \u003clabel_value\u003e } } } ] } ","categories":"","description":"Load resources from a public or private GitHub repository.","excerpt":"Load resources from a public or private GitHub repository.","ref":"/docs/providers/core/repositories/github/","tags":"","title":"Github Resource Repository"},{"body":"Releases The latest stable release of Jikkou API Server is available:\nAs a Java binary distribution (.zip) from GitHub Releases As a docker image available from Docker Hub. Standalone Installation Follow these few steps to download the latest stable versions and get started.\nPrerequisites To be able to run Jikkou API Server, the only requirement is to have a working Java 21 installation. You can check the correct installation of Java by issuing the following command:\njava -version Step 1: Download Download the latest Java binary distribution from the GitHub Releases (e.g. jikkou-api-server-0.36.0.zip)\nUnpack the download distribution and move the unpacked directory to a desired destination\nunzip jikkou-api-server-$LATEST_VERSION.zip mv jikkou-api-server-$LATEST_VERSION /opt/jikkou Step 2: Start the API Server Launch the application with:\n./bin/jikkou-api-server.sh Step 3: Test the API Server $ curl -sX GET http://localhost:28082 -H \"Accept: application/json\" | jq { \"version\": \"0.31.0\", \"build_time\": \"2023-11-14T18:07:38+0000\", \"commit_id\": \"dae1be11c092256f36c18c8f1d90f16b0c951716\", \"_links\": { \"self\": { \"href\": \"/\", \"templated\": false }, \"get-apis\": { \"href\": \"/apis\", \"templated\": false } } } Step 4: Stop the API Server PID=`ps -ef | grep -v grep | grep JikkouApiServer | awk '{print $2}'` kill $PID Docker # Run Docker docker run -it \\ --net host \\ streamthoughts/jikkou-api-server:latest Development Builds In addition to releases you can download or install development snapshots of Jikkou API Server.\nFrom Docker Hub Docker images are built and push to Docker Hub from the latest main branch.\nThey are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features.\n$ docker run -it streamthoughts/jikkou-api-server:main ","categories":"","description":"This guide shows how to install Jikkou API Server.\n","excerpt":"This guide shows how to install Jikkou API Server.\n","ref":"/docs/jikkou-api-server/install/","tags":"","title":"Install Jikkou API Server"},{"body":"Overview The LocalResourceRepository can be used to load resources from local files or directories.\nConfiguration jikkou { repositories = [ { # Name of your local repositories name = \"\u003cstring\u003e\" # The fully qualified class name (FQCN) of the repository type = io.streamthoughts.jikkou.core.repository.LocalResourceRepository config { # Specify the locations containing the definitions for resources in a YAML file, a directory. files = [] # Specify the pattern used to match YAML file paths when one or multiple directories are given through the `files` property. # Pattern should be passed in the form of 'syntax:pattern'. The \"glob\" and \"regex\" syntaxes are supported (e.g.: **/*.{yaml,yml}). # If no syntax is specified the 'glob' syntax is used. file-name = \"\u003cstring\u003e\" # Default: **/*.{yaml,yml} # Specify the locations of the values-files containing the variables to pass into the template engine built-in object 'Values'. values-files = [] # Specify the pattern used to match YAML file paths when one or multiple directories are given through the `values-files` property. # Pattern should be passed in the form of 'syntax:pattern'. The \"glob\" and \"regex\" syntaxes are supported (e.g.: **/*.{yaml,yml}). # If no syntax is specified the 'glob' syntax is used. values-file-name = \"\u003cstring\u003e\" # Default: **/*.{yaml,yml} # The labels to add to all resources loaded from the repository labels { \u003clabel_key\u003e = \u003clabel_value\u003e } } } ] } ","categories":"","description":"Load resources from local files or directories.","excerpt":"Load resources from local files or directories.","ref":"/docs/providers/core/repositories/local/","tags":"","title":"Local Resource Repository"},{"body":"The command line interface to Jikkou is the jikkou command, which accepts a variety of subcommands such as jikkou apply or jikkou validate.\nTo view a list of the commands available in your current Jikkou version, run jikkou with no additional arguments:\nUsage: jikkou [-hV] [--logger-level=\u003clevel\u003e] [COMMAND] Jikkou CLI:: A command-line client designed to provide an efficient and easy way to manage, automate, and provision resources. Find more information at: https://www.jikkou.io/. OPTIONS: -h, --help Show this help message and exit. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -V, --version Print version information and exit. CORE COMMANDS: apply Update the resources as described by the resource definition files. create Create resources from the resource definition files (only non-existing resources will be created). delete Delete resources that are no longer described by the resource definition files. diff Show resource changes required by the current resource definitions. get Display one or many specific resources. patch Execute all changes for the specified reconciliation mode. prepare Prepare the resource definition files for validation. replace Replace all resources. update Create or update resources from the resource definition files validate Check whether the resources definitions meet all validation requirements. SYSTEM MANAGEMENT COMMANDS: action List/execute actions. health Print or describe health indicators. ADDITIONAL COMMANDS: api-extensions Print the supported API extensions api-resources Print the supported API resources config Sets or retrieves the configuration of this client generate-completion Generate bash/zsh completion script for jikkou. help Display help information about the specified command. See 'jikkou --help' for more information about a command. (The output from your current Jikkou version may be different than the above example.)\nFor detailed options and usage examples for each command, see the Commands Reference.\nChecking Jikkou Version Run the jikkou --version to display your current installation version:\nJikkou version \"0.37.0\" 2026-02-17 JVM: 25.0.1 (GraalVM Community Substrate VM 25.0.1+8) Shell Tab-completion It is recommended to install the bash/zsh completion script jikkou_completion.\nThe completion script can be downloaded from the project Github repository:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/main/jikkou_completion -O jikkou_completion or alternatively, you can run the following command to generate it.\nsource \u003c(jikkou generate-completion) ","categories":"","description":"Global options, version information, and shell tab-completion.\n","excerpt":"Global options, version information, and shell tab-completion.\n","ref":"/docs/jikkou-cli/basic-cli-features/","tags":"","title":"Overview"},{"body":" Welcome to the Jikkou documentation! Jikkou, means ‚Äúexecution (e.g. of a plan) or actual state (of things)‚Äù in Japanese.\nWhat Is Jikkou ? Jikkou is a powerful, flexible open-source framework that enables self-serve resource provisioning. It allows developers and DevOps teams to easily manage, automate, and provision all the resources needed for their Apache Kafka¬Æ platform.\nJikkou was born with the aim to streamline day-to-day operations on Apache Kafka¬Æ, ensuring that platform governance is no longer a tedious and boring task for both developers and administrators.\nWhat Are The Use-Cases ? Jikkou is primarily used as a GitOps solution for Kafka configuration management.\nHere are some of the various use cases we‚Äôve observed in different projects:\nTopic as a Service: Build a self-serve platform for managing Kafka topics. ACL Management: Centrally manage all ACLs of an Apache Kafka cluster. Kafka Connectors Management: Deploy and manage Kafka Connect connectors. Ad Hoc Changes: Apply ad hoc changes as needed. Audit: Easily check configurations of topics, brokers, or identify divergences between different environments. Kafka Configuration Backup: Periodically export all critical configurations of your Kafka cluster. Configuration Replication: Replicate the Kafka configuration from one cluster to another. How Does Jikkou Work ? Jikkou offers flexibility in deployment, functioning either as a simple CLI (Command Line Interface) or as a REST server, based on your requirements.\nBy adopting a stateless approach, Jikkou does not store any internal state. Instead, it leverages your platforms or services as the source of truth. This design enables seamless integration with other solutions, such as Ansible and Terraform, or allows for ad hoc use for specific tasks, making Jikkou incredibly flexible and versatile.\nIs Jikkou For Me ? Jikkou can be implemented regardless of the size of your team or data platform.\nSmall Development Team Jikkou is particularly useful for small development teams looking to quickly automate the creation and maintenance of their topics without having to implement a complex solution that requires learning a new technology or language.\nCentralized Infrastructure (DevOps) Team Jikkou can be very effective in larger contexts, where the configuration of your Kafka Topics, ACLs, and Quotas for all your data platform is managed by a single and centralized devops team.\nDecentralized Data Product Teams In an organization adopting Data Mesh principles, Jikkou can be leveraged in a decentralized way by each of your Data Teams to manage all the assets (e.g. Topics, ACLs, Schemas, Connectors, etc.) necessary to expose and manage their Data Products.\nCan I Use Jikkou with my Apache Kafka vendor ? Jikkou can be used any Apache Kafka infrastructures, including:\nApache Kafka Aiven Amazon MSK Confluent Cloud Redpanda ","categories":"","description":"What is Jikkou ?","excerpt":"What is Jikkou ?","ref":"/docs/overview/","tags":"","title":"Overview"},{"body":"Packaging Extensions You can extend Jikkou‚Äôs capabilities by developing custom extensions and resources.\nAn extension must be developed in Java and packaged as a tarball or ZIP archive. The archive must contain a single top-level directory containing the extension JAR files, as well as any resource files or third party libraries required by your extensions. An alternative approach is to create an uber-JAR that contains all the extension‚Äôs JAR files and other resource files needed.\nAn extension package is more commonly described as an Extension Provider.\nDependencies Jikkou‚Äôs sources are available on Maven Central\nTo start developing custom extension for Jikkou, simply add the Core library to your project‚Äôs dependencies.\nFor Maven:\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-core\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e For Gradle:\nimplementation group: 'io.streamthoughts', name: 'jikkou-core', version: ${jikkou.version} Extension Discovery Jikkou uses the standard Java ServiceLoader mechanism to discover and registers custom extensions and resources. For this, you will need to the implement the Service Provider Interface: io.streamthoughts.jikkou.spi.ExtensionProvider\n/** * \u003cpre\u003e * Service interface for registering extensions and resources to Jikkou at runtime. * The implementations are discovered using the standard Java {@link java.util.ServiceLoader} mechanism. * * Hence, the fully qualified name of the extension classes that implement the {@link ExtensionProvider} * interface must be added to a {@code META-INF/services/io.streamthoughts.jikkou.spi.ExtensionProvider} file. * \u003c/pre\u003e */ public interface ExtensionProvider extends HasName, Configurable { /** * Registers the extensions for this provider. * * @param registry The ExtensionRegistry. */ void registerExtensions(@NotNull ExtensionRegistry registry); /** * Registers the resources for this provider. * * @param registry The ResourceRegistry. */ void registerResources(@NotNull ResourceRegistry registry); } Recommendations If you are using Maven as project management tool, we recommended to use the Apache Maven Assembly Plugin to package your extensions as a tarball or ZIP archive.\nSimply create an assembly descriptor in your project as follows:\nsrc/main/assembly/package.xml\n\u003cassembly xmlns=\"http://maven.apache.org/ASSEMBLY/2.2.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/ASSEMBLY/2.2.0 http://maven.apache.org/xsd/assembly-2.2.0.xsd\"\u003e \u003cid\u003epackage\u003c/id\u003e \u003cformats\u003e \u003cformat\u003ezip\u003c/format\u003e \u003c/formats\u003e \u003cincludeBaseDirectory\u003efalse\u003c/includeBaseDirectory\u003e \u003cfileSets\u003e \u003cfileSet\u003e \u003cdirectory\u003e${project.basedir}\u003c/directory\u003e \u003coutputDirectory\u003e${organization.name}-${project.artifactId}/doc\u003c/outputDirectory\u003e \u003cincludes\u003e \u003cinclude\u003eREADME*\u003c/include\u003e \u003cinclude\u003eLICENSE*\u003c/include\u003e \u003cinclude\u003eNOTICE*\u003c/include\u003e \u003c/includes\u003e \u003c/fileSet\u003e \u003c/fileSets\u003e \u003cdependencySets\u003e \u003cdependencySet\u003e \u003coutputDirectory\u003e${organization.name}-${project.artifactId}/lib\u003c/outputDirectory\u003e \u003cuseProjectArtifact\u003etrue\u003c/useProjectArtifact\u003e \u003cuseTransitiveFiltering\u003etrue\u003c/useTransitiveFiltering\u003e \u003cunpack\u003efalse\u003c/unpack\u003e \u003cexcludes\u003e \u003cexclude\u003eio.streamthoughts:jikkou-core\u003c/exclude\u003e \u003c/excludes\u003e \u003c/dependencySet\u003e \u003c/dependencySets\u003e \u003c/assembly\u003e Then, configure the maven-assembly-plugin in the pom.xml file of your project:\n\u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-assembly-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cfinalName\u003e${organization.name}-${project.artifactId}-${project.version}\u003c/finalName\u003e \u003cappendAssemblyId\u003efalse\u003c/appendAssemblyId\u003e \u003cdescriptors\u003e \u003cdescriptor\u003esrc/assembly/package.xml\u003c/descriptor\u003e \u003c/descriptors\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003emake-assembly\u003c/id\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003cexecution\u003e \u003cid\u003etest-make-assembly\u003c/id\u003e \u003cphase\u003epre-integration-test\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e Finally, use the mvn clean package to build your project and create the archive.\nInstalling Extension Providers To install an Extension Provider, all you need to do is to unpacks the archive into a desired location ( e.g., /usr/share/jikkou-extensions). Also, you should ensure that the archive‚Äôs top-level directory name is unique, to prevent overwriting existing files or extensions.\nConfiguring Extension Providers Custom extensions can be supplied to the Jikkou‚Äôs API Server and Jikkou CLI (when running the Java Binary Distribution, i.e., not the native version). For this, you simply need to configure the jikkou.extension.paths property. The property accepts a list of paths from which to load extension providers.\nExample for the Jikkou API Server:\n# application.yaml jikkou: extension.paths: - /usr/share/jikkou-extensions Once your extensions are configured you should be able to list your extensions using either :\nThe Jikkou CLI: jikkou api-extensions list command, or The Jikkou API Server: GET /apis/core.jikkou.io/v1/extensions -H \"Accept: application/json\" ","categories":"","description":"Learn how to package and install custom extensions for Jikkou.\n","excerpt":"Learn how to package and install custom extensions for Jikkou.\n","ref":"/docs/developer-guide/extensions/package/","tags":"","title":"Package Extensions"},{"body":" Here, you will find the list of core Resource Repositories supported for Jikkou.\nCore Repositories More information:\n","categories":"","description":"","excerpt":" Here, you will find the list of core Resource Repositories supported ‚Ä¶","ref":"/docs/providers/core/repositories/","tags":"","title":"Resource Repositories"},{"body":" Jikkou Resources are entities that represent the state of a concrete instance of a concept that are part of the state of your system, like a Topic on an Apache Kafka cluster.\nResource Objects All resources can be distinguished between persistent objects, which are used to describe the desired state of your system, and transient objects, which are only used to enrich or provide additional capabilities for the definition of persistent objects.\nA resource is an object with a type (called a Kind) and a concrete model that describe the associated data. All resource are scoped by an API Group and Version.\nResource Definition Resources are described in YAML format.\nHere is a sample resource that described a Kafka Topic.\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic' labels: environment: test annotations: {} spec: partitions: 1 replicas: 1 configs: min.insync.replicas: 1 cleanup.policy: 'delete' Resource Properties The following are the properties that can be set to describe a resource:\nProperty Description apiVersion The group/version of the resource type. kind The type of the describe resource. metadata.name An optional name to identify the resource. metadata.labels Arbitrary metadata to attach to the resource that can be handy when you have a lot of resources and you only need to identity or filters some objects. metadata.annotations Arbitrary non-identifying metadata to attach to the resource to mark them for a specific operation or to record some metadata. spec The object properties describing a desired state ","categories":"","description":"","excerpt":" Jikkou Resources are entities that represent the state of a concrete ‚Ä¶","ref":"/docs/concepts/resource/","tags":["concept"],"title":"Resource"},{"body":" Here, you will find the list of core resources supported for Jikkou.\nCore Resources More information:\n","categories":"","description":"","excerpt":" Here, you will find the list of core resources supported for Jikkou. ‚Ä¶","ref":"/docs/providers/core/resources/","tags":"","title":"Resources"},{"body":"Configuration To set up the configuration settings used by Jikkou CLI, you will need create a jikkou config file, which is created automatically when you create a configuration context using:\njikkou config set-context \u003ccontext-name\u003e [--config-file=\u003cconfig-file\u003e] [--config-props=\u003cconfig-value\u003e] By default, the configuration of jikkou is located under the path $HOME/.jikkou/config.\nThis jikkou config file defines all the contexts that can be used by jikkou CLI.\nFor example, below is the config file created during the Getting Started.\n{ \"currentContext\": \"localhost\", \"localhost\": { \"configFile\": null, \"configProps\": { \"provider.kafka.client.bootstrap.servers\": \"localhost:9092\" } } } Most of the time, a context does not directly contain the configuration properties to be used, but rather points to a specific HOCON (Human-Optimized Config Object Notation) through the configFile property.\nThen, the configProps allows you to override some of the property define by this file.\nIn addition, if no configuration file path is specified, Jikkou will lookup for an application.conf to those following locations:\n./application.conf $HOME/.jikkou/application.conf Finally, Jikkou always fallback to a reference.conf file that you can use as a template to define your own configuration.\nreference.conf:\njikkou { # Configure Jikkou Proxy Mode # proxy { # url = \"http://localhost:8080\" # } # Configure Extension Providers extension { } # Core provider.core { enabled = true } # Apache Kafka Provider provider.kafka { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { # The default Kafka Client configuration client { bootstrap.servers = \"localhost:9092\" bootstrap.servers = ${?JIKKOU_DEFAULT_KAFKA_BOOTSTRAP_SERVERS} } brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } } # Schema Registry Provider provider.schemaregistry { enabled = true type = io.streamthoughts.jikkou.schema.registry.SchemaRegistryExtensionProvider config = { url = \"http://localhost:8081\" url = ${?JIKKOU_DEFAULT_SCHEMA_REGISTRY_URL} } } # The default custom transformations to apply on any resources. transformations = [] # The default custom validations to apply on any resources. validations = [ { name = \"topicMustHaveValidName\" type = io.streamthoughts.jikkou.kafka.validation.TopicNameRegexValidation priority = 100 config = { topicNameRegex = \"[a-zA-Z0-9\\\\._\\\\-]+\" topicNameRegex = ${?VALIDATION_DEFAULT_TOPIC_NAME_REGEX} } }, { name = \"topicMustHavePartitionsEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation priority = 100 config = { topicMinNumPartitions = 1 topicMinNumPartitions = ${?VALIDATION_DEFAULT_TOPIC_MIN_NUM_PARTITIONS} } }, { name = \"topicMustHaveReplicasEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactorValidation priority = 100 config = { topicMinReplicationFactor = 1 topicMinReplicationFactor = ${?VALIDATION_DEFAULT_TOPIC_MIN_REPLICATION_FACTOR} } } ] # The default custom reporters to report applied changes. reporters = [ # Uncomment following lines to enable default kafka reporter # { # name = \"default\" # type = io.streamthoughts.jikkou.kafka.reporter.KafkaChangeReporter # config = { # event.source = \"jikkou/cli\" # kafka = { # topic.creation.enabled = true # topic.creation.defaultReplicationFactor = 1 # topic.name = \"jikkou-resource-change-event\" # client = ${jikkou.kafka.client} { # client.id = \"jikkou-reporter-producer\" # } # } # } # } ] jinja { enableRecursiveMacroCalls = false } } Managing Contexts For detailed usage of all configuration commands, see the Commands Reference:\njikkou config set-context - Create or update a configuration context jikkou config get-contexts - List all configured contexts jikkou config current-context - Show the current context jikkou config use-context - Switch to a different context jikkou config view - Show the merged configuration Tips To debug the configuration used by Jikkou, you can run the following command: jikkou config view --comments or jikkou config view --debug ","categories":"","description":"Contexts, configuration files, and HOCON settings.\n","excerpt":"Contexts, configuration files, and HOCON settings.\n","ref":"/docs/jikkou-cli/cli-configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported by the extension for Aiven.\nConfiguration You can configure the properties to be used to connect the Aiven service through the Jikkou client configuration property jikkou.provider.aiven.\nExample:\njikkou { provider.aiven { enabled = true type = io.streamthoughts.jikkou.extension.aiven.AivenExtensionProvider config = { # Aiven project name project = \"http://localhost:8081\" # Aiven service name service = generic # URL to the Aiven REST API. apiUrl = \"https://api.aiven.io/v1/\" # Aiven Bearer Token. Tokens can be obtained from your Aiven profile page tokenAuth = null # Enable debug logging debugLoggingEnabled = false } } } ","categories":"","description":"Learn how to configure the extensions for Aiven.\n","excerpt":"Learn how to configure the extensions for Aiven.\n","ref":"/docs/providers/aiven/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported by the extension for AWS.\nConfiguration You can configure the properties to be used to connect the AWS services through the Jikkou client configuration property jikkou.providers.\nExample:\njikkou { # AWS provider.aws { enabled = true config = { # The AWS S3 Region, e.g. us-east-1 aws.client.region = \"\" # The AWS Access Key ID. aws.client.accessKeyId = \"\" # The AWS Secret Access Key. aws.client.secretAccessKey = \"\" # The AWS session token. aws.client.sessionToken = \"\" # The endpoint with which the SDK should communicate allowing you to use a different S3 compatible service aws.client.endpointOverride = \"\" # The name of the registries. Used only for lookup. aws.glue.registryNames = \"\" } } } ","categories":"","description":"Learn how to configure the extensions for AWS.\n","excerpt":"Learn how to configure the extensions for AWS.\n","ref":"/docs/providers/aws/configuration/","tags":"","title":"Configuration"},{"body":" This section describes how to configure the Kafka Connect extension.\nConfiguration You can configure the properties to be used to connect the Kafka Connect cluster through the Jikkou client configuration property: jikkou.provider.kafkaconnect.\nExample:\njikkou { provider.kafkaconnect { enabled: true type: io.streamthoughts.jikkou.kafka.connect.KafkaConnectExtensionProvider # Array of Kafka Connect clusters configurations. clusters = [ { # Name of the cluster (e.g., dev, staging, production, etc.) name = \"locahost\" # URL of the Kafka Connect service url = \"http://localhost:8083\" # Method to use for authenticating on Kafka Connect. Available values are: [none, basicauth, ssl] authMethod = none # Use when 'authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false # Ssl Config: Use when 'authMethod' is 'ssl' # The location of the key store file. sslKeyStoreLocation = \"/certs/registry.keystore.jks\" # The file format of the key store file. sslKeyStoreType = \"JKS\" # The password for the key store file. sslKeyStorePassword = \"password\" # The password of the private key in the key store file. sslKeyPassword = \"password\" # The location of the trust store file. sslTrustStoreLocation = \"/certs/registry.truststore.jks\" # The file format of the trust store file. sslTrustStoreType = \"JKS\" # The password for the trust store file. sslTrustStorePassword = \"password\" # Specifies whether to ignore the hostname verification. sslIgnoreHostnameVerification = true } ] } } ","categories":"","description":"Learn how to configure the extensions for Kafka Connect.\n","excerpt":"Learn how to configure the extensions for Kafka Connect.\n","ref":"/docs/providers/kafka-connect/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nConfiguration The Apache Kafka extension is built on top of the Kafka Admin Client. You can configure the properties to be passed to kafka client through the Jikkou client configuration property jikkou.provider.kafka.\nExample:\njikkou { provider.kafka { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { client { bootstrap.servers = \"localhost:9092\" security.protocol = \"SSL\" ssl.keystore.location = \"/tmp/client.keystore.p12\" ssl.keystore.password = \"password\" ssl.keystore.type = \"PKCS12\" ssl.truststore.location = \"/tmp/client.truststore.jks\" ssl.truststore.password = \"password\" ssl.key.password = \"password\" } } } } In addition, the extension support configuration settings to wait for at least a minimal number of brokers before processing.\njikkou { provider.kafka { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } } } ","categories":"","description":"Learn how to configure the extensions for Apache Kafka.\n","excerpt":"Learn how to configure the extensions for Apache Kafka.\n","ref":"/docs/providers/kafka/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for SchemaRegistry.\nConfiguration You can configure the properties to be used to connect the SchemaRegistry service through the Jikkou client configuration property jikkou.provider.schemaregistry.\nExample:\njikkou { provider.schemaregistry { enabled = true type = io.streamthoughts.jikkou.schema.registry.SchemaRegistryExtensionProvider config = { # Comma-separated list of URLs for schema registry instances that can be used to register or look up schemas url = \"http://localhost:8081\" # The name of the schema registry implementation vendor - can be any value vendor = generic # Method to use for authenticating on Schema Registry. Available values are: [none, basicauth, ssl] authMethod = none # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false # Ssl Config: Use when 'authMethod' is 'ssl' # The location of the key store file. sslKeyStoreLocation = \"/certs/registry.keystore.jks\" # The file format of the key store file. sslKeyStoreType = \"JKS\" # The password for the key store file. sslKeyStorePassword = \"password\" # The password of the private key in the key store file. sslKeyPassword = \"password\" # The location of the trust store file. sslTrustStoreLocation = \"/certs/registry.truststore.jks\" # The file format of the trust store file. sslTrustStoreType = \"JKS\" # The password for the trust store file. sslTrustStorePassword = \"password\" # Specifies whether to ignore the hostname verification. sslIgnoreHostnameVerification = true } } } ","categories":"","description":"Learn how to configure the extensions for SchemaRegistry.\n","excerpt":"Learn how to configure the extensions for SchemaRegistry.\n","ref":"/docs/providers/schema-registry/configuration/","tags":"","title":"Configuration"},{"body":"Jikkou API Server is built with Micronaut Framework.\nThe default configuration file is located in the installation directory of you server under the path /etc/application.yaml.\nYou can either modify this configuration file directly or create a new one. Then, your configuration file path can be targeted through the MICRONAUT_CONFIG_FILES environment variable.\nA YAML Configuration file example can be found here: application.yaml\nNote For more information about how to configure the application, we recommend you to read the official Micronaut documentation (see: Application Configuration). ","categories":"","description":"Learn how to configure Jikkou API Server.\n","excerpt":"Learn how to configure Jikkou API Server.\n","ref":"/docs/jikkou-api-server/configuration/","tags":"","title":"Configurations"},{"body":" This section covers the core classes to develop validation extensions.\nInterface To create a custom validation, you will need to implement the Java interface: io.streamthoughts.jikkou.core.validation.Validation.\nThis interface defines two methods, with a default implementation for each, to give you the option of validating either all resources accepted by validation at once, or each resource one by one.\npublic interface Validation\u003cT extends HasMetadata\u003e extends Interceptor { /** * Validates the specified resource list. * * @param resources The list of resources to be validated. * @return The ValidationResult. */ default ValidationResult validate(@NotNull final List\u003cT\u003e resources) { // code omitted for clarity } /** * Validates the specified resource. * * @param resource The resource to be validated. * @return The ValidationResult. */ default ValidationResult validate(@NotNull final T resource) { // code omitted for clarity } } Examples The validation class below shows how to validate that any resource has a specific non-empty label.\n@Title(\"HasNonEmptyLabelValidation allows validating that resources have a non empty label.\") @Description(\"This validation can be used to ensure that all resources are associated to a specific label. The labe key is passed through the configuration of the extension.\") @Example( title = \"Validate that resources have a non-empty label with key 'owner'.\", full = true, code = {\"\"\" validations: - name: \"resourceMustHaveNonEmptyLabelOwner\" type: \"com.example.jikkou.validation.HasNonEmptyLabelValidation\" priority: 100 config: key: owner \"\"\" } ) @SupportedResources(value = {}) // an empty list implies that the extension supports any resource-type public final class HasNonEmptyLabelValidation implements Validation { // The required config property. static final ConfigProperty\u003cString\u003e LABEL_KEY_CONFIG = ConfigProperty.ofString(\"key\"); private String key; /** * Empty constructor - required. */ public HasNonEmptyLabelValidation() { } /** * {@inheritDoc} */ @Override public void configure(@NotNull final Configuration config) { // Get the key from the configuration. this.key = LABEL_KEY_CONFIG .getOptional(config) .orElseThrow(() -\u003e new ConfigException( String.format(\"The '%s' configuration property is required for %s\", LABEL_KEY_CONFIG.key(), TopicNamePrefixValidation.class.getSimpleName() ) )); } /** * {@inheritDoc} */ @Override public ValidationResult validate(final @NotNull HasMetadata resource) { Optional\u003cString\u003e label = resource.getMetadata() .findLabelByKey(this.key) .map(NamedValue::getValue) .map(Value::asString) .filter(String::isEmpty); // Failure if (label.isEmpty()) { String error = String.format( \"Resource for name '%s' have no defined or empty label for key: '%s'\", resource.getMetadata().getName(), this.key ); return ValidationResult.failure(new ValidationError(getName(), resource, error)); } // Success return ValidationResult.success(); } } ","categories":"","description":"Learn how to develop custom resource validations.\n","excerpt":"Learn how to develop custom resource validations.\n","ref":"/docs/developer-guide/extensions/validation/","tags":"","title":"Develop Custom Validations"},{"body":" This guide describes how developers can write new extensions for Jikkou.\nMore information:\n","categories":"","description":"Learn how to write custom extensions for Jikkou\n","excerpt":"Learn how to write custom extensions for Jikkou\n","ref":"/docs/developer-guide/extensions/","tags":"","title":"Extension Developer Guide"},{"body":" Jikkou can be installed either from source, or from releases.\nFrom SDKMan! (recommended) The latest stable release of jikkou (x86) for Linux, and macOS can be retrieved via SDKMan!:\nsdk install jikkou From The Jikkou Project Releases Every release released versions of Jikkou is available:\nAs a zip/tar.gz package from GitHub Releases (for Linux, MacOS) As a fatJar available from Maven Central As a docker image available from Docker Hub. These are the official ways to get Jikkou releases that you manually downloaded and installed.\nInstall From Release distribution Download your desired version Unpack it (unzip jikkou-\u003cversion\u003e-linux-x86_64.zip) Move the unpacked directory to the desired destination (mv jikkou-\u003cversion\u003e-linux-x86_64 /opt/jikkou) Add the executable to your PATH (export PATH=$PATH:/opt/jikkou/bin) From there, you should be able to run the client: jikkou help.\nIt is recommended to install the bash/zsh completion script jikkou_completion:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/main/jikkou_completion -O jikkou_completion or alternatively, run the following command for generation the completion script.\n$ source \u003c(jikkou generate-completion) Using Docker Image # Create a Jikkou configfile (i.e., jikkouconfig) cat \u003c\u003c EOF \u003ejikkouconfig { \"currentContext\" : \"localhost\", \"localhost\" : { \"configFile\" : null, \"configProps\" : { \"provider.kafka.config.client.bootstrap.servers\" : \"localhost:9092\" } } } EOF # Run Docker docker run -it \\ --net host \\ --mount type=bind,source=\"$(pwd)\"/jikkouconfig,target=/etc/jikkou/config \\ streamthoughts/jikkou:latest -V Development Builds In addition to releases you can download or install development snapshots of Jikkou.\nFrom Docker Hub Docker images are built and push to Docker Hub from the latest main branch. They are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features.\n$ docker run -it streamthoughts/jikkou:main From Source (Linux, macOS) Building Jikkou from source is slightly more work, but is the best way to go if you want to test the latest ( pre-release) Jikkou version.\nPrerequisites To build the project you will need:\nJava 25 (i.e. $JAVA_HOME environment variable is configured). GraalVM 25.0.2 or newer to create native executable TestContainer to run integration tests Create Native Executable # Build and run all tests ./mvnw clean verify -Pnative You can then execute the native executable with: ./jikkou-cli/target/jikkou-$PROJECT_VERSION-runner\nBuild Debian Package (.deb) # Build and run all tests ./mvnw clean package -Pnative ./mvnw package -Pdeb You can then install the package with: sudo dpkg -i ./dist/jikkou-$PROJECT_VERSION-linux-x86_64.deb\nNOTE: Jikkou will install itself in the directory : /opt/jikkou\nBuild RPM Package # Build and run all tests ./mvnw clean package -Pnative ./mvnw package -Prpm The RPM package will available in the ./target/rpm/jikkou/RPMS/noarch/ directory.\n","categories":"","description":"This guide shows how to install the Jikkou CLI.\n","excerpt":"This guide shows how to install the Jikkou CLI.\n","ref":"/docs/install/","tags":"","title":"Install Jikkou"},{"body":"Labels You can use labels to attach arbitrary identifying metadata to objects.\nLabels are key/value maps:\nmetadata: labels: \"key1\": \"value-1\" \"key2\": \"value-2\" Note The keys in the map must be string, but values can be any scalar types (string, boolean, or numeric). Labels are not persistent Jikkou is completely stateless. In other words, it will not store any state about the describe resources objects. Thus, when retrieving objects from your system labels may not be reattached to the metadata objects. Example metadata: labels: environment: \"stating\" Annotations You can use annotations to attach arbitrary non-identifying metadata to objects.\nAnnotations are key/value maps:\nmetadata: annotations: key1: \"value-1\" key2: \"value-2\" Note The keys in the map must be string, but the values can be of any scalar types (string, boolean, or numeric). Built-in Annotations jikkou.io/ignore Used on: All Objects.\nThis annotation indicates whether the object should be ignored for reconciliation.\njikkou.io/bypass-validations Used on: All Objects.\nThis annotation indicates whether the object should bypass the validation chain. In other words, no validations will be applied on the object.\njikkou.io/delete Used on: All Objects.\nThis annotation indicates (when set to true) that the object should be deleted from your system.\njikkou.io/resource-location Used by jikkou.\nThis annotation is automatically added by Jikkou to an object when loaded from your local filesystem.\njikkou.io/items-count Used by jikkou.\nThis annotation is automatically added by Jikkou to an object collection grouping several resources of homogeneous type.\n","categories":"","description":"","excerpt":"Labels You can use labels to attach arbitrary identifying metadata to ‚Ä¶","ref":"/docs/concepts/labels-and-annotations/","tags":"","title":"Labels and annotations"},{"body":" Here, you will find the list of resources supported by the extensions for Aiven.\nAiven for Apache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/docs/providers/aiven/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported by the extensions for AWS.\nAWS Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for AWS.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/docs/providers/aws/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported by the Kafka Connect Extension.\nKafka Connect Resources More information:\n","categories":"","description":"Learn how to use the resources provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the resources provided by the Kafka Connect ‚Ä¶","ref":"/docs/providers/kafka-connect/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nApache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the built-in resources provided by the Extension ‚Ä¶","ref":"/docs/providers/kafka/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Schema Registry.\nSchema Registry Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/docs/providers/schema-registry/resources/","tags":"","title":"Resources"},{"body":" Here, you will find information to use the built-in transformations for Apache Kafka resources.\nMore information:\n","categories":"","description":"Learn how to use the built-in transformation provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the built-in transformation provided by the Extension ‚Ä¶","ref":"/docs/providers/kafka/transformations/","tags":"","title":"Transformations"},{"body":" Try the tutorials for common Jikkou tasks and use cases.\n","categories":"","description":"Learn common Jikkou tasks and use cases.\n","excerpt":"Learn common Jikkou tasks and use cases.\n","ref":"/docs/tutorials/","tags":"","title":"Jikkou Tutorials"},{"body":"Enable Security To enable secure access to the API Server:\nConfiguration File Update the configuration file (i.e., application.yaml) of the server with:\nmicronaut: security: enabled: true Environment Variable As an alternative, you can set the following environment variable MICRONAUT_SECUTIRY_ENABLED=true.\nNote For more information about how Micronaut binds environment variables and configuration property: https://docs.micronaut.io/latest/guide/index.html#_property_value_binding). Unauthorized Access When accessing a secured path, the server will return the following response if access is not authorized:\n{ \"message\": \"Unauthorized\", \"errors\": [ { \"status\": 401, \"error_code\": \"authentication_user_unauthorized\", \"message\": \"Unauthorized\" } ] } ","categories":"","description":"Learn how to secure access to Jikkou API server.\n","excerpt":"Learn how to secure access to Jikkou API server.\n","ref":"/docs/jikkou-api-server/configuration/authentication/","tags":"","title":"Authentication"},{"body":"Jikkou API Server can be secured using a Basic HTTP Authentication Scheme.\nRFC7617 defines the ‚ÄúBasic‚Äù Hypertext Transfer Protocol (HTTP) authentication scheme, which transmits credentials as user-id/password pairs, encoded using Base64.\nBasic Authentication should be used over a secured connection using HTTPS.\nConfigure Basic HTTP Authentication Step1: Enable security Add the following configuration to your server configuration.\n# ./etc/application.yaml micronaut: security: enabled: true Step2: Configure the list of users The list of username/password authorized to connect to the API server can be configured as follows:\n# ./etc/application.yaml jikkou: security: basic-auth: - username: \"admin\" password: \"{noop}password\" For production environment, password must not be configured in plaintext. Password can be passed encoded in bcrypt, scrypt, argon2, and sha256.\nExample echo -n password | sha256sum 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8 # ./etc/application.yaml jikkou: security: basic-auth: - username: \"admin\" password: \"{sha256}5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\" Step3: Validate authentication Encode credentials\necho -n \"admin:password\" | base64 YWRtaW46cGFzc3dvcmQ= Send request\ncurl -IX GET http://localhost:28082/apis/kafka.jikkou.io/v1beta2/kafkabrokers \\ -H \"Accept: application/json\" \\ -H \"Authorization: Basic YWRtaW46cGFzc3dvcmQ\" HTTP/1.1 200 OK Content-Type: application/hal+json content-length: 576 ","categories":"","description":"Learn how to secure Jikkou API Server using Basic HTTP Authentication Scheme.\n","excerpt":"Learn how to secure Jikkou API Server using Basic HTTP Authentication ‚Ä¶","ref":"/docs/jikkou-api-server/configuration/authentication/basic_auth/","tags":"","title":"Basic Auth"},{"body":" This section contains the complete reference for every Jikkou CLI command, including synopsis, options, usage examples, and related commands.\nYou can also run jikkou \u003ccommand\u003e --help from the terminal to view built-in help for any command.\nGlobal Options The following options are available on all commands:\nFlag Description --logger-level=\u003clevel\u003e Specify the log level verbosity. Valid values: TRACE, DEBUG, INFO, WARN, ERROR -h, --help Show help message and exit -V, --version Print version information and exit Core Commands Commands for reconciling and inspecting resources against your target platform.\nReconciliation These commands apply changes to your target platform based on resource definition files. Each command corresponds to a specific reconciliation mode.\nCommand Mode Description jikkou apply FULL Create, update, and delete resources to match the desired state jikkou create CREATE Create only non-existing resources jikkou update UPDATE Create new and update existing resources (no deletions) jikkou delete DELETE Delete resources no longer described in definition files jikkou patch (explicit) Reconcile with an explicitly specified mode jikkou replace Delete and recreate all resources from scratch Inspect \u0026 Validate Command Description jikkou diff Preview resource changes without applying them jikkou get Display one or many resources from the target platform jikkou validate Check resource definitions against all validation requirements jikkou prepare Render templates and apply transformations without validating System Management Commands Command Description jikkou action List and execute provider actions jikkou health Print or describe health indicators for target environments jikkou server-info Display Jikkou API server information (proxy mode only) Configuration \u0026 Discovery Commands Command Description jikkou config Manage CLI configuration contexts jikkou api-resources List the supported API resource types and their verbs jikkou api-extensions List and inspect registered API extensions ","categories":"","description":"Comprehensive reference for all Jikkou CLI commands.\n","excerpt":"Comprehensive reference for all Jikkou CLI commands.\n","ref":"/docs/jikkou-cli/commands/","tags":"","title":"Commands"},{"body":" This section covers the core classes to develop action extensions.\nInterface To create a custom action, you will need to implement the Java interface: io.streamthoughts.jikkou.core.action.Action.\n/** * Interface for executing a one-shot action on a specific type of resources. * * @param \u003cT\u003e The type of the resource. */ @Category(ExtensionCategory.ACTION) public interface Action\u003cT extends HasMetadata\u003e extends HasMetadataAcceptable, Extension { /** * Executes the action. * * @param configuration The configuration * @return The ExecutionResultSet */ @NotNull ExecutionResultSet\u003cT\u003e execute(@NotNull Configuration configuration); } Examples The Action class below shows how to implement a custom action accepting options`.\n@Named(EchoAction.NAME) @Title(\"Print the input.\") @Description(\"The EchoAction allows printing the text provided in input.\") @ExtensionSpec( options = { @ExtensionOptionSpec( name = INPUT_CONFIG_NAME, description = \"The input text to print.\", type = String.class, required = true ) } ) public final class EchoAction extends ContextualExtension implements Action\u003cHasMetadata\u003e { public static final String NAME = \"EchoAction\"; public static final String INPUT_CONFIG_NAME = \"input\"; @Override public @NotNull ExecutionResultSet\u003cHasMetadata\u003e execute(@NotNull Configuration configuration) { String input = extensionContext().\u003cString\u003econfigProperty(INPUT_CONFIG_NAME).get(configuration); return ExecutionResultSet .newBuilder() .result(ExecutionResult .newBuilder() .status(ExecutionStatus.SUCCEEDED) .data(new EchoOut(input)) .build()) .build(); } @Kind(\"EchoOutput\") @ApiVersion(\"core.jikkou.io/v1\") @Reflectable record EchoOut(@JsonProperty(\"out\") String out) implements HasMetadata { @Override public ObjectMeta getMetadata() { return new ObjectMeta(); } @Override public HasMetadata withMetadata(ObjectMeta objectMeta) { throw new UnsupportedOperationException(); } } } ","categories":"","description":"Learn how to develop custom actions.\n","excerpt":"Learn how to develop custom actions.\n","ref":"/docs/developer-guide/extensions/action/","tags":"","title":"Develop Custom Action"},{"body":" This section covers the core classes to develop transformation extensions.\nInterface To create a custom transformation, you will need to implement the Java interface: io.streamthoughts.jikkou.core.transformation.Transformation.\n/** * This interface is used to transform or filter resources. * * @param \u003cT\u003e The resource type supported by the transformation. */ public interface Transformation\u003cT extends HasMetadata\u003e extends Interceptor { /** * Executes the transformation on the specified {@link HasMetadata} object. * * @param resource The {@link HasMetadata} to be transformed. * @param resources The {@link ResourceListObject} involved in the current operation. * @param context The {@link ReconciliationContext}. * @return The list of resources resulting from the transformation. */ @NotNull Optional\u003cT\u003e transform(@NotNull T resource, @NotNull HasItems resources, @NotNull ReconciliationContext context); } Examples The transformation class below shows how to filter resource having an annotation exclude: true.\nimport java.util.Optional; @Named(\"ExcludeIgnoreResource\") @Title(\"ExcludeIgnoreResource allows filtering resources whose 'metadata.annotations.ignore' property is equal to 'true'\") @Description(\"The ExcludeIgnoreResource transformation is used to exclude from the\" + \" reconciliation process any resource whose 'metadata.annotations.ignore'\" + \" property is equal to 'true'. This transformation is automatically enabled.\" ) @Enabled @Priority(HasPriority.HIGHEST_PRECEDENCE) public final class ExcludeIgnoreResourceTransformation implements Transformation\u003cHasMetadata\u003e { /** {@inheritDoc}**/ @Override public @NotNull Optional\u003cHasMetadata\u003e transform(@NotNull HasMetadata resource, @NotNull HasItems resources, @NotNull ReconciliationContext context) { return Optional.of(resource) .filter(r -\u003e HasMetadata.getMetadataAnnotation(resource, \"ignore\") .map(NamedValue::getValue) .map(Value::asBoolean) .orElse(false) ); } } ","categories":"","description":"Learn how to develop custom resource transformations.\n","excerpt":"Learn how to develop custom resource transformations.\n","ref":"/docs/developer-guide/extensions/transformation/","tags":"","title":"Develop Custom Transformations"},{"body":"Setup Jikkou The streamthoughts/setup-jikkou action is a JavaScript action that sets up Jikkou in your GitHub Actions workflow by:\nDownloading a specific version of Jikkou CLI and adding it to the PATH. Configuring JIKKOU CLI with a custom configuration file. After you‚Äôve used the action, subsequent steps in the same job can run arbitrary Jikkou commands using the GitHub Actions run syntax. This allows most Jikkou commands to work exactly like they do on your local command line.\nUsage steps: - uses: streamthoughts/setup-jikkou@v1 A specific version of Jikkou CLI can be installed:\nsteps: - uses: streamthoughts/setup-jikkou@v0.1.0 with: jikkou_version: 0.37.0 A custom configuration file can be specified:\nsteps: - uses: streamthoughts/setup-jikkou@v0.1.0 with: jikkou_config: ./config/jikkouconfig.json Inputs This Action additionally supports the following inputs :\nProperty Default Description jikkou_version latest The version of Jikkou CLI to install. A value of latest will install the latest version of Jikkou CLI. jikkou_config The path to the Jikkou CLI config file. If set, Jikkou CLI will be configured through the JIKKOUCONFIG environment variable. ","categories":"","description":"Learn Jikkou Setup Github Action in your CI/CD Workflows\n","excerpt":"Learn Jikkou Setup Github Action in your CI/CD Workflows\n","ref":"/docs/jikkou-cli/automating/githubactions/","tags":"","title":"Automate Jikkou with GitHub Actions"},{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\nThe Jikkou CLI (jikkou) is the primary interface for managing infrastructure resources. This section covers everything you need to use the CLI effectively:\nOverview ‚Äî Global options, version check, and shell tab-completion Configuration ‚Äî Contexts, configuration files, and HOCON settings Commands ‚Äî Complete reference for every command Automating ‚Äî CI/CD integration with GitHub Actions ","categories":"","description":"Complete guide to the Jikkou command-line interface.\n","excerpt":"Complete guide to the Jikkou command-line interface.\n","ref":"/docs/jikkou-cli/","tags":"","title":"Jikkou CLI"},{"body":"Jikkou API Server can be secured using JWT (JSON Web Token) Authentication.\nConfigure JWT Step1: Set JWT signature secret Add the following configuration to your server configuration.\n# ./etc/application.yaml micronaut: security: enabled: true authentication: bearer \u003c1\u003e token: enabled: true jwt: signatures: secret: generator: secret: ${JWT_GENERATOR_SIGNATURE_SECRET:pleaseChangeThisSecretForANewOne} \u003c2\u003e \u003c1\u003e Set authentication to bearer to receive a JSON response from the login endpoint. \u003c2\u003e Change this to your own secret and keep it safe (do not store this in your VCS). Step2: Generate a Token Generate a valid JSON Web Token on https://jwt.io/ using your secret.\nExample with pleaseChangeThisSecretForANewOne as signature secret.\nTOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.6cD3MnZmX2xyEAWyh-GgGD11TX8SmvmHVLknuAIJ8yE Step3: Validate authentication $ curl -I -X GET http://localhost:28082/apis/kafka.jikkou.io/v1beta2/kafkabrokers \\ -H \"Accept: application/json\" \\ -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.6cD3MnZmX2xyEAWyh-GgGD11TX8SmvmHVLknuAIJ8yE\" HTTP/1.1 200 OK Content-Type: application/hal+json content-length: 576 ","categories":"","description":"Learn how to secure Jikkou API Server using JWT (JSON Web Token) Authentication.\n","excerpt":"Learn how to secure Jikkou API Server using JWT (JSON Web Token) ‚Ä¶","ref":"/docs/jikkou-api-server/configuration/authentication/jwt/","tags":"","title":"JWT"},{"body":" In the context of Jikkou, reconciliation refers to the process of comparing the desired state of an object with the actual state of the system and making any necessary corrections or adjustments to align them.\nChanges A Change represents a difference, detected during reconciliation, between two objects that can reconciled or corrected by adding, updating, or deleting an object or property attached to the actual state of the system.\nA Change represents a detected difference between two objects during the reconciliation process. These differences can be reconciled or corrected by adding, updating, or deleting an object or property associated with the actual state of the system\nJikkou identifies four types of changes:\nADD: Indicates the addition of a new object or property to an existing object.\nUPDATE: Indicates modifications made to an existing object or property of an existing object.\nDELETE: Indicates the removal of an existing object or property of an existing object.\nNONE: Indicates that no changes were made to an existing object or property.\nReconciliation Modes Depending on the chosen reconciliation mode, only specific types of changes will be applied.\nJikkou provides four distinct reconciliation modes that determine the types of changes to be applied:\nCREATE: This mode only applies changes that create new resource objects in your system. DELETE: This mode only applies changes that delete existing resource objects in your system. UPDATE: This mode only applies changes that create or update existing resource objects in your system. APPLY_ALL: This mode applies all changes to ensure that the actual state of a resource in the cluster matches the desired state defined in your resource definition file, regardless of the specific type of change. Each mode corresponds to a command offered by the Jikkou CLI (i.e., create, update, delete, and apply). Choose the appropriate mode based on your requirements.\nUsing JIKKOU CLI Some reconciliation modes might not be supported for all resources. Use jikkou extensions list --type Controller to check which actions could be perfomed for each resources. Reconciliation Options Depending on the type of resources being reconciled, the controller that will be involved in the reconciliation process might accept some options (i.e., using --options argument).\nMark Resource for Deletion To delete all the states associated with resource‚Äôs entities, you must add the following annotation to the resource definition:\nmetadata: annotations: jikkou.io/delete: true ","categories":"","description":"","excerpt":" In the context of Jikkou, reconciliation refers to the process of ‚Ä¶","ref":"/docs/concepts/reconciliation/","tags":["concept"],"title":"Reconciliation"},{"body":"","categories":"","description":"Integrate Jikkou into CI/CD pipelines.\n","excerpt":"Integrate Jikkou into CI/CD pipelines.\n","ref":"/docs/jikkou-cli/automating/","tags":"","title":"Automating"},{"body":"Configuration Step 1: Enable Proxy Mode To enable proxy mode so that the CLI communicates directly with your API Server, add the following parameters to your configuration:\njikkou { # Proxy Configuration proxy { # Specify whether proxy mode is enabled (default: false). enabled = true # URL of the API Server url = \"http://localhost:28082\" # Specifcy whether HTTP request debugging should be enabled (default: false) debugging = false # The connect timeout in millisecond (if not configured used ` default-timeout` ). connect-timeout = 10000 # The read timeout in millisecond (if not configured used ` default-timeout` ). read-timeout = 10000 # The write timeout in millisecond (if not configured used ` default-timeout` ). write-timeout = 10000 # The default timeout (i.e., for read/connect) in millisecond (default: 10000) default-timeout = 10000 # Security settings to authenticate to the API Server. security = { # For Token based Authentication. # access-token = \"\" # For Username/Password Basic-Authentication. # basic-auth = { # username = \"\" # password = \"\" # } } } } Step 2: Check connection When enabling Proxy Mode, Jikkou CLI provides the additional command server-info. You can use it to verify the connectivity with teh server.\n$ jikkou server-info -o JSON | jq { \"version\": \"0.31.0\", \"build_time\": \"2023-11-15T10:35:22+0100\", \"commit_id\": \"f3384d38e606fb32599c175895d0cbef28258540\" } ","categories":"","description":"Learn how to configure Jikkou CLI in proxy mode.\n","excerpt":"Learn how to configure Jikkou CLI in proxy mode.\n","ref":"/docs/jikkou-api-server/configuration/cli_proxy_mode/","tags":"","title":"CLI Proxy Mode"},{"body":"Jikkou API Server provides a REST interface to any platform supported by Jikkou, making it even easier to manage, automate and visualise all your data platform assets.\nJikkou CLI can be used in combination with Jikkou API Server by configuring it in proxy mode. In this mode, the CLI no longer connects directly to your various platforms, but forwards all operations to the API server. This deployment method allows you to enhance the overall security of the platforms managed through Jikkou.\n","categories":"","description":"Learn Jikkou's API Server usages.\n","excerpt":"Learn Jikkou's API Server usages.\n","ref":"/docs/jikkou-api-server/","tags":"","title":"Jikkou API Server Documentation"},{"body":" Selectors allow you to include or exclude resource objects when returned or reconciled by Jikkou.\nOverview Selectors filter which resources Jikkou operates on. They are passed via the --selector (or -s) CLI option and can be used with any command that accepts resources, including get, apply, create, update, delete, diff, validate, and prepare.\nYou can specify multiple selectors by repeating the --selector flag, and control how they are combined using the --selector-match option.\nSelector Expression Syntax A selector expression follows one of two formats:\nWith an explicit selector type prefix: \u003cSELECTOR_TYPE\u003e: \u003cEXPRESSION\u003e Using the default field selector (the prefix can be omitted): \u003cKEY\u003e \u003cOPERATOR\u003e \u003cVALUE\u003e \u003cKEY\u003e \u003cOPERATOR\u003e (VALUE1, VALUE2, ...) Supported Selectors Field (default) The field selector filters resources based on any field in the resource object, using dot-notation to navigate nested properties.\nfield: \u003cKEY\u003e \u003cOPERATOR\u003e \u003cVALUE\u003e field: \u003cKEY\u003e \u003cOPERATOR\u003e (VALUE1, VALUE2, ...) Since field is the default selector type, the field: prefix can be omitted.\nExamples # Select resources by name jikkou get kafkatopics -s 'metadata.name IN (my-topic, other-topic)' # Select by kind jikkou get kafkatopics -s 'kind IN (KafkaTopic)' # Select by label (using field selector) jikkou get kafkatopics -s 'metadata.labels.environment IN (staging, production)' # Select topics matching a regex pattern jikkou get kafkatopics -s 'metadata.name MATCHES (^public-.*)' # Exclude internal topics jikkou get kafkatopics -s 'metadata.name DOESNOTMATCH (^__.*)' # With explicit field: prefix (equivalent to above) jikkou get kafkatopics -s 'field: metadata.name MATCHES (^public-.*)' Label The label selector provides a shorthand to filter resources by their metadata labels, without needing the full metadata.labels. path prefix.\nlabel: \u003cLABEL_KEY\u003e \u003cOPERATOR\u003e \u003cVALUE\u003e label: \u003cLABEL_KEY\u003e \u003cOPERATOR\u003e (VALUE1, VALUE2, ...) Examples # Select resources with a specific label value jikkou get kafkatopics -s 'label: environment IN (staging, production)' # Select resources that have a specific label (regardless of value) jikkou get kafkatopics -s 'label: team EXISTS' # Select resources that do NOT have a specific label jikkou get kafkatopics -s 'label: deprecated DOESNOTEXIST' Expression (since Jikkou v0.36) The expr selector enables complex filtering using the Common Expression Language (CEL). This is the most powerful selector type, supporting conditions on nested fields, arrays, maps, and computed expressions.\nThe resource being evaluated is available as the resource variable.\nexpr: \u003cCEL_EXPRESSION\u003e Examples # Select resources with a label value in a list jikkou get kafkatopics \\ -s 'expr: has(resource.metadata.labels.env) \u0026\u0026 resource.metadata.labels.env in [\"staging\", \"production\"]' # Select Kafka topics with at least 12 partitions jikkou get kafkatopics \\ -s 'expr: resource.kind == \"KafkaTopic\" \u0026\u0026 resource.spec.partitions \u003e= 12' # Select resources missing a specific annotation jikkou get kafkatopics \\ -s 'expr: !has(resource.metadata.annotations[\"mycompany.io/owner\"])' Tip Use the expr selector when you need advanced filtering logic that goes beyond simple key-operator-value matching, such as numeric comparisons, boolean combinations, or presence checks on nested maps. Expression Operators The following operators are available for field and label selectors:\nOperator Description IN Match if the value is in the given list NOTIN Match if the value is not in the given list EXISTS Match if the field or label exists DOESNOTEXIST Match if the field or label does not exist MATCHES Match if the value matches a regular expression DOESNOTMATCH Match if the value does not match the regular expression Note These operators are not available for the expr selector, which uses CEL syntax instead. Matching Strategies When specifying multiple selectors, use --selector-match to control how they are combined:\nStrategy Behavior ALL The resource must match all selectors (logical AND). ANY The resource must match at least one selector (logical OR). NONE The resource must match none of the selectors (logical NOT). The default strategy is ALL.\nExamples # Match resources whose name starts with \"__\" OR is exactly \"_schemas\" (ANY) jikkou get kafkatopics \\ --selector 'metadata.name MATCHES (^__.*)' \\ --selector 'metadata.name IN (_schemas)' \\ --selector-match ANY # Match resources that have both labels (ALL - default) jikkou get kafkatopics \\ -s 'label: environment IN (production)' \\ -s 'label: team EXISTS' # Exclude resources matching any selector (NONE) jikkou get kafkatopics \\ -s 'metadata.name MATCHES (^__.*)' \\ -s 'metadata.name IN (_schemas)' \\ --selector-match NONE SEE ALSO jikkou get - Display resources with selectors jikkou apply - Apply resources with selectors jikkou diff - Diff resources with selectors Labels and annotations - Using labels for resource organization ","categories":"","description":"","excerpt":" Selectors allow you to include or exclude resource objects when ‚Ä¶","ref":"/docs/concepts/selectors/","tags":["concept","feature"],"title":"Selectors"},{"body":" This section explains key concepts used within Jikkou:\n","categories":"","description":"Learn the differents concepts used within Jikkou\n","excerpt":"Learn the differents concepts used within Jikkou\n","ref":"/docs/concepts/","tags":"","title":"Concepts"},{"body":" The section helps you learn more about the built-in Extension Providers for Jikkou.\n","categories":"","description":"Learn how to use Jikkou Extension Provider to provision and manage configuration assets on your data infrastructure.\n","excerpt":"Learn how to use Jikkou Extension Provider to provision and manage ‚Ä¶","ref":"/docs/providers/","tags":["how-to","docs"],"title":"Extension Providers"},{"body":" Transformations are applied to inbound resources. Transformations are used to transform, enrich, or filter resource entities before they are validated and thus before the reconciliation process is executed on them.\nAvailable Transformations You can list all the available transformations using the Jikkou CLI command:\njikkou extensions list --type=Transformation [-kinds \u003ca resource kind to filter returned results\u003e] Transformation chain When using Jikkou CLI, you can configure a transformation chain that will be applied to every resource. This chain consists of multiple transformations, each designed to handle different types of resources. Jikkou ensures that a transformation is executed only for the resource types it supports. In cases where a resource is not accepted by a transformation, it is passed to the next transformation in the chain. This process continues until a suitable transformation is found or until all transformations have been attempted.\nConfiguration jikkou { # The list of transformations to execute transformations: [ { # Simple or fully qualified class name of the transformation extension. type = \"\" # Priority to be used for executing this transformation extension. # The lowest value has the highest priority, so it's run first. Minimum value is -2^31 (highest) and a maximum value is 2^31-1 (lowest). # Usually, values under 0 should be reserved for internal transformation extensions. priority = 0 config = { # Configuration properties for this transformation } } ] } Tips The config object of a Transformation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute transformations: [ { # Enforce a minimum number of replicas for a kafka topic type = KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 4 } }, { # Enforce a {@code min.insync.replicas} for a kafka topic. type = KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" Transformations are applied to inbound resources. Transformations are ‚Ä¶","ref":"/docs/concepts/transformations/","tags":["concept","feature","extension"],"title":"Transformations"},{"body":" Here, you will find the necessary information to develop with the Jikkou API.\nMore information:\n","categories":"","description":"Learn how to use the Jikkou Core API\n","excerpt":"Learn how to use the Jikkou Core API\n","ref":"/docs/developer-guide/","tags":"","title":"Developer Guide"},{"body":" Validations are applied to inbound resources to ensure that the resource entities adhere to specific rules or constraints. These validations are carried out after the execution of the transformation chain and before the reconciliation process takes place.\nAvailable Validations You can list all the available validations using the Jikkou CLI command:\njikkou api-extensions list --category=validation [--kinds \u003ca resource kind to filter returned results\u003e] Validation chain When using Jikkou CLI, you can configure a validation chain that will be applied to every resource. This chain consists of multiple validations, each designed to handle different types of resources. Jikkou ensures that a validation is executed only for the resource types it supports. In cases where a resource is not accepted by a validation, it is passed to the next validation in the chain. This process continues until a suitable validation is found or until all validations have been attempted.\nConfiguration jikkou { # The list of validations to execute validations: [ { # Custom name for the validation rule name = \"\" # Simple or fully qualified class name of the validation extension. type = \"\" config = { # Configuration properties for this validation } } ] } Tips The config object of a Validation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute validations: [ { # Custom name for the validation rule name = topicMustBePrefixedWithRegion # Simple or fully qualified class name of the validation extension. type = TopicNameRegexValidation # The config values that will be passed to the validation. config = { topicNameRegex = \"(europe|northamerica|asiapacific)-.+\" } } ] } ","categories":"","description":"","excerpt":" Validations are applied to inbound resources to ensure that the ‚Ä¶","ref":"/docs/concepts/validations/","tags":["concept","feature","extension"],"title":"Validations"},{"body":" Template helps you to dynamically define resource definition files from external data.\nTemplate Engine Jikkou provides a simple templating mechanism based-on Jinjava, a Jinja template engine for Java.\nRead the official documentation of Jinja to learn more about the syntax and semantics of the template engine.\nHow Does It Work ? Jikkou performs the rendering of your template in two phases:\nFirst, an initial rendering is performed using only the values and labels passed through the command-lines arguments. Thus, it is perfectly OK if your resource file is not initially a valid YAML file. Then, a second and final rendering is performed after parsing the YAML resource file using the additional values and labels as defined into the YAML resource file. Therefore, it‚Äôs important that your resource file is converted into a valid YAML file after the first rendering. Important You should use {% raw %}...{% endraw %} tags to ensure the variables defined into the template are not be interpreted during the first rendering. Variables Jikkou defines a number of top-level variables that are passed to the template engine.\nvalues:\nThe values passed into the template through the command-line --values-files and/or --set-value arguments In addition, values can be defined into the application.conf file and directly into the template file using the property template.values. By default, values is empty. labels:\nThe labels passed into the template through the command-line argument: --set-label. In addition, labels can be defined into the template file using the property metadata.labels. By default, labels is empty. system.env:\nThis provides access to all environment variables. system.props:\nThis provides access to all system properties. Template Values When using templating, a resource definition file may contain the additional property template. fields:\napiVersion: The api version (required) kind: The resource kind (required) metadata: labels: The set of key/value pairs that you can use to describe your resource file (optional) annotations: The set of key/value pairs automatically generated by the tool (optional) template: values: The set of key/value pairs to be passed to the template engine (optional) spec: Specification of the resource Values Data File Values Data File are used to define all the necessary values (i.e., the variables) to be used for generating a template.\nExample # file: ./values.yaml topicConfigs: partitions: 4 replicas: 3 topicPrefix: \"{{ system.env.TOPIC_PREFIX | default('test', true) }}\" countryCodes: - fr - be - de - es - uk - us Template Resource File Example # file: ./kafka-topics.tpl apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaTopicList' items: { % for country in values.countryCodes % } - metadata: name: \"{{ values.topicPrefix}}-iot-events-{{ country }}\" spec: partitions: { { values.topicConfigs.partitions } } replicas: { { values.topicConfigs.replicas } } configMapRefs: - TopicConfig { % endfor % } --- apiVersion: \"core.jikkou.io/v1beta2\" kind: \"ConfigMap\" metadata: name: TopicConfig template: values: default_min_insync_replicas: \"{{ values.topicConfigs.replicas | default(3, true) | int | add(-1) }}\" data: retention.ms: 3600000 max.message.bytes: 20971520 min.insync.replicas: '{% raw %}{{ values.default_min_insync_replicas }}{% endraw %}' Command\n$ TOPIC_PREFIX=local jikkou validate --files topics.tpl --values-files values.yaml (Output)\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: labels: { } annotations: jikkou.io/resource-location: \"file:///tmp/jikkou/topics.tpl\" spec: topics: - metadata: name: \"local-iot-events-fr\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-be\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-de\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-es\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-uk\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-us\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 Configuration jinja { # Enable/Disable recursive macro calls for rendering enableRecursiveMacroCalls = false } ","categories":"","description":"","excerpt":" Template helps you to dynamically define resource definition files ‚Ä¶","ref":"/docs/concepts/template/","tags":["concept","feature"],"title":"Template"},{"body":" Collectors are used to collect and describe all entities that exist into your system for a specific resource type.\nAvailable Collectors You can list all the available collectors using the Jikkou CLI command:\njikkou extensions list --type=Collector [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Collectors are used to collect and describe all entities that exist ‚Ä¶","ref":"/docs/concepts/collector/","tags":["concept","feature","extension"],"title":"Collectors"},{"body":" Controllers are used to compute and apply changes required to reconcile resources into a managed system.\nAvailable Controllers You can list all the available controllers using the Jikkou CLI command:\njikkou extensions list --type=Controller [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Controllers are used to compute and apply changes required to ‚Ä¶","ref":"/docs/concepts/controller/","tags":["concept","feature","extension"],"title":"Controllers"},{"body":" The KafkaTopicAclEntry resources are used to manage the Access Control Lists in Aiven for Apache Kafka¬Æ. A KafkaTopicAclEntry resource defines the permission to be granted to a user for one or more kafka topics.\nKafkaTopicAclEntry Specification Here is the resource definition file for defining a KafkaTopicAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE, READWRITE, ADMIN username: \u003c\u003e # The username topic: \u003c\u003e # Topic name or glob pattern Example Here is a simple example that shows how to define a single ACL entry using the KafkaTopicAclEntry resource type.\nfile: kafka-topic-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntry\" metadata: labels: { } annotations: { } spec: permission: \"READWRITE\" username: \"alice\" topic: \"public-*\" KafkaTopicAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a KafkaTopicAclEntryList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntryList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaTopicAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaTopicAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntryList\" items: - spec: permission: \"READWRITE\" username: \"alice\" topic: \"public-*\" - spec: permission: \"READ\" username: \"bob\" topic: \"public-*\" ","categories":"","description":"Learn how to manage Access Control Lists (ACLs) in Aiven for Apache Kafka¬Æ\n","excerpt":"Learn how to manage Access Control Lists (ACLs) in Aiven for Apache ‚Ä¶","ref":"/docs/providers/aiven/resources/kafka-topic-acl/","tags":["feature","resources"],"title":"ACL for Aiven Apache Kafka¬Æ"},{"body":" This section describes the resource definition format for kafkabrokers entities, which can be used to define the brokers you plan to manage on a specific Kafka cluster.\nListing KafkaBroker You can retrieve the state of Kafka Consumer Groups using the jikkou get kafkabrokers (or jikkou get kb) command.\nUsage Usage: Get all 'KafkaBroker' resources. jikkou get kafkabrokers [-hV] [--default-configs] [--dynamic-broker-configs] [--list] [--static-broker-configs] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkabrokers when you want to describe the state of all resources of type 'KafkaBroker'. OPTIONS: --default-configs Describe built-in default configuration for configs that have a default value. --dynamic-broker-configs Describe dynamic configs that are configured as default for all brokers or for specific broker in the cluster. -h, --help Show this help message and exit. --list Get resources as ResourceListObject. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. --static-broker-configs Describe static configs provided as broker properties at start up (e.g. server.properties file). -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kafkabrokers --static-broker-configs (output)\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaBroker\" metadata: name: \"101\" labels: {} annotations: kafka.jikkou.io/cluster-id: \"xtzWWN4bTjitpL3kfd9s5g\" spec: id: \"101\" host: \"localhost\" port: 9092 configs: advertised.listeners: \"PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\" authorizer.class.name: \"org.apache.kafka.metadata.authorizer.StandardAuthorizer\" broker.id: \"101\" controller.listener.names: \"CONTROLLER\" controller.quorum.voters: \"101@kafka:29093\" inter.broker.listener.name: \"PLAINTEXT\" listener.security.protocol.map: \"CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\" listeners: \"PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092,CONTROLLER://kafka:29093\" log.dirs: \"/var/lib/kafka/data\" node.id: \"101\" offsets.topic.replication.factor: \"1\" process.roles: \"broker,controller\" transaction.state.log.replication.factor: \"1\" zookeeper.connect: \"\" ","categories":"","description":"Learn how to manage Kafka Brokers.\n","excerpt":"Learn how to manage Kafka Brokers.\n","ref":"/docs/providers/kafka/resources/brokers/","tags":["feature","resources"],"title":"Kafka Brokers"},{"body":" This section describes the resource definition format for KafkaConsumerGroup entities, which can be used to define the consumer groups you plan to manage on a specific Kafka cluster.\nListing KafkaConsumerGroup You can retrieve the state of Kafka Consumer Groups using the jikkou get kafkaconsumergroups (or jikkou get kcg) command.\nUsage $ jikkou get kafkaconsumergroups --help Usage: Get all 'KafkaConsumerGroup' resources. jikkou get kafkaconsumergroups [-hV] [--list] [--offsets] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--in-states=PARAM]... [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkaconsumergroups when you want to describe the state of all resources of type 'KafkaConsumerGroup'. OPTIONS: -h, --help Show this help message and exit. --in-states=PARAM If states is set, only groups in these states will be returned. Otherwise, all groups are returned. This operation is supported by brokers with version 2.6.0 or later --list Get resources as ResourceListObject. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). --offsets Specify whether consumer group offsets should be described. -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. -V, --version Print version information and exit (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kafkaconsumergroups --in-states STABLE --offsets (output)\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false status: state: \"STABLE\" members: - memberId: \"console-consumer-b103994e-bcd5-4236-9d03-97065057e594\" clientId: \"console-consumer\" host: \"/127.0.0.1\" assignments: - \"my-topic-0\" offsets: - topic: \"my-topic\" partition: 0 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 ","categories":"","description":"Learn how to manage Kafka Consumer Groups.\n","excerpt":"Learn how to manage Kafka Consumer Groups.\n","ref":"/docs/providers/kafka/resources/consumer_groups/","tags":["feature","resources"],"title":"Kafka Consumer Groups"},{"body":" Here, you will find information to use the Core extensions.\nMore information:\n","categories":"","description":"The core Extensions for Jikkou\n","excerpt":"The core Extensions for Jikkou\n","ref":"/docs/providers/core/","tags":"","title":"Core"},{"body":"Prerequisites Jdk 17 (see https://sdkman.io/ for installing java locally) Git Docker and Docker-Compose Your favorite IDE Building Jikkou We use Maven Wrapper to build our project. The simplest way to get started is:\nFor building distribution files.\n$ ./mvnw clean package -Pdist -DskipTests Alternatively, we also use Make to package and build the Docker image for Jikkou:\n$ make Running tests For running all tests and checks:\n$ ./mvnw clean verify Code Format This project uses the Maven plugin Spotless to format all Java classes and to apply some code quality checks.\nBugs \u0026 Security This project uses the Maven plugin SpotBugs and FindSecBugs to run some static analysis to look for bugs in Java code.\nReported bugs can be analysed using SpotBugs GUI:\n$ ./mvnw spotbugs:gui ","categories":"","description":"How to set up your environment for developing on Jikkou.\n","excerpt":"How to set up your environment for developing on Jikkou.\n","ref":"/docs/community/developer-guide/","tags":"","title":"Developer Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Documentation"},{"body":" This section regroups all frequently asked questions about Jikkou.\nIs Jikkou Free to Use? Yes, Jikkou is developed and distributed under the Apache License 2.0.\nCan I Use Jikkou with Any Kafka Implementation? Yes, Jikkou can be used with a wide range of Apache Kafka infrastructures, including:\nApache Kafka Aiven Amazon MSK Confluent Cloud Redpanda Why would I use Jikkou over Terraform? What is Terraform and how is it typically used? Terraform (OpenToFu) is widely recognized as the leading solution for infrastructure provisioning and management. It is commonly used by operations teams for managing cloud infrastructure through its HCL (HashiCorp Configuration Language) syntax.\nWhat are the limitations of Terraform for Kafka Users ? Many development teams find Terraform challenging to use because:\nThey need to learn HCL syntax, which is not commonly known among developers. They often lack the necessary permissions to apply configuration files directly. They often struggle with Terraform states. How does Jikkou address these limitations? Jikkou is designed to be a straightforward CLI tool for both developers and operations teams. It simplifies the process of managing infrastructure, especially for development teams who may not have expertise in HCL or the permissions required for Terraform.\nWhat are the benefits of using Jikkou for Kafka management? On-Premises and Multi-Cloud Support: Unlike many Terraform providers which focus on cloud-based Kafka services ( e.g., Confluent Cloud), Jikkou supports on-premises, multi-cloud, and hybrid infrastructures.\nVersatility: Jikkou can manage Kafka topics across various environments, including local Kafka clusters in Docker, ephemeral clusters in Kubernetes for CI/CD, and production clusters in Aiven Cloud.\nAuditing and Backup: Beyond provisioning, Jikkou can audit Kafka platforms for configuration issues and create backups of Kafka configurations (Topics, ACLs, Quotas, etc.).\nThere are, of course, many reasons to use Terraform rather than Jikkou and vice versa. As usual, the choice of tool really depends on your needs, the organization you‚Äôre in, the skills of the people involved and so on.\n","categories":"","description":"","excerpt":" This section regroups all frequently asked questions about Jikkou.\nIs ‚Ä¶","ref":"/docs/frequently-asked-questions/","tags":"","title":"Frequently Asked Questions"},{"body":" This section describes the resource definition format for KafkaConnector entities, which can be used to define the configuration and status of connectors you plan to create and manage on specific Kafka Connect clusters.\nDefinition Format of KafkaConnector Below is the overall structure of the KafkaConnector resource.\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" # The api version (required) kind: \"KafkaConnector\" # The resource kind (required) metadata: name: \u003cstring\u003e # The name of the connector (required) labels: # Name of the Kafka Connect cluster to create the connector instance in (required). kafka.jikkou.io/connect-cluster: \u003cstring\u003e annotations: # Override client properties to connect to Kafka Connect cluster (optional). jikkou.io/config-override: | \u003cjson\u003e spec: connectorClass: \u003cstring\u003e # Name or alias of the class for this connector. tasksMax: \u003cinteger\u003e # The maximum number of tasks for the Kafka Connector. config: # Configuration properties of the connector. \u003ckey\u003e: \u003cvalue\u003e state: \u003cstring\u003e # The state the connector should be in. Defaults to running. See below for details about all these fields.\nMetadata metadata.name [required] The name of the connector.\nlabels.kafka.jikkou.io/connect-cluster [required] The name of the Kafka Connect cluster to create the connector instance in. The cluster name must be configured through the kafkaConnect.clusters[] Jikkou‚Äôs configuration setting (see: Configuration).\njikkou.io/config-override: [optional] The JSON client configurations to override for connecting to the Kafka Connect cluster. The configuration properties passed through this annotation override any cluster properties defined in the Jikkou‚Äôs configuration setting (see: Configuration).\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"my-connector\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: jikkou.io/config-override: | { \"url\": \"http://localhost:8083\" } Specification spec.connectorClass [required] The name or alias of the class for this connector.\nspec.tasksMax [optional] The maximum number of tasks for the Kafka Connector. Default is 1.\nspec.config [required] The connector‚Äôs configuration properties.\nspec.state [optional] The state the connector should be in. Defaults to running.\nBelow are the valid values:\nrunning: Transition the connector and its tasks to RUNNING state. paused: Pause the connector and its tasks, which stops message processing until the connector is resumed. stopped: Completely shut down the connector and its tasks. The connector config remains present in the config topic of the cluster (if running in distributed mode), unmodified. Examples The following is an example of a resource describing a Kafka connector:\n--- # Example: file: kafka-connector-filestream-sink.yaml apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" Listing KafkaConnector You can retrieve the state of Kafka Connector instances running on your Kafka Connect clusters using the jikkou get kafkaconnectors (or jikkou get kc) command.\nUsage $jikkou get kc --help Usage: Get all 'KafkaConnector' resources. jikkou get kafkaconnectors [-hV] [--expand-status] [-o=\u003cformat\u003e] [-s=\u003cexpressions\u003e]... Description: Use jikkou get kafkaconnectors when you want to describe the state of all resources of type 'KafkaConnector'. Options: --expand-status Retrieves additional information about the status of the connector and its tasks. -h, --help Show this help message and exit. -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). -s, --selector=\u003cexpressions\u003e The selector expression use for including or excluding resources. -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kc --expand-status (output)\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"localhost\" spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" worker_id: \"localhost:8083\" tasks: id: 1 state: \"RUNNING\" worker_id: \"localhost:8083\" The status.connectorStatus provides the connector status, as reported by the Kafka Connect REST API.\n","categories":"","description":"Learn how to manage Kafka Connectors.\n","excerpt":"Learn how to manage Kafka Connectors.\n","ref":"/docs/providers/kafka-connect/resources/connector/","tags":["feature","resources"],"title":"KafkaConnectors"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your SchemaRegistry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nDefinition Format of SchemaRegistrySubject Below is the overall structure of the SchemaRegistrySubject resource.\napiVersion: \"schemaregistry.jikkou.io/v1beta2\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: \u003cvendor_name\u003e # (optional) The vendor of the SchemaRegistry, e.g., Confluent, Karapace, etc compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003cstring\u003e # The name for the reference. subject: \u003cstring\u003e # The subject under which the referenced schema is registered. version: \u003cstring\u003e # The exact version of the schema under the registered subject. ] Metadata The metadata.name property is mandatory and specifies the name of the Subject.\nSpecification To use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } SchemaRegistrySubjectList If you need to manage multiple Schemas at once (e.g. using a template), it may be more suitable to use the resource collection SchemaRegistrySubjectList.\nSpecification Here the resource definition file for defining a SchemaRegistrySubjectList.\napiVersion: \"schemaregistry.jikkou.io/v1beta2\" # The api version (required) kind: \"SchemaRegistrySubjectList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # The array of SchemaRegistrySubject ","categories":"","description":"Learn how to manage SchemaRegistry Subjects.\n","excerpt":"Learn how to manage SchemaRegistry Subjects.\n","ref":"/docs/providers/schema-registry/resources/subject/","tags":["feature","resources"],"title":"Schema Registry Subjects"},{"body":" KafkaTopic resources are used to define the topics you want to manage on your Kafka Cluster(s). A KafkaTopic resource defines the number of partitions, the replication factor, and the configuration properties to be associated to a topics.\nKafkaTopic Specification Here is the resource definition file for defining a KafkaTopic.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopic\" # The resource kind (required) metadata: name: \u003cThe name of the topic\u003e # (required) labels: { } annotations: { } spec: partitions: \u003cNumber of partitions\u003e # (optional) replicas: \u003cNumber of replicas\u003e # (optional) configs: \u003cconfig_key\u003e: \u003cConfig Value\u003e # The topic config properties keyed by name to override (optional) configMapRefs: [ ] # The list of ConfigMap to be applied to this topic (optional) The metadata.name property is mandatory and specifies the name of the kafka topic.\nTo use the cluster default values for the number of partitions and replicas you can set the property spec.partitions and spec.replicas to -1.\nExample Here is a simple example that shows how to define a single YAML file containing two topic definition using the KafkaTopic resource type.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p1-r1' # Name of the topic labels: environment: example spec: partitions: 1 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p2-r1' # Name of the topic labels: environment: example spec: partitions: 2 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' See official Apache Kafka documentation for details about the topic-level configs.\nTips: Multiple topics can be included in the same YAML file by using --- lines. KafkaTopicList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaTopicList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopicList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaTopic Example Here is a simple example that shows how to define a single YAML file containing two topic definitions using the KafkaTopicList resource type. In addition, the example uses a ConfigMap object to define the topic configuration only once.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopicList metadata: labels: environment: example items: - metadata: name: 'my-topic-p1-r1' spec: partitions: 1 replicas: 1 configMapRefs: [ \"TopicConfig\" ] - metadata: name: 'my-topic-p2-r1' spec: partitions: 2 replicas: 1 configMapRefs: [ \"TopicConfig\" ] --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'TopicConfig' data: min.insync.replicas: 1 cleanup.policy: 'delete' ","categories":"","description":"Learn how to manage Kafka Topics.\n","excerpt":"Learn how to manage Kafka Topics.\n","ref":"/docs/providers/kafka/resources/topics/","tags":["feature","resources"],"title":"Kafka Topics"},{"body":" This section describes the resource definition format for KafkaUser entities, which can be used to manage SCRAM Users for Apache Kafka.\nDefinition Format of KafkaUser Below is the overall structure of the KafkaUser resource.\n--- apiVersion: kafka.jikkou.io/v1 # The api version (required) kind: KafkaUser # The resource kind (required) metadata: name: \u003cstring\u003e annotations: # force update kafka.jikkou.io/force-password-renewal: \u003cboolean\u003e spec: authentications: - type: \u003cenum\u003e # or password: \u003cstring\u003e # leave empty to generate secure password See below for details about all these fields.\nMetadata metadata.name [required] The name of the User.\nkafka.jikkou.io/force-password-renewal [optional] Specification spec.authentications [required] The list of authentications to manage for the user.\nspec.authentications[].type [required] The authentication type:\nscram-sha-256 scram-sha-512 spec.authentications[].password [required] The password of the user.\nExamples The following is an example of a resource describing a User:\n--- # Example: file: kafka-scram-users.yaml apiVersion: \"kafka.jikkou.io/v1\" kind: \"User\" metadata: name: \"Bob\" spec: authentications: - type: scram-sha-256 password: null - type: scram-sha-512 password: null Listing Kafka Users You can retrieve the SCRAM users of a Kafka cluster using the jikkou get kafkausers (or jikkou get ku) command.\nUsage $ jikkou get kc --help Usage: Get all 'KafkaUser' resources. jikkou get kafkausers [-hV] [--list] [--logger-level=\u003clevel\u003e] [--name=\u003cname\u003e] [-o=\u003cformat\u003e] [--selector-match=\u003cselectorMatchingStrategy\u003e] [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkausers when you want to describe the state of all resources of type 'KafkaUser'. OPTIONS: -h, --help Show this help message and exit. --list Get resources as ResourceListObject (default: false). --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` --name=\u003cname\u003e The name of the resource. -o, --output=\u003cformat\u003e Prints the output in the specified format. Valid values: JSON, YAML (default: YAML). -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. --selector-match=\u003cselectorMatchingStrategy\u003e The selector matching strategy. Valid values: NONE, ALL, ANY (default: ALL) -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get ku (output)\napiVersion: \"kafka.jikkou.io/v1\" kind: \"KafkaUser\" metadata: name: \"Bob\" labels: {} annotations: kafka.jikkou.io/cluster-id: \"xtzWWN4bTjitpL3kfd9s5g\" spec: authentications: - type: \"scram-sha-256\" iterations: 8192 - type: \"scram-sha-512\" iterations: 8192 ","categories":"","description":"Learn how to manage Kafka Users.\n","excerpt":"Learn how to manage Kafka Users.\n","ref":"/docs/providers/kafka/resources/users/","tags":["feature","resources"],"title":"Kafka Users"},{"body":" Providers are pluggable modules that supply Jikkou with extensions and resource definitions for a specific platform or service (e.g., Apache Kafka, Schema Registry, Aiven).\nWhat is a Provider? A provider is a pluggable module that registers a cohesive set of extensions (controllers, collectors, transformations, validations, actions, health indicators) and resource types into Jikkou at runtime.\nEach provider targets a specific platform and is responsible for:\nRegistering extensions such as controllers, collectors, transformations, validations, and actions. Registering resource types that describe the API resources it manages. Accepting configuration specific to its platform (e.g., Kafka bootstrap servers, Schema Registry URL). Configuration Providers are configured in the Jikkou configuration file under the jikkou.provider namespace. Each provider entry has a unique name and includes its type, an enabled flag, and a config block.\njikkou { provider.kafka { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { client { bootstrap.servers = \"localhost:9092\" } } } } Configuration Properties Property Type Description enabled Boolean Whether this provider is active. Defaults to true. type String Fully qualified class name of the provider implementation. default Boolean Mark this instance as the default for its provider type. Defaults to false. config Object Provider-specific configuration (e.g., connection settings). Multiple Instances You can configure multiple instances of the same provider type by giving each a unique name. This is useful when you need to manage resources across different environments (e.g., dev vs. production Kafka clusters) from a single Jikkou installation.\nUse default = true to mark one instance as the default for its provider type:\njikkou { provider.kafka-prod { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider default = true config = { client.bootstrap.servers = \"kafka-prod:9092\" } } provider.kafka-staging { enabled = true type = io.streamthoughts.jikkou.kafka.KafkaExtensionProvider config = { client.bootstrap.servers = \"kafka-staging:9092\" } } } Use the --provider flag to target a specific provider instance when running commands:\n# Uses the default provider (kafka-prod) jikkou get kafkatopics # Explicitly target the staging provider jikkou get kafkatopics --provider kafka-staging # Apply resources to staging jikkou apply -f my-resources.yaml --provider kafka-staging Provider Resolution The --provider flag is always optional. Jikkou resolves the target provider using the following fallback chain:\nSingle provider of a given type: It is used automatically ‚Äî no --provider flag needed, no default property needed. If you only have one Kafka provider configured, everything works exactly as before. Multiple providers, one marked default = true: The default is used when --provider is omitted. You only need the flag when targeting a non-default instance. Multiple providers, no default: You must specify --provider on every command. Omitting it results in an error: ‚ÄúNo default configuration defined, and multiple configurations found for provider type‚Äù. Note Existing single-provider configurations continue to work without any changes. The default property and --provider flag only matter once you add a second instance of the same provider type. Provider selection works across all commands ‚Äî apply, create, update, delete, diff, validate, replace, patch, get, action, and health ‚Äî and extends to the REST API server with a provider field in reconciliation request bodies.\nBuilt-in Providers Jikkou ships with the following built-in extension providers:\nProvider Description Apache Kafka Manage Kafka Topics, ACLs, Quotas, and Consumer Groups Schema Registry Manage Schema Registry subjects and schemas Kafka Connect Manage Kafka Connect connectors Aiven Manage Aiven-specific resources (ACLs, Quotas, Schema Registry) AWS Manage AWS Glue Schema Registry resources Core Core resource types (e.g., ConfigMap) Discovering Providers You can inspect providers and the extensions they contribute using the CLI:\n# List all registered extensions with their provider jikkou api-extensions list # List extensions for a specific provider jikkou api-extensions list --provider kafka # List API resources and their supported verbs jikkou api-resources SEE ALSO Controllers - Extensions that reconcile resources Collectors - Extensions that collect resource state Configuration - How to configure providers via contexts ","categories":"","description":"","excerpt":" Providers are pluggable modules that supply Jikkou with extensions ‚Ä¶","ref":"/docs/concepts/providers/","tags":["concept","feature","extension"],"title":"Providers"},{"body":" Reporters can be used to report changes applied by Jikkou to a third-party system.\nConfiguration Jikkou allows you to configure multiple reporters as follows:\njikkou { # The list of reporters to execute reporters: [ { # Custom name for the reporter name = \"\" # Simple or fully qualified class name of the transformation extension. type = \"\" config = { # Configuration properties for this reporter } } ] } Tips The config object passed to a reporter will fallback on the top-level jikkou config. This allows you to globally declare some configuration settings. Built-in implementations Jikkou packs with some built-in ChangeReporter implementations:\nKafkaChangeReporter The KafkaChangeReporter can be used to send change results into a given kafka topic. Changes will be published as Cloud Events.\nConfiguration The below example shows how to configure the KafkaChangeReporter.\njikkou { # The default custom reporters to report applied changes. reporters = [ { name = \"kafka-reporter\" type = io.streamthoughts.jikkou.kafka.reporter.KafkaChangeReporter config = { # The 'source' of the event that will be generated. event.source = \"jikkou/cli\" kafka = { # If 'true', topic will be automatically created if it does not already exist. topic.creation.enabled = true # The default replication factor used for creating topic. topic.creation.defaultReplicationFactor = 1 # The name of the topic the events will be sent. topic.name = \"jikkou-resource-change-event\" # The configuration settings for Kafka Producer and AdminClient client = ${jikkou.kafka.client} { client.id = \"jikkou-reporter-producer\" } } } } ] } ","categories":"","description":"","excerpt":" Reporters can be used to report changes applied by Jikkou to a ‚Ä¶","ref":"/docs/concepts/reporters/","tags":["feature","extensions"],"title":"Reporters"},{"body":" Actions allow a user to execute a specific and one-shot operation on resources.\nAvailable Actions (CLI) You can list all the available actions using the Jikkou CLI command:\njikkou api-extensions list --category=action [-kinds \u003ca resource kind to filter returned results\u003e] Execution Actions (CLI) You can execute a specific extension using the Jikkou CLI command:\njikkou action \u003cACTION_NAME\u003e execute [\u003coptions\u003e] ","categories":"","description":"","excerpt":" Actions allow a user to execute a specific and one-shot operation on ‚Ä¶","ref":"/docs/concepts/actions/","tags":["feature","extensions"],"title":"Actions"},{"body":"Overview A Resource Repository is an extensible component in Jikkou that enables dynamic loading of resources from various sources (e.g., local directories, remote Git repositories) into the execution context.\nRepositories are typically used to:\nLoad reusable resources across multiple environments or teams Keep transient or computed resources separate from persistent definitions Inject configuration or validation policies dynamically This feature is particularly useful when you want to define shareable or transient resources such as ConfigMap, ValidatingResourcePolicy, or other resource types outside the CLI input or local context.\nConfiguration Jikkou allows you to configure multiple repositories as follows:\njikkou { repositories: [ { # Unique name for the repository name = \"\u003crepository-name\u003e\" # Fully qualified class name or alias for the repository type type = \"\u003crepository-class-or-alias\u003e\" # Optional configuration specific to the repository implementation config = { # key = value } } ] } Built-in implementations Jikkou ships with the following built-in ResourceRepository implementations:\nLocalResourceRepository Loads resources from local files or directories.\nType: io.streamthoughts.jikkou.core.repository.LocalResourceRepository\nExample Configuration\njikkou { repositories = [ { name = \"local\" type = io.streamthoughts.jikkou.core.repository.LocalResourceRepository config { files = [ \"./resources/\", \"./policies/\" ] } } ] } See more: LocalResourceRepository Configuration\nGitHubResourceRepository Loads resources from a public or private GitHub repository.\nType: io.streamthoughts.jikkou.core.repository.GitHubResourceRepository\nExample Configuration\njikkou { repositories = [ { name = \"github-repository\" type = io.streamthoughts.jikkou.core.repository.GitHubResourceRepository config { repository = \"streamthoughts/jikkou\" branch = \"main\" paths = [ \"examples/\", \"config/\" ] # Optionally set an access token for private repositories # token = ${?GITHUB_TOKEN} } } ] } See more: GitHubResourceRepository Configuration\n","categories":"","description":"","excerpt":"Overview A Resource Repository is an extensible component in Jikkou ‚Ä¶","ref":"/docs/concepts/repositories/","tags":["concept","feature","extension"],"title":"Resource Repositories"},{"body":" Here, you will find information to use the Apache Kafka extensions.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extension Provider for Apache Kafka.\n","excerpt":"Lean how to use the Jikkou Extension Provider for Apache Kafka.\n","ref":"/docs/providers/kafka/","tags":"","title":"Apache Kafka"},{"body":" KafkaPrincipalAuthorization resources are used to define Access Control Lists (ACLs) for principals authenticated to your Kafka Cluster.\nJikkou can be used to describe all ACL policies that need to be created on Kafka Cluster\nKafkaPrincipalAuthorization Specification --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: [ ] # List of roles to be added to the principal (optional) acls: # List of KafkaPrincipalACL (required) - resource: type: \u003cThe type of the resource\u003e # (required) pattern: \u003cThe pattern to be used for matching resources\u003e # (required) patternType: \u003cThe pattern type\u003e # (required) type: \u003cThe type of this ACL\u003e # ALLOW or DENY (required) operations: [ ] # Operation that will be allowed or denied (required) host: \u003cHOST\u003e # IP address from which principal will have access or will be denied (optional) For more information on how to define authorization and ACLs, see the official Apache Kafka documentation: Security\nOperations The list below describes the valid values for the spec.acls.[].operations property :\nREAD WRITE CERATE DELETE ALTER DESCRIBE CLUSTER_ACTION DESCRIBE_CONFIGS ALTER_CONFIGS IDEMPOTENT_WRITE CREATE_TOKEN DESCRIBE_TOKENS ALL For more information see official Apache Kafka documentation: Operations in Kafka\nResource Types The list below describes the valid values for the spec.acls.[].resource.type property :\nTOPIC GROUP CLUSTER USER TRANSACTIONAL_ID For more information see official Apache Kafka documentation: Resources in Kafka\nPattern Types The list below describes the valid values for the spec.acls.[].resource.patternType property :\nLITERAL: Use to allow or denied a principal to have access to a specific resource name. MATCH: Use to allow or denied a principal to have access to all resources matching the given regex. PREFIXED: Use to allow or denied a principal to have access to all resources having the given prefix. Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalAuthorization\" # The resource kind (required) metadata: name: \"User:Alice\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: \"ALLOW\" operations: [ 'READ', 'WRITE' ] host: \"*\" - resource: type: 'topic' pattern: 'my-other-topic-.*' patternType: 'MATCH' type: 'ALLOW' operations: [ 'READ' ] host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: 'ALLOW' operations: [ 'READ', 'WRITE' ] host: \"*\" KafkaPrincipalRole Specification apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalRole\" # The resource kind (required) metadata: name: \u003cName of role\u003e # The name of the role (required) spec: acls: [ ] # A list of KafkaPrincipalACL (required) Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicRead\" spec: acls: - type: \"ALLOW\" operations: [ 'READ' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicWrite\" spec: acls: - type: \"ALLOW\" operations: [ 'WRITE' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: - \"KafkaTopicPublicRead\" - \"KafkaTopicPublicWrite\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: roles: - \"KafkaTopicPublicRead\" ","categories":"","description":"Learn how to manage Kafka Authorizations and ACLs. \n","excerpt":"Learn how to manage Kafka Authorizations and ACLs. \n","ref":"/docs/providers/kafka/resources/acls/","tags":["feature","resources"],"title":"Kafka Authorizations"},{"body":" ","categories":"","description":"What does your user need to know to try your project?\n","excerpt":"What does your user need to know to try your project?\n","ref":"/docs/community/","tags":"","title":"Community"},{"body":"Jikkou is an open source project, and we love getting patches and contributions to make Jikkou and its docs even better.\nContributing to Jikkou The Jikkou project itself lives in https://github.com/streamthoughts/jikkou\nCode reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nCreating issues Alternatively, if there‚Äôs something you‚Äôd like to see in Jikkou (or if you‚Äôve found something that isn‚Äôt working the way you‚Äôd expect), but you‚Äôre not sure how to fix it yourself, please create an issue.\n","categories":"","description":"How to contribute to Jikkou\n","excerpt":"How to contribute to Jikkou\n","ref":"/docs/community/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":" The KafkaConnectRestartConnectors action allows a user to restart all or just the failed Connector and Task instances for one or multiple named connectors.\nUsage (CLI) Usage: Execute the action. jikkou action KafkaConnectRestartConnectors execute [-hV] [--include-tasks] [--only-failed] [--connect-cluster=PARAM] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--connector-name=PARAM]... DESCRIPTION: The KafkaConnectRestartConnectors action a user to restart all or just the failed Connector and Task instances for one or multiple named connectors. OPTIONS: --connect-cluster=PARAM The name of the connect cluster. --connector-name=PARAM The connector's name. -h, --help Show this help message and exit. --include-tasks Specifies whether to restart the connector instance and task instances (includeTasks=true) or just the connector instance (includeTasks=false) --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: JSON, YAML (default YAML). --only-failed Specifies whether to restart just the instances with a FAILED status (onlyFailed=true) or all instances (onlyFailed=false) -V, --version Print version information and exit. Examples Restart all connectors for all Kafka Connect clusters. jikkou action kafkaconnectrestartconnectors execute (output)\n--- kind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: {} annotations: {} results: - status: \"SUCCEEDED\" data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" annotations: {} spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" workerId: \"connect:8083\" tasks: - id: 0 state: \"RUNNING\" workerId: \"connect:8083\" Restart all connectors with a FAILED status on all Kafka Connect clusters. jikkou action kafkaconnectrestartconnectors execute \\ --only-failed Restart specific connector and tasks for on Kafka Connect cluster jikkou action kafkaconnectrestartconnectors execute \\ --cluster-name my-connect-cluster --connector-name local-file-sink \\ --include-tasks ","categories":"","description":"Learn how to use the KafkaConnectRestartConnector action. \n","excerpt":"Learn how to use the KafkaConnectRestartConnector action. \n","ref":"/docs/providers/kafka-connect/actions/kafkaconnectrestartconnectors/","tags":["action","apache kafka","kafka connect"],"title":"KafkaConnectRestartConnectors"},{"body":" The KafkaConsumerGroupsResetOffsets action allows resetting offsets of consumer group. It supports one consumer group at the time, and group should be in EMPTY state.\nUsage (CLI) Usage: Execute the action. jikkou action KafkaConsumerGroupsResetOffsets execute [-hV] [--all] [--dry-run] [--to-earliest] [--to-latest] [--group=PARAM] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--to-datetime=PARAM] [--to-offset=PARAM] [--excludes=PARAM]... [--groups=PARAM]... [--includes=PARAM]... --topic=PARAM [--topic=PARAM]... DESCRIPTION: Reset offsets of consumer group. Supports multiple consumer groups, and groups should be in EMPTY state. You must choose one of the following reset specifications: to-datetime, by-duration, to-earliest, to-latest, to-offset. OPTIONS: --all Specifies to act on all consumer groups. --dry-run Only show results without executing changes on Consumer Groups. --excludes=PARAM List of patterns to match the consumer groups that must be excluded from the reset-offset action. --group=PARAM The consumer group to act on. --groups=PARAM The consumer groups to act on. -h, --help Show this help message and exit. --includes=PARAM List of patterns to match the consumer groups that must be included in the reset-offset action. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO` -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: JSON, YAML (default YAML). --to-datetime=PARAM Reset offsets to offset from datetime. Format: 'YYYY-MM-DDTHH:mm:SS.sss' --to-earliest Reset offsets to earliest offset. --to-latest Reset offsets to latest offset. --to-offset=PARAM Reset offsets to a specific offset. --topic=PARAM The topic whose partitions must be included in the reset-offset action. -V, --version Print version information and exit. Examples Reset Single Consumer Group to the earliest offsets jikkou action kafkaconsumergroupresetoffsets execute \\ --group my-group \\ --topic test \\ --to-earliest (output)\n--- kind: \"ApiActionResultSet\" apiVersion: \"core.jikkou.io/v1\" metadata: labels: {} annotations: configs.jikkou.io/to-earliest: \"true\" configs.jikkou.io/group: \"my-group\" configs.jikkou.io/dry-run: \"false\" configs.jikkou.io/topic: - \"test\" results: - status: \"SUCCEEDED\" errors: [] data: apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false annotations: {} status: state: \"EMPTY\" members: [] offsets: - topic: \"test\" partition: 1 offset: 0 - topic: \"test\" partition: 0 offset: 0 - topic: \"test\" partition: 2 offset: 0 - topic: \"--test\" partition: 0 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 Reset All Consumer Groups to the earliest offsets jikkou action kafkaconsumergroupresetoffsets execute \\ --all \\ --topic test \\ --to-earliest ","categories":"","description":"Learn how to use the KafkaConsumerGroupsResetOffsets action. \n","excerpt":"Learn how to use the KafkaConsumerGroupsResetOffsets action. \n","ref":"/docs/providers/kafka/actions/kafkaconsumergroupsresetoffsets/","tags":["action","apache kafka","kafka"],"title":"KafkaConsumerGroupsResetOffsets"},{"body":" The KafkaQuota resources are used to manage the Quotas in Aiven for Apache Kafka¬Æ service. For more details, see https://docs.aiven.io/docs/products/kafka/concepts/kafka-quotas\nKafkaQuota Specification Here is the resource definition file for defining a KafkaQuota.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaQuota\" # The resource kind (required) metadata: labels: { } annotations: { } spec: user: \u003cstring\u003e # The username: (Optional: 'default' if null) clientId: \u003cstring\u003e # The client-id consumerByteRate: \u003cnumber\u003e # The quota in bytes for restricting data consumption producerByteRate: \u003cnumber\u003e # The quota in bytes for restricting data production requestPercentage: \u003cnumber\u003e Example Here is a simple example that shows how to define a single ACL entry using the KafkaQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaQuota\" spec: user: \"default\" clientId: \"default\" consumerByteRate: 1048576 producerByteRate: 1048576 requestPercentage: 25 KafkaQuotaList If you need to define multiple Kafka quotas (e.g. using a template), it may be easier to use a KafkaQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaQuotaList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaQuotaList Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaQuotaList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaQuotaList\" items: - spec: user: \"default\" clientId: \"default\" consumerByteRate: 1048576 producerByteRate: 1048576 requestPercentage: 5 - spec: user: \"avnadmin\" consumerByteRate: 5242880 producerByteRate: 5242880 requestPercentage: 25 ","categories":"","description":"Learn how to manage Quotas in Aiven for Apache Kafka¬Æ\n","excerpt":"Learn how to manage Quotas in Aiven for Apache Kafka¬Æ\n","ref":"/docs/providers/aiven/resources/kafka-quota/","tags":["feature","resources"],"title":"Quotas for Aiven Apache Kafka¬Æ"},{"body":"The ValidatingResourcePolicy resource is used to define validation rules applied to resources or resource changes before they are applied by Jikkou.\nIt allows enforcing organizational policies, validating constraints, or filtering out undesired operations.\nEach policy can select one or more resource kinds and define rules expressed in Google CEL (Common Expression Language).\nRules can either fail the execution or filter the invalid resources, depending on the configured failurePolicy.\nSpecification apiVersion: core.jikkou.io/v1 kind: ValidatingResourcePolicy metadata: name: \u003cstring\u003e # Required. Unique policy name. spec: failurePolicy: \u003cstring\u003e # Required. One of: FAIL | FILTER selector: matchingStrategy: \u003cstring\u003e # Optional. One of: ALL | ANY (default: ALL) matchResources: - apiVersion: \u003cstring\u003e # Optional. API version to match (e.g., core.jikkou.io/v1) kind: \u003cstring\u003e # Required. Resource kind (e.g., KafkaTopic) matchLabels: - key: \u003cstring\u003e # Label key to match operator: \u003cstring\u003e # One of: In | NotIn | Exists | DoesNotExist values: [\u003cstring\u003e] # Optional list of values matchExpressions: - \u003cstring\u003e # CEL expression rules: - name: \u003cstring\u003e # Required. Rule identifier. expression: \u003cstring\u003e # Required. A CEL expression evaluated against the resource. message: \u003cstring\u003e # Optional. Static message returned when the rule fails. messageExpression: \u003cstring\u003e # Optional. CEL expression to generate a dynamic error message. Fields Field Type Required Description spec.failurePolicy string Yes Defines the policy behavior when validation fails. Possible values:‚Ä¢ FAIL ‚Üí stop execution with an error.‚Ä¢ FILTER ‚Üí skip the invalid resource(s) but continue processing others. spec.selector.matchingStrategy string No Strategy for combining multiple selectors. Possible values:‚Ä¢ ALL ‚Üí resource must match all conditions.‚Ä¢ ANY ‚Üí resource must match at least one condition.Default: ALL. spec.selector.matchResources list No Selects resources by API version and kind. spec.selector.matchLabels list No Selects resources based on labels, using operators (In, NotIn, Exists, DoesNotExist). spec.selector.matchExpressions list No Selects resources using CEL expressions for advanced filtering. spec.rules list Yes A list of validation rules. spec.rules[].name string Yes A unique identifier for the rule. spec.rules[].expression string Yes A CEL expression evaluated against the resource. The rule fails when the expression evaluates to true. spec.rules[].message string No Static error message returned when validation fails. spec.rules[].messageExpression string No CEL expression returning a dynamic error message string. Resource Selection Policies define which resources they apply to using a selector.\nA selector can combine multiple strategies to target resources based on:\nResource metadata (kind, apiVersion). Labels (with operators like In, NotIn, Exists, DoesNotExist). CEL expressions (arbitrary conditions on resource content). Matching Strategy Strategy Description ALL The resource must match all specified selectors (matchResources, matchLabels, and matchExpressions). ANY The resource is selected if it matches at least one of the specified selectors. Default: ALL\nmatchResources Selects resources by API version and/or kind.\nmatchResources: - apiVersion: core.jikkou.io/v1 kind: KafkaTopic apiVersion ‚Üí Optional. Restricts matching to a specific API group/version. kind ‚Üí Required. Matches the resource kind (e.g. KafkaTopic, KafkaTopicChange). matchLabels Selects resources based on their metadata labels using operators.\nmatchLabels: - key: environment operator: In values: [\"prod\", \"staging\"] - key: team operator: NotIn values: [\"test\"] - key: critical operator: Exists Supported operators:\nOperator Description In Matches if the label value is in the list of values. NotIn Matches if the label value is not in the list of values. Exists Matches if the label key is defined (value doesn‚Äôt matter). DoesNotExist Matches if the label key is not defined. matchExpressions Selects resources using CEL expressions for maximum flexibility.\nmatchExpressions: - \"resource.metadata.name.startsWith('topic-')\" - \"resource.spec.partitions \u003e 10\" Examples:\nMatch resources with names starting with topic-. Match topics with more than 10 partitions. Examples Example 1: Filtering DELETE operations on KafkaTopic resources apiVersion: core.jikkou.io/v1 kind: ValidatingResourcePolicy metadata: name: KafkaTopicPolicy spec: failurePolicy: FILTER selector: matchResources: - kind: KafkaTopicChange rules: - name: FilterDeleteOperation expression: \"size(resource.spec.changes) \u003e 0 \u0026\u0026 resource.spec.op == 'DELETE'\" messageExpression: \"'Operation ' + resource.spec.op + ' on topics is not authorized'\" This policy prevents delete operations on Kafka topics from being executed by filtering them out.\nExample 2: Validating partitions count for KafkaTopic apiVersion: core.jikkou.io/v1 kind: ValidatingResourcePolicy metadata: name: KafkaTopicPolicy spec: failurePolicy: FAIL selector: matchResources: - kind: KafkaTopic rules: - name: MaxTopicPartitions expression: \"resource.spec.partitions \u003e= 50\" messageExpression: \"'Topic partition MUST be inferior to 50, but was: ' + string(resource.spec.partitions)\" - name: MinTopicPartitions expression: \"resource.spec.partitions \u003c 3\" message: \"Topic must have at-least 3 partitions\" This policy enforces a minimum of 3 partitions and a maximum of 49 partitions for Kafka topics.\nExample 3: Match only KafkaTopic in prod environment selector: matchingStrategy: ALL matchResources: - kind: KafkaTopic matchLabels: - key: environment operator: In values: [\"prod\"] Example 4: Match any KafkaTopic OR resources with label critical=true selector: matchingStrategy: ANY matchResources: - kind: KafkaTopic matchLabels: - key: critical operator: In values: [\"true\"] Example 5: Match using CEL expression selector: matchExpressions: - \"resource.spec.replicationFactor \u003c 3\" Use cases Preventing destructive operations (e.g., deleting topics, removing configs). Enforcing resource limits (e.g., partition count, replication factor). Ensuring naming conventions or metadata compliance. Dynamically generating error messages with contextual information. ","categories":"","description":"The ValidatingResourcePolicy resource defines validation rules and selection strategies for applying policies to resources in Jikkou.","excerpt":"The ValidatingResourcePolicy resource defines validation rules and ‚Ä¶","ref":"/docs/providers/core/resources/validatingresourcepolicy/","tags":"","title":"ValidatingResourcePolicy"},{"body":" The SchemaRegistryAclEntry resources are used to manage the Access Control Lists in Aiven for Schema Registry. A SchemaRegistryAclEntry resource defines the permission to be granted to a user for one or more Schema Registry Subjects.\nSchemaRegistryAclEntry Specification Here is the resource definition file for defining a SchemaRegistryAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE username: \u003c\u003e # The username resource: \u003c\u003e # The Schema Registry ACL entry resource name pattern NOTE: The resource name pattern should be Config: or Subject:\u003csubject_name\u003e where subject_name must consist of alpha-numeric characters, underscores, dashes, dots and glob characters * and ?.\nExample Here is an example that shows how to define a simple ACL entry using the SchemaRegistryAclEntry resource type.\nfile: schema-registry-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntry\" spec: permission: \"READ\" username: \"Alice\" resource: \"Subject:*\" SchemaRegistryAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a SchemaRegistryAclEntryList resource.\nSpecification Here the resource definition file for defining a SchemaRegistryAclEntryList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntryList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of SchemaRegistryAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the SchemaRegistryAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntryList\" items: - spec: permission: \"READ\" username: \"alice\" resource: \"Config:\" - spec: permission: \"WRITE\" username: \"alice\" resource: \"Subject:*\" ","categories":"","description":"Learn how to manage Access Control Lists (ACLs) in Aiven for Schema Registry\n","excerpt":"Learn how to manage Access Control Lists (ACLs) in Aiven for Schema ‚Ä¶","ref":"/docs/providers/aiven/resources/schema-registry-acl/","tags":["feature","resources"],"title":"ACL for Aiven Schema Registry"},{"body":" Here, you will find information to use the Kafka Connect extension for Jikkou.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extension Provider for Apache Kafka Connect.\n","excerpt":"Lean how to use the Jikkou Extension Provider for Apache Kafka ‚Ä¶","ref":"/docs/providers/kafka-connect/","tags":"","title":"Apache Kafka Connect"},{"body":" KafkaClientQuota resources are used to define the quota limits to be applied on Kafka consumers and producers. A KafkaClientQuota resource can be used to apply limit to consumers and/or producers identified by a client-id or a user principal.\nKafkaClientQuota Specification Here is the resource definition file for defining a KafkaClientQuota.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuota\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } spec: type: \u003cThe quota type\u003e # (required) entity: clientId: \u003cThe id of the client\u003e # (required depending on the quota type). user: \u003cThe principal of the user\u003e # (required depending on the quota type). configs: requestPercentage: \u003cThe quota in percentage (%) of total requests\u003e # (optional) producerByteRate: \u003cThe quota in bytes for restricting data production\u003e # (optional) consumerByteRate: \u003cThe quota in bytes for restricting data consumption\u003e # (optional) Quota Types The list below describes the supported quota types:\nUSERS_DEFAULT: Set default quotas for all users. USER: Set quotas for a specific user principal. USER_CLIENT: Set quotas for a specific user principal and a specific client-id. USER_ALL_CLIENTS: Set default quotas for a specific user and all clients. CLIENT: Set default quotas for a specific client. CLIENTS_DEFAULT: Set default quotas for all clients. Example Here is a simple example that shows how to define a single YAML file containing two quota definitions using the KafkaClientQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 --- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 KafkaClientQuotaList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaClientQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuotaList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaClientQuota Example Here is a simple example that shows how to define a single YAML file containing two KafkaClientQuota definition using the KafkaClientQuotaList resource type.\napiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuotaList' metadata: labels: { } annotations: { } items: - spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 - spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 ","categories":"","description":"Learn how to manage Kafka Client Quotas\n","excerpt":"Learn how to manage Kafka Client Quotas\n","ref":"/docs/providers/kafka/resources/quotas/","tags":["feature","resources"],"title":"Kafka Quotas"},{"body":" A KafkaTableRecord resource can be used to produce a key/value record into a given compacted topic, i.e., a topic with cleanup.policy=compact (a.k.a. KTable).\nKafkaTableRecord Specification Here is the resource definition file for defining a KafkaTableRecord.\napiVersion: \"kafka.jikkou.io/v1beta1\" # The api version (required) kind: \"KafkaTableRecord\" # The resource kind (required) metadata: labels: { } annotations: { } spec: type: \u003cstring\u003e # The topic name (required) headers: # The list of headers - name: \u003cstring\u003e value: \u003cstring\u003e key: # The record-key (required) type: \u003cstring\u003e # The record-key type. Must be one of: BINARY, STRING, JSON (required) data: # The record-key in JSON serialized form. $ref: \u003curl or path\u003e # Or an url to a local file containing the JSON string value. value: # The record-value (required) type: \u003cstring\u003e # The record-value type. Must be one of: BINARY, STRING, JSON (required) data: # The record-value in JSON serialized form. $ref: \u003curl or path\u003e # Or an url to a local file containing the JSON string value. Usage The KafkaTableRecord resource has been designed primarily to manage reference data published and shared via Kafka. Therefore, it is highly recommended to use this resource only with compacted Kafka topics containing a small amount of data.\nExamples Here are some examples that show how to a KafkaTableRecord using the different supported data type.\nSTRING:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" spec: topic: \"my-topic\" headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: STRING data: | \"foo\" JSON:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" spec: topic: \"my-topic\" headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: JSON data: | { \"foo\": \"bar\" } BINARY:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" spec: topic: \"my-topic\" headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: BINARY data: | \"eyJmb28iOiAiYmFyIn0K\" ","categories":"","description":"Learn how to manage a KTable Topic Records\n","excerpt":"Learn how to manage a KTable Topic Records\n","ref":"/docs/providers/kafka/resources/ktable-records/","tags":["feature","resources"],"title":"Kafka Table Records"},{"body":" AwsGlueSchema resources are used to define the schemas you want to manage on your AWS Glue Schema registry. A AwsGlueSchema resource defines the schema, and the compatibility mode to be associated with a subject definition.\nAwsGlueSchema Specification Here is the resource definition file for defining a AwsGlueSchema.\napiVersion: \"aws.jikkou.io/v1\" # The api version (required) kind: \"AwsGlueSchema\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # The schema name (required) labels: glue.aws.amazon.com/registry-name: # The registry name (required) annotations: { } spec: compatibility: \u003ccompatibility\u003e # The schema compatibility level for this subject (required). dataFormat: \u003cThe data format\u003e # Accepted values are: AVRO, PROTOBUF, JSON (required). schemaDefinition: $ref: \u003curl or path\u003e # ] The metadata.name property is mandatory and specifies the name of the Subject.\nCompatibility Supported compatibility mode are:\nBACKWARD (recommended) ‚Äî Consumer can read both current and previous version. BACKWARD_ALL ‚Äî Consumer can read current and all previous versions. FORWARD ‚Äî Consumer can read both current and subsequent version. FORWARD_ALL ‚Äî Consumer can read both current and all subsequent versions. FULL ‚Äî Combination of Backward and Forward. FULL_ALL ‚Äî Combination of Backward all and Forward all. NONE ‚Äî No compatibility checks are performed. DISABLED ‚Äî Prevent any versioning for this schema Example Here is a simple example that shows how to define a single subject AVRO schema for type using the AwsGlueSchema resource type.\nfile: subject-user.yaml\n--- apiVersion: \"aws.jikkou.io/v1\" kind: \"AwsGlueSchema\" metadata: name: \"User\" labels: glue.aws.amazon.com/registry-name: Test annotations: glue.aws.amazon.com/normalize-schema: true spec: compatibility: \"BACKWARD\" dataFormat: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"aws.jikkou.io/v1\" kind: \"AwsGlueSchema\" metadata: name: \"User\" labels: { } annotations: glue.aws.amazon.com/normalize-schema: true spec: compatibility: \"BACKWARD\" dataFormat: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } ","categories":"","description":"Learn how to manage Schema in AWS Glue Schema Registry.\n","excerpt":"Learn how to manage Schema in AWS Glue Schema Registry.\n","ref":"/docs/providers/aws/resources/aws-glue-schema/","tags":["feature","resources"],"title":"Schema for AWS Glue Schema Registry"},{"body":" Here, you will find information to use the Schema Registry extensions.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extension Provider for Schema Registry.\n","excerpt":"Lean how to use the Jikkou Extension Provider for Schema Registry.\n","ref":"/docs/providers/schema-registry/","tags":"","title":"Schema Registry"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your Schema Registry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nSchemaRegistrySubject Specification Here is the resource definition file for defining a SchemaRegistrySubject.\napiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: 'Karapace' # (optional) The vendor of the Schema Registry compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003c\u003e # The name for the reference. subject: \u003c\u003e # The subject under which the referenced schema is registered. version: \u003c\u003e # The exact version of the schema under the registered subject. ] The metadata.name property is mandatory and specifies the name of the Subject.\nTo use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } ","categories":"","description":"Learn how to manage Schema Registry Subject Schema in Aiven.\n","excerpt":"Learn how to manage Schema Registry Subject Schema in Aiven.\n","ref":"/docs/providers/aiven/resources/schema-registry-subject/","tags":["feature","resources"],"title":"Subject for Aiven Schema Registry"},{"body":" Here, you will find information to use the Aiven for Kafka extensions.\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extensions Providers for Aiven.\n","excerpt":"Lean how to use the Jikkou Extensions Providers for Aiven.\n","ref":"/docs/providers/aiven/","tags":"","title":"Aiven"},{"body":" Here, you will find information to use the Aws extensions (Jikkou v0.36).\nMore information:\n","categories":"","description":"Lean how to use the Jikkou Extensions Providers for \"Aws\".\n","excerpt":"Lean how to use the Jikkou Extensions Providers for \"Aws\".\n","ref":"/docs/providers/aws/","tags":"","title":"Aws"},{"body":"Jikkou ships with the following built-in validations:\nNo validation\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/docs/providers/aiven/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nNo validation\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for AWS.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/docs/providers/aws/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\n","categories":"","description":"Learn how to use the validations provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the validations provided by the Kafka Connect ‚Ä¶","ref":"/docs/providers/kafka-connect/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nTopics NoDuplicateTopicsAllowedValidation (auto registered)\nTopicConfigKeysValidation (auto registered)\ntype = io.streamthoughts.jikkou.kafka.validation.TopicConfigKeysValidation The TopicConfigKeysValidation allows checking if the specified topic configs are all valid.\nTopicMinNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation The TopicMinNumPartitions allows checking if the specified number of partitions for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinNumPartitions Int Minimum number of partitions allowed TopicMaxNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMaxNumPartitions The TopicMaxNumPartitions allows checking if the number of partitions for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxNumPartitions Int Maximum number of partitions allowed TopicMinReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactor The TopicMinReplicationFactor allows checking if the specified replication factor for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinReplicationFactor Int Minimum replication factor allowed TopicMaxReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMaxReplicationFactor The TopicMaxReplicationFactor allows checking if the specified replication factor for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxReplicationFactor Int Maximum replication factor allowed TopicNamePrefix type = io.streamthoughts.jikkou.kafka.validation.TopicNamePrefix The TopicNamePrefix allows checking if the specified name for a topic starts with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNamePrefixes List List of topic name prefixes allows TopicNameSuffix type = io.streamthoughts.jikkou.kafka.validation.TopicNameSuffix The TopicNameSuffix allows checking if the specified name for a topic ends with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNameSuffixes List List of topic name suffixes allows ACLs NoDuplicateUsersAllowedValidation (auto registered)\nNoDuplicateRolesAllowedValidation (auto registered)\nQuotas QuotasEntityValidation (auto registered)\n","categories":"","description":"Learn how to use the built-in validations provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the built-in validations provided by the Extension ‚Ä¶","ref":"/docs/providers/kafka/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nSubject SchemaCompatibilityValidation type = io.streamthoughts.jikkou.schema.registry.validation.SchemaCompatibilityValidation The SchemaCompatibilityValidation allows testing the compatibility of the schema with the latest version already registered in the Schema Registry using the provided compatibility-level.\nAvroSchemaValidation The AvroSchemaValidation allows checking if the specified Avro schema matches some specific avro schema definition rules;\ntype = io.streamthoughts.jikkou.schema.registry.validation.AvroSchemaValidation Configuration\nName Type Description Default fieldsMustHaveDoc Boolean Verify that all record fields have a doc property false fieldsMustBeNullable Boolean Verify that all record fields are nullable false fieldsMustBeOptional Boolean Verify that all record fields are optional false ","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/docs/providers/schema-registry/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":" Here, you will find information about the annotations provided by the Aiven extension for Jikkou.\nList of built-in annotations kafka.aiven.io/acl-entry-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the ID of an ACL entry.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/aiven/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided by the AWS extension for Jikkou.\nList of built-in annotations glue.aws.amazon.com/created-time The date and time the schema was created.\nThe annotation is automatically added by Jikkou.\nglue.aws.amazon.com/updated-time The date and time the schema was updated.\nThe annotation is automatically added by Jikkou.\nglue.aws.amazon.com/registry-name The name of the registry.\nThe annotation is automatically added by Jikkou.\nglue.aws.amazon.com/registry-arn The Amazon Resource Name (ARN) of the registry.\nThe annotation is automatically added by Jikkou.\nglue.aws.amazon.com/schema-arn The Amazon Resource Name (ARN) of the schema.\nThe annotation is automatically added by Jikkou.\nglue.aws.amazon.com/schema-version-id The SchemaVersionId of the schema version.\nThe annotation is automatically added by Jikkou.\nglue.aws.amazon.com/use-canonical-fingerprint This annotation can be used to use a canonical fingerprint to compare schemas (only supported for Avro schema).\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for AWS.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/aws/annotations/","tags":"","title":"Annotations"},{"body":" This section lists a number of well known annotations, that have defined semantics. They can be attached to KafkaConnect resources through the metadata.annotations field and consumed as needed by extensions (i.e., validations, transformations, controller, collector, etc.).\nList of built-in annotations ","categories":"","description":"Learn how to use the metadata annotations provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the metadata annotations provided by the Kafka ‚Ä¶","ref":"/docs/providers/kafka-connect/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided the Apache Kafka extension for Jikkou.\nList of built-in annotations kafka.jikkou.io/cluster-id Used by jikkou.\nThe annotation is automatically added by Jikkou to a describe object part of an Apache Kafka cluster.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/kafka/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided by the Schema Registry extension for Jikkou.\nList of built-in annotations schemaregistry.jikkou.io/url Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the SchemaRegistry URL from which a subject schema is retrieved.\nschemaregistry.jikkou.io/schema-version Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject schema.\nschemaregistry.jikkou.io/schema-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject id.\nschemaregistry.jikkou.io/normalize-schema Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThis annotation can be used to normalize the schema on SchemaRegistry server side. Note, that Jikkou will attempt to normalize AVRO and JSON schema.\nSee: Confluent SchemaRegistry API Reference\nschemaregistry.jikkou.io/permanante-delete Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThe annotation can be used to specify a hard delete of the subject, which removes all associated metadata including the schema ID. The default is false. If the flag is not included, a soft delete is performed. You must perform a soft delete first, then the hard delete.\nSee: Confluent SchemaRegistry API Reference\nschemaregistry.jikkou.io/use-canonical-fingerprint This annotation can be used to use a canonical fingerprint to compare schemas (only supported for Avro schema).\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/docs/providers/schema-registry/annotations/","tags":"","title":"Annotations"},{"body":" This section lists a number of well known labels, that have defined semantics. They can be attached to KafkaConnect resources through the metadata.labels field and consumed as needed by extensions (i.e., validations, transformations, controller, collector, etc.).\nLabels kafka.jikkou.io/connect-cluster # Example --- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: labels: kafka.jikkou.io/connect-cluster: 'my-connect-cluster' The value of this label defined the name of the Kafka Connect cluster to create the connector instance in. The cluster name must be configured through the kafkaConnect.clusters[] Jikkou‚Äôs configuration setting (see: Configuration).\n","categories":"","description":"Learn how to use the metadata labels provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the metadata labels provided by the Kafka Connect ‚Ä¶","ref":"/docs/providers/kafka-connect/labels/","tags":"","title":"Labels"},{"body":" Here, you will find the list of actions provided by the Extension Provider for Kafka Connect.\nKafka Connect Action More information:\n","categories":"","description":"Learn how to use the actions provided by the Extension Provider for Kafka Connect.\n","excerpt":"Learn how to use the actions provided by the Extension Provider for ‚Ä¶","ref":"/docs/providers/kafka-connect/actions/","tags":"","title":"Actions"},{"body":" Here, you will find the list of actions provided by the Extension Provider for Apache Kafka.\nApache Kafka Action More information:\n","categories":"","description":"Learn how to use the actions provided by the Extension Provider for Apache Kafka.\n","excerpt":"Learn how to use the actions provided by the Extension Provider for ‚Ä¶","ref":"/docs/providers/kafka/actions/","tags":"","title":"Actions"},{"body":"The Apache Kafka extension for Jikkou utilizes the Kafka Admin Client which is compatible with any Kafka infrastructure, such as :\nAiven Apache Kafka Confluent Cloud MSK Redpanda etc. In addition, Kafka Protocol has a ‚Äúbidirectional‚Äù client compatibility policy. In other words, new clients can talk to old servers, and old clients can talk to new servers.\n","categories":"","description":"Compatibility for Apache Kafka.\n","excerpt":"Compatibility for Apache Kafka.\n","ref":"/docs/providers/kafka/compatibility/","tags":"","title":"Compatibility"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/action/","tags":"","title":"action"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/apache-kafka/","tags":"","title":"apache kafka"},{"body":"","categories":"","description":"Jikkou - API References","excerpt":"Jikkou - API References","ref":"/docs/jikkou-api-server/api-references/","tags":"","title":"Jikkou - API References"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/concept/","tags":"","title":"concept"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/extension/","tags":"","title":"extension"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/extensions/","tags":"","title":"extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/feature/","tags":"","title":"feature"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/how-to/","tags":"","title":"how-to"},{"body":" Open source\nResource as Code\nFramework\nfor Apache Kafka¬Æ Install Jikkou What is Jikkou ? Jikkou is a powerful, flexible open-source framework that enables self-serve resource provisioning. It allows developers and DevOps teams to easily manage, automate, and provision all the resources needed for their Apache Kafka platform. Get started with Jikkou Declarative \u0026 Automated Describe the entire desired state of any resource you need to manage using YAML descriptor files.\nDesigned for Apache Kafka¬Æ Jikkou was initially developed to manage Apache Kafka resources. You can use it with most of Apache Kafka vendors: Apache Kafka, Aiven, Amazon MSK, Confluent Cloud, Redpanda.\nExtensible and Customizable Jikkou can be extended to manage almost anything. It provides a simple and powerful Core API (in Java) allowing you to write custom extensions for managing your own system and resources.\nOpen source Jikkou is released under the Apache License 2.0. Anyone can contribute to Jikkou by opening an issue, a pull request (PR) or just by discussing with other users on the Slack Channel.\nJoin us on Slack Join the Jikkou community on Slack\nJoins Us Contributions welcome Want to join the fun on Github? New users are always welcome!\nContribute Support Jikkou Team Add a star to the GitHub project, it only takes 5 seconds!\nStar ","categories":"","description":"","excerpt":" Open source\nResource as Code\nFramework\nfor Apache Kafka¬Æ Install ‚Ä¶","ref":"/","tags":"","title":"Jikkou"},{"body":"List and execute actions.\nSynopsis The action command manages Jikkou actions. Actions are named operations that can be executed on the target platform. Like the get command, action uses dynamic subcommands based on the actions available from your configured providers.\nEach action is registered as a subcommand under jikkou action, and has an execute subcommand to run it.\njikkou action \u003caction-name\u003e execute [flags] Examples # List all available actions (shown as subcommands in help) jikkou action --help # Execute an action jikkou action KafkaConsumerGroupsResetOffsets execute --options topic=my-topic --options group=my-group # Execute an action with YAML output jikkou action KafkaConsumerGroupsResetOffsets execute -o YAML # Execute an action targeting a specific provider jikkou action KafkaConsumerGroupsResetOffsets execute --provider kafka-prod --options topic=my-topic Options (execute subcommand) Flag Short Default Description --output -o YAML Output format. Valid values: JSON, YAML --provider Select a specific provider instance Additional options may be available depending on the action. These are dynamically registered based on the action‚Äôs ApiOptionSpec definitions. Use jikkou api-extensions get \u003caction-name\u003e to view options for a specific action.\nOptions inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou api-extensions - List and inspect available extensions jikkou get - Display resources from the platform ","categories":"","description":"","excerpt":"List and execute actions.\nSynopsis The action command manages Jikkou ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-action/","tags":"","title":"jikkou action"},{"body":"Print the supported API extensions.\nSynopsis The api-extensions command lists and inspects the API extensions registered in Jikkou. Extensions include resource collectors, transformations, validations, actions, health indicators, and controllers.\nThis command has two subcommands:\napi-extensions list - List all extensions api-extensions get - Get details about a specific extension Subcommands jikkou api-extensions list Print a summary table of all supported API extensions.\njikkou api-extensions list [flags] Examples # List all extensions jikkou api-extensions list # List extensions of a specific category jikkou api-extensions list --category Transformation # List extensions from a specific provider jikkou api-extensions list --provider kafka # List extensions that support a specific resource kind jikkou api-extensions list --kind KafkaTopic Options Flag Default Description --category Limit to extensions of the specified category --provider Limit to extensions of the specified provider --kind Limit to extensions that support the specified resource kind jikkou api-extensions get Print detailed information about a specific API extension, including its title, description, configuration options, and usage examples.\njikkou api-extensions get \u003cname\u003e [flags] Examples # Get details about an extension in WIDE format (default) jikkou api-extensions get KafkaTopicMaxRetentionMs # Get details in JSON format jikkou api-extensions get KafkaTopicMaxRetentionMs -o JSON # Get details in YAML format jikkou api-extensions get KafkaTopicMaxRetentionMs -o YAML Options Flag Short Default Description --output -o WIDE Output format. Valid values: JSON, YAML, WIDE Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou api-resources - List available resource types jikkou get - Display resources of a given type ","categories":"","description":"","excerpt":"Print the supported API extensions.\nSynopsis The api-extensions ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-api-extensions/","tags":"","title":"jikkou api-extensions"},{"body":"Print the supported API resources.\nSynopsis List the API resources supported by the Jikkou CLI or Jikkou API Server (in proxy mode). This command shows the resource name, short names, API version, kind, and supported verbs for each resource type.\nUse this command to discover which resource types are available with your current configuration and providers.\njikkou api-resources [flags] Examples # List all available API resources jikkou api-resources # List resources for a specific API group jikkou api-resources --api-group kafka.jikkou.io # List resources that support specific verbs jikkou api-resources --verbs LIST,GET # List resources that support CREATE jikkou api-resources --verbs CREATE # Combine filters jikkou api-resources --api-group kafka.jikkou.io --verbs LIST Sample output NAME SHORTNAMES APIVERSION KIND VERBS kafkatopics kt kafka.jikkou.io/v1beta2 KafkaTopic CREATE, DELETE, GET, LIST, UPDATE kafkaconsumergroups kafka.jikkou.io/v1beta2 KafkaConsumerGroup DELETE, GET, LIST Options Flag Default Description --api-group Limit to resources in the specified API group --verbs Limit to resources that support the specified verbs (comma-separated) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou api-extensions - List and inspect available extensions jikkou get - Display resources of a given type ","categories":"","description":"","excerpt":"Print the supported API resources.\nSynopsis List the API resources ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-api-resources/","tags":"","title":"jikkou api-resources"},{"body":"Update the resources aand as described by the resource definition files.\nSynopsis Reconciles the target platform so that the resources match the resource definition files passed as arguments. This command uses the FULL reconciliation mode, meaning it will create, update, and delete resources as needed to match the desired state defined in your resource files.\njikkou apply [flags] Examples # Apply resources defined in a YAML file jikkou apply -f my-resources.yaml # Apply resources from a directory jikkou apply -f ./resources/ # Apply resources with dry-run to preview changes jikkou apply -f my-resources.yaml --dry-run # Apply resources with specific selector jikkou apply -f my-resources.yaml --selector 'metadata.name IN (my-topic)' # Apply resources with template values jikkou apply -f my-resources.yaml --values-files values.yaml # Apply with inline template variable jikkou apply -f my-resources.yaml -v topicName=my-topic # Apply with custom labels jikkou apply -f my-resources.yaml -l environment=production # Apply with controller options jikkou apply -f my-resources.yaml --options delete-orphans=true # Apply targeting a specific provider jikkou apply -f my-resources.yaml --provider kafka-prod # Apply with JSON output format jikkou apply -f my-resources.yaml -o JSON --pretty Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o TEXT Output format. Valid values: TEXT, COMPACT, JSON, YAML --pretty false Pretty print JSON output --dry-run false Execute command in dry-run mode (preview changes without applying) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou create - Create resources (CREATE mode only) jikkou update - Create or update resources (UPDATE mode only) jikkou delete - Delete resources (DELETE mode only) jikkou diff - Show changes without applying jikkou validate - Validate resource definitions ","categories":"","description":"","excerpt":"Update the resources aand as described by the resource definition ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-apply/","tags":"","title":"jikkou apply"},{"body":"Sets or retrieves the configuration of this client.\nSynopsis The config command manages Jikkou CLI configuration contexts. A context defines the configuration settings (config file path, inline properties, provider bindings) used when running Jikkou commands.\nConfiguration is stored in the Jikkou config file, located by default at $HOME/.jikkou/config.\nSubcommands Command Description jikkou config set-context Configure a context with the provided arguments jikkou config get-contexts List all configured contexts jikkou config current-context Display the current context jikkou config use-context Switch to a specified context jikkou config view Show merged configuration settings Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO Configuration - Learn how to configure Jikkou CLI ","categories":"","description":"","excerpt":"Sets or retrieves the configuration of this client.\nSynopsis The ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-config/","tags":"","title":"jikkou config"},{"body":"Display the current context.\nSynopsis Displays the current context used by Jikkou CLI, including the configuration file path and inline configuration properties associated with it.\njikkou config current-context Examples # Show the current context $ jikkou config current-context Using context 'localhost' KEY VALUE ConfigFile ConfigProps {\"provider.kafka.config.client.bootstrap.servers\":\"localhost:9092\"} Options This command has no specific options.\nOptions inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou config get-contexts - List all contexts jikkou config use-context - Switch to a different context jikkou config view - Show the merged configuration jikkou config - Config command overview ","categories":"","description":"","excerpt":"Display the current context.\nSynopsis Displays the current context ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-config-current-context/","tags":"","title":"jikkou config current-context"},{"body":"List all configured contexts.\nSynopsis Get all contexts defined in the Jikkou config file. The current context is marked with an asterisk (*).\njikkou config get-contexts Examples # List all contexts $ jikkou config get-contexts NAME localhost * development staging production Options This command has no specific options.\nOptions inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou config current-context - Show the current context details jikkou config use-context - Switch to a context jikkou config set-context - Create or update a context jikkou config - Config command overview ","categories":"","description":"","excerpt":"List all configured contexts.\nSynopsis Get all contexts defined in the ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-config-get-contexts/","tags":"","title":"jikkou config get-contexts"},{"body":"Configure a context with the provided arguments.\nSynopsis Configures the specified context with the provided arguments. A context stores a reference to a configuration file and/or inline configuration properties. If the context already exists, it will be updated.\nAfter setting a context, use jikkou config use-context to switch to it.\njikkou config set-context \u003ccontext-name\u003e [flags] Examples # Create a context with inline Kafka bootstrap server jikkou config set-context localhost \\ --config-props=provider.kafka.config.client.bootstrap.servers=localhost:9092 # Create a context pointing to a configuration file jikkou config set-context production --config-file=/path/to/production.conf # Create a context with a properties file jikkou config set-context staging --config-props-file=/path/to/staging.properties # Create a context with provider-scoped properties jikkou config set-context dev \\ --provider kafka \\ --config-props=client.bootstrap.servers=kafka-dev:9092 # Create a context with a config prefix jikkou config set-context dev \\ --config-prefix=provider.kafka.config \\ --config-props=client.bootstrap.servers=kafka-dev:9092 Options Flag Default Description --config-file Path to a Jikkou configuration file --config-props Inline configuration properties as key=value pairs (repeatable) --config-props-file Path(s) to one or more configuration properties files (repeatable) --provider Name of the provider to which this configuration should be attached --config-prefix Prefix to apply to all configuration property keys Arguments Argument Description context-name (required) The name of the context to configure Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou config use-context - Switch to a context jikkou config get-contexts - List all contexts jikkou config - Config command overview ","categories":"","description":"","excerpt":"Configure a context with the provided arguments.\nSynopsis Configures ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-config-set-context/","tags":"","title":"jikkou config set-context"},{"body":"Switch to a specified context.\nSynopsis Configures Jikkou to use the specified context. All subsequent commands will use the configuration associated with this context.\njikkou config use-context \u003ccontext-name\u003e Examples # Switch to the production context $ jikkou config use-context production Using context 'production' # If already using the specified context $ jikkou config use-context production Already using context production Arguments Argument Description context-name (required) The name of the context to switch to Options This command has no specific options.\nOptions inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou config get-contexts - List all available contexts jikkou config current-context - Display the current context jikkou config set-context - Create or update a context jikkou config - Config command overview ","categories":"","description":"","excerpt":"Switch to a specified context.\nSynopsis Configures Jikkou to use the ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-config-use-context/","tags":"","title":"jikkou config use-context"},{"body":"Show merged configuration settings.\nSynopsis Show the merged Jikkou configuration settings for the current context (or a named context). The configuration is rendered in HOCON format, showing all resolved values from the config file, inline properties, and defaults.\nUse --debug to see the origin of each setting, or --comments to include human-written comments from the configuration files.\njikkou config view [flags] Examples # View the current configuration jikkou config view # View configuration for a specific context jikkou config view --name production # View configuration with origin comments (useful for debugging) jikkou config view --debug # View configuration with human-written comments jikkou config view --comments Options Flag Default Description --name The name of the context configuration to view (defaults to current context) --debug false Print configuration with the origin of each setting as comments --comments false Print configuration with human-written comments Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou config current-context - Display the current context jikkou config set-context - Create or update a context jikkou config - Config command overview ","categories":"","description":"","excerpt":"Show merged configuration settings.\nSynopsis Show the merged Jikkou ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-config-view/","tags":"","title":"jikkou config view"},{"body":"Create resources from the resource definition files (only non-existing resources will be created).\nSynopsis Reconcile the target platform by creating all non-existing resources that are described by the resource definition files passed as arguments. This command uses the CREATE reconciliation mode, meaning only new resources will be created. Existing resources will not be updated or deleted.\njikkou create [flags] Examples # Create resources defined in a YAML file jikkou create -f my-resources.yaml # Preview what would be created jikkou create -f my-resources.yaml --dry-run # Create resources from a directory jikkou create -f ./resources/ # Create resources with template values jikkou create -f my-resources.yaml --values-files values.yaml # Create resources targeting a specific provider jikkou create -f my-resources.yaml --provider kafka-dev Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o TEXT Output format. Valid values: TEXT, COMPACT, JSON, YAML --pretty false Pretty print JSON output --dry-run false Execute command in dry-run mode (preview changes without applying) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou apply - Apply all changes (FULL mode) jikkou update - Create or update resources (UPDATE mode) jikkou delete - Delete resources (DELETE mode) jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Create resources from the resource definition files (only non-existing ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-create/","tags":"","title":"jikkou create"},{"body":"Delete resources that are no longer described by the resource definition files.\nSynopsis Reconcile the target platform by deleting all existing resources that are no longer described by the resource definition files passed as arguments. This command uses the DELETE reconciliation mode, meaning only deletions will be performed. No resources will be created or updated.\njikkou delete [flags] Examples # Delete resources no longer defined in a YAML file jikkou delete -f my-resources.yaml # Preview what would be deleted jikkou delete -f my-resources.yaml --dry-run # Delete resources from a directory jikkou delete -f ./resources/ # Delete with selector to target specific resources jikkou delete -f my-resources.yaml -s 'metadata.name IN (old-topic)' # Delete resources targeting a specific provider jikkou delete -f my-resources.yaml --provider kafka-dev Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o TEXT Output format. Valid values: TEXT, COMPACT, JSON, YAML --pretty false Pretty print JSON output --dry-run false Execute command in dry-run mode (preview changes without applying) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou apply - Apply all changes (FULL mode) jikkou create - Create resources only (CREATE mode) jikkou update - Create or update resources (UPDATE mode) jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Delete resources that are no longer described by the resource ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-delete/","tags":"","title":"jikkou delete"},{"body":"Show resource changes required by the current resource definitions.\nSynopsis Generates a speculative reconciliation plan, showing the resource changes Jikkou would apply to reconcile the resource definitions. This command does not actually perform the reconciliation actions.\nUse diff to preview what changes would be made before running apply, create, update, or delete.\njikkou diff [flags] Examples # Show changes required by a resource definition file jikkou diff -f my-resources.yaml # Show only resources that would be created jikkou diff -f my-resources.yaml --filter-resource-op CREATE # Show only resources that would be created or deleted jikkou diff -f my-resources.yaml --filter-resource-op CREATE,DELETE # Filter changes to show only update operations jikkou diff -f my-resources.yaml --filter-change-op UPDATE # Output as a resource list jikkou diff -f my-resources.yaml --list # Output in JSON format jikkou diff -f my-resources.yaml -o JSON # Diff with a specific provider jikkou diff -f my-resources.yaml --provider kafka-prod # Diff with selector jikkou diff -f my-resources.yaml -s 'metadata.name MATCHES (my-topic-.*)' Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o YAML Output format. Valid values: JSON, YAML --filter-resource-op Filter resources by operation (comma-separated). Valid values: NONE, CREATE, DELETE, REPLACE, UPDATE --filter-change-op Filter state changes by operation (comma-separated). Valid values: NONE, CREATE, DELETE, REPLACE, UPDATE --list false Output resources as an ApiResourceChangeList Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou apply - Apply all changes jikkou validate - Validate resource definitions jikkou prepare - Prepare resources for validation ","categories":"","description":"","excerpt":"Show resource changes required by the current resource definitions. ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-diff/","tags":"","title":"jikkou diff"},{"body":"Display one or many specific resources.\nSynopsis Display one or many resources of a given type from the target platform. The get command uses dynamic subcommands based on the resource types available from your configured providers. Use jikkou api-resources to discover the available resource types.\nWhen a resource name is specified, it retrieves that specific resource. Otherwise, it lists all resources of the given type.\njikkou get \u003cresource-type\u003e [name] [flags] Examples # List all Kafka topics jikkou get kafkatopics # Get a specific Kafka topic by name jikkou get kafkatopics my-topic # List resources with a selector jikkou get kafkatopics -s 'metadata.name MATCHES (my-.*)' # List resources with JSON output jikkou get kafkatopics -o JSON # List resources as a ResourceListObject jikkou get kafkatopics --list # List resources with custom options jikkou get kafkatopics --options describe-default-configs=true # List resources targeting a specific provider jikkou get kafkatopics --provider kafka-prod # Discover available resource types jikkou api-resources --verbs LIST,GET Options Flag Short Default Description --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --output -o YAML Output format. Valid values: JSON, YAML --list false Output resources as a ResourceListObject --provider Select a specific provider instance Additional options may be available depending on the resource type. These are dynamically registered based on the provider‚Äôs ApiOptionSpec definitions. Use jikkou api-extensions get \u003cextension-name\u003e to view options for a specific resource collector.\nOptions inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou api-resources - List available resource types jikkou api-extensions - List and inspect extensions jikkou diff - Show changes between desired and actual state ","categories":"","description":"","excerpt":"Display one or many specific resources.\nSynopsis Display one or many ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-get/","tags":"","title":"jikkou get"},{"body":"Print or describe health indicators.\nSynopsis The health command provides information about the health of target environments. It has two subcommands:\nhealth get - Retrieve health status for one or all indicators health get-indicators - List all available health indicators Subcommands jikkou health get Get health information for a specific indicator or all indicators.\njikkou health get \u003cindicator|all\u003e [flags] Examples # Get health for all indicators jikkou health get all # Get health for a specific indicator jikkou health get kafka # Get health with a custom timeout jikkou health get all --timeout-ms 5000 # Get health in JSON format jikkou health get all -o JSON # Get health targeting a specific provider jikkou health get kafka --provider kafka-prod Options Flag Short Default Description --output -o YAML Output format. Valid values: JSON, YAML --timeout-ms 2000 Timeout in milliseconds for retrieving health indicators --provider Select a specific provider instance The command exits with code 0 if the health status is UP, or a non-zero code otherwise.\njikkou health get-indicators List all available health indicators. This command takes no options and displays the name and description of each registered health indicator.\njikkou health get-indicators Examples # List all health indicators jikkou health get-indicators Options This subcommand has no specific options.\nOptions inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou api-extensions - List and inspect extensions jikkou server-info - Display server information (proxy mode) ","categories":"","description":"","excerpt":"Print or describe health indicators.\nSynopsis The health command ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-health/","tags":"","title":"jikkou health"},{"body":"Execute all changes for the specified reconciliation mode.\nSynopsis Reconcile resources by applying all the changes as defined in the resource descriptor files passed through the arguments. Unlike the other reconciliation commands (apply, create, update, delete), patch requires you to explicitly specify the reconciliation mode using the --mode flag.\nThe patch command applies changes to existing resources on the platform by computing a diff between the current and desired states, then executing only the operations allowed by the selected mode.\njikkou patch --mode \u003cmode\u003e [flags] Examples # Patch resources using FULL reconciliation mode jikkou patch --mode FULL -f my-resources.yaml # Patch resources - create only jikkou patch --mode CREATE -f my-resources.yaml # Patch resources - update only jikkou patch --mode UPDATE -f my-resources.yaml # Preview patches without applying jikkou patch --mode FULL -f my-resources.yaml --dry-run # Patch with a specific provider jikkou patch --mode UPDATE -f my-resources.yaml --provider kafka-prod Options Flag Short Default Description --mode (required) The reconciliation mode. Valid values: CREATE, DELETE, UPDATE, FULL --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o TEXT Output format. Valid values: TEXT, COMPACT, JSON, YAML --pretty false Pretty print JSON output --dry-run false Execute command in dry-run mode (preview changes without applying) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou apply - Apply all changes (FULL mode) jikkou replace - Replace resources by deleting and recreating jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Execute all changes for the specified reconciliation mode.\nSynopsis ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-patch/","tags":"","title":"jikkou patch"},{"body":"Prepare the resource definition files for validation.\nSynopsis Prepare the resource definition files specified through the command line arguments for validation. This command applies all configured transformations to the resource definitions and outputs the result, without running validation rules or performing reconciliation.\nUse prepare to inspect how your resources look after template rendering and transformations, before validation and reconciliation happen.\njikkou prepare [flags] Examples # Prepare resources from a YAML file jikkou prepare -f my-resources.yaml # Prepare resources from a directory jikkou prepare -f ./resources/ # Prepare with template values jikkou prepare -f my-resources.yaml --values-files values.yaml # Prepare with JSON output jikkou prepare -f my-resources.yaml -o JSON # Prepare with selector jikkou prepare -f my-resources.yaml -s 'metadata.name IN (my-topic)' # Prepare targeting a specific provider jikkou prepare -f my-resources.yaml --provider kafka-prod Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o YAML Output format. Valid values: JSON, YAML Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou validate - Validate prepared resources jikkou apply - Apply changes jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Prepare the resource definition files for validation.\nSynopsis Prepare ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-prepare/","tags":"","title":"jikkou prepare"},{"body":"Replace all resources.\nSynopsis Replaces resources by deleting and (re)creating all the resources as defined in the resource descriptor files passed through the arguments. Unlike apply, which performs incremental updates, replace performs a full replacement of resources: existing resources are deleted first and then recreated from scratch.\nUse replace when you need to ensure resources exactly match the desired state without any leftover configuration from previous versions.\njikkou replace [flags] Examples # Replace resources defined in a YAML file jikkou replace -f my-resources.yaml # Preview what would be replaced jikkou replace -f my-resources.yaml --dry-run # Replace resources from a directory jikkou replace -f ./resources/ # Replace resources targeting a specific provider jikkou replace -f my-resources.yaml --provider kafka-prod # Replace with JSON output jikkou replace -f my-resources.yaml -o JSON --pretty Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o TEXT Output format. Valid values: TEXT, COMPACT, JSON, YAML --pretty false Pretty print JSON output --dry-run false Execute command in dry-run mode (preview changes without applying) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou apply - Apply all changes (FULL mode) jikkou patch - Patch resources with a specified mode jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Replace all resources.\nSynopsis Replaces resources by deleting and ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-replace/","tags":"","title":"jikkou replace"},{"body":"Display Jikkou API server information.\nSynopsis Print information about the Jikkou API server. This command is only available when Jikkou is configured in proxy mode (i.e., when jikkou.proxy.url is set in your configuration).\njikkou server-info [flags] Examples # Get server information in YAML format (default) jikkou server-info # Get server information in JSON format jikkou server-info -o JSON Options Flag Short Default Description --output -o YAML Output format. Valid values: JSON, YAML Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou health - Check health of target environments Configuration - Learn how to configure proxy mode ","categories":"","description":"","excerpt":"Display Jikkou API server information.\nSynopsis Print information ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-server-info/","tags":"","title":"jikkou server-info"},{"body":"Create or update resources from the resource definition files.\nSynopsis Reconcile the target platform by creating or updating resources that are described by the resource definition files passed as arguments. This command uses the UPDATE reconciliation mode, meaning new resources will be created and existing resources will be updated. Resources that exist on the platform but are not described in the resource files will not be deleted.\njikkou update [flags] Examples # Update resources defined in a YAML file jikkou update -f my-resources.yaml # Preview what would be created or updated jikkou update -f my-resources.yaml --dry-run # Update resources from a directory jikkou update -f ./resources/ # Update resources with selector to filter specific resources jikkou update -f my-resources.yaml -s 'metadata.name MATCHES (my-topic-.*)' # Update resources targeting a specific provider jikkou update -f my-resources.yaml --provider kafka-prod Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o TEXT Output format. Valid values: TEXT, COMPACT, JSON, YAML --pretty false Pretty print JSON output --dry-run false Execute command in dry-run mode (preview changes without applying) Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou apply - Apply all changes (FULL mode) jikkou create - Create resources only (CREATE mode) jikkou delete - Delete resources (DELETE mode) jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Create or update resources from the resource definition files. ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-update/","tags":"","title":"jikkou update"},{"body":"Check whether the resources definitions meet all validation requirements.\nSynopsis Validate the resource definition files specified through the command line arguments.\nValidate runs all the user-defined validation requirements after performing any relevant resource transformations. Validation rules are applied only to resources matching the selectors passed through the command line arguments.\nIf validation passes, the command outputs the validated resources. If validation fails, the command prints the validation errors and exits with a non-zero status code.\njikkou validate [flags] Examples # Validate resources defined in a YAML file jikkou validate -f my-resources.yaml # Validate resources from a directory jikkou validate -f ./resources/ # Validate with a selector to filter specific resources jikkou validate -f my-resources.yaml -s 'metadata.name MATCHES (my-topic-.*)' # Validate with template values jikkou validate -f my-resources.yaml --values-files values.yaml # Validate with JSON output jikkou validate -f my-resources.yaml -o JSON # Validate targeting a specific provider jikkou validate -f my-resources.yaml --provider kafka-prod Options Flag Short Default Description --files -f Resource definition file or directory locations (one or more required) --file-name -n **/*.{yaml,yml} Glob pattern to filter resource files when using directories --values-files Template values file locations (one or more) --values-file-name **/*.{yaml,yml} Glob pattern to filter values files --set-label -l Set labels on resources (key=value, repeatable) --set-annotation Set annotations on resources (key=value, repeatable) --set-value -v Set template variables for the built-in Values object (key=value, repeatable) --selector -s Selector expression for including or excluding resources --selector-match ALL Selector matching strategy. Valid values: ALL, ANY, NONE --options Controller configuration options (key=value, repeatable) --provider Select a specific provider instance --output -o YAML Output format. Valid values: JSON, YAML Options inherited from parent commands Flag Description --logger-level=\u003clevel\u003e Log level: TRACE, DEBUG, INFO, WARN, ERROR SEE ALSO jikkou prepare - Prepare resources for validation jikkou apply - Apply changes after validation jikkou diff - Show changes without applying ","categories":"","description":"","excerpt":"Check whether the resources definitions meet all validation ‚Ä¶","ref":"/docs/jikkou-cli/commands/jikkou-validate/","tags":"","title":"jikkou validate"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kafka/","tags":"","title":"kafka"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kafka-connect/","tags":"","title":"kafka connect"},{"body":" This transformation can be used to enforce a maximum value for the number of partitions of kafka topics.\nConfiguration Name Type Description Default maxNumPartitions Int maximum value for the number of partitions to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMaxNumPartitions priority = 100 config = { maxNumPartitions = 50 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicmaxnumpartitions/","tags":"","title":"KafkaTopicMaxNumPartitions"},{"body":" This transformation can be used to enforce a maximum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default maxRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { maxRetentionMs = 2592000000 # 30 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicmaxretentionms/","tags":"","title":"KafkaTopicMaxRetentionMs"},{"body":" This transformation can be used to enforce a minimum value for the min.insync.replicas property of kafka topics.\nConfiguration Name Type Description Default minInSyncReplicas Int Minimum value of min.insync.replicas to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicmininsyncreplicas/","tags":"","title":"KafkaTopicMinInSyncReplicas"},{"body":" This transformation can be used to enforce a minimum value for the replication factor of kafka topics.\nConfiguration Name Type Description Default minReplicationFactor Int Minimum value of replication factor to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 3 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicminreplicas/","tags":"","title":"KafkaTopicMinReplicas"},{"body":" This transformation can be used to enforce a minimum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default minRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { minRetentionMs = 604800000 # 7 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/docs/providers/kafka/transformations/kafkatopicminretentionms/","tags":"","title":"KafkaTopicMinRetentionMs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/releases/","tags":"","title":"Releases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/resources/","tags":"","title":"resources"},{"body":"Blog GitOps \u0026 Kafka: Enabling smooth and seamless Data Schema management with Jikkou and GitHub Actions (by Florian Hussonnois) Jikkou: Declarative ACLs configuration for Apache Kafka¬Æ and Schema Registry on Aiven (by Florian Hussonnois) Kafka Connect + Jikkou- Easily manage Kafka connectors (by Florian Hussonnois) Why is Managing Kafka Topics Still Such a Pain? Introducing Jikkou! (Florian Hussonnois) ","categories":"","description":"","excerpt":"Blog GitOps \u0026 Kafka: Enabling smooth and seamless Data Schema ‚Ä¶","ref":"/docs/resources/","tags":"","title":"Resources"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]